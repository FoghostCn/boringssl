# Copyright (c) 2014, Google Inc.
#
# Permission to use, copy, modify, and/or distribute this software for any
# purpose with or without fee is hereby granted, provided that the above
# copyright notice and this permission notice appear in all copies.
#
# THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
# WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
# MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY
# SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
# WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN ACTION
# OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF OR IN
# CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.

# This file contains a pre-compiled version of chacha_vec.c for ARM. This is
# needed to support switching on NEON code at runtime. If the whole of OpenSSL
# were to be compiled with the needed flags to build chacha_vec.c, then it
# wouldn't be possible to run on non-NEON systems.
#
# This file was generated by:
#
#     /opt/gcc-linaro-4.9-2014.11-x86_64_arm-linux-gnueabihf/bin/arm-linux-gnueabihf-gcc -O3 -mcpu=cortex-a8 -mfpu=neon -S chacha_vec.c -DASM_GEN -I ../../include -fpic -o chacha_vec_arm.S

#if !defined(OPENSSL_NO_ASM)

	.syntax unified
	.cpu cortex-a8
	.eabi_attribute 27, 3

# EABI attribute 28 sets whether VFP register arguments were used to build this
# file. If object files are inconsistent on this point, the linker will refuse
# to link them. Thus we report whatever the compiler expects since we don't use
# VFP arguments.

#if defined(__ARM_PCS_VFP)
	.eabi_attribute 28, 1
#else
	.eabi_attribute 28, 0
#endif

	.fpu neon
	.eabi_attribute 20, 1
	.eabi_attribute 21, 1
	.eabi_attribute 23, 3
	.eabi_attribute 24, 1
	.eabi_attribute 25, 1
	.eabi_attribute 26, 2
	.eabi_attribute 30, 2
	.eabi_attribute 34, 1
	.eabi_attribute 18, 4
	.thumb
	.file	"chacha_vec.c"
	.text
	.align	2
	.global	CRYPTO_chacha_20_neon
	.hidden	CRYPTO_chacha_20_neon
	.thumb
	.thumb_func
	.type	CRYPTO_chacha_20_neon, %function
CRYPTO_chacha_20_neon:
	@ args = 8, pretend = 0, frame = 120
	@ frame_needed = 1, uses_anonymous_args = 0
	push	{r4, r5, r6, r7, r8, r9, r10, fp, lr}
	mov	r5, r3
	vpush.64	{d8, d9, d10, d11, d12, d13, d14, d15}
	mov	lr, r2
	mov	r10, r0
	mov	ip, r1
	mov	r6, r5
	movw	r8, #43691
	movt	r8, 43690
	vldr	d4, .L92
	vldr	d5, .L92+8
	sub	sp, sp, #124
	add	r7, sp, #0
	sub	sp, sp, #96
	umull	r8, r9, lr, r8
	str	r3, [r7, #32]
	add	r3, sp, #15
	str	r0, [r7, #60]
	bic	r4, r3, #15
	ldr	r0, [r5]	@ unaligned
	ldr	r3, [r5, #12]	@ unaligned
	add	lr, r7, #104
	str	r1, [r7, #12]
	str	r2, [r7, #8]
	ldr	r1, [r5, #4]	@ unaligned
	ldr	r2, [r5, #8]	@ unaligned
	str	r4, [r7, #64]
	adds	r4, r4, #64
	ldr	fp, [r7, #228]
	stmia	r4!, {r0, r1, r2, r3}
	ldr	r2, [r7, #64]
	ldr	r0, [r6, #16]!	@ unaligned
	add	r3, r2, #64
	ldr	r2, [r7, #224]
	str	r3, [r7, #56]
	mov	r4, r3
	movs	r3, #0
	ldr	r5, [r2]
	str	r3, [r7, #108]
	ldr	r3, [r6, #12]	@ unaligned
	ldr	r1, [r6, #4]	@ unaligned
	ldr	r2, [r6, #8]	@ unaligned
	ldr	r6, [r7, #224]
	ldr	r6, [r6, #4]
	str	r5, [r7, #112]
	str	fp, [r7, #104]
	str	r6, [r7, #100]
	str	r6, [r7, #116]
	ldr	r6, [r7, #64]
	vldr	d28, [r6, #64]
	vldr	d29, [r6, #72]
	vldr	d26, [r7, #104]
	vldr	d27, [r7, #112]
	stmia	r4!, {r0, r1, r2, r3}
	lsrs	r3, r9, #7
	vldr	d30, [r6, #64]
	vldr	d31, [r6, #72]
	beq	.L26
	lsls	r2, r3, #8
	vldr	d6, .L92+16
	vldr	d7, .L92+24
	sub	r3, r2, r3, lsl #6
	ldr	r2, [r7, #100]
	mov	r1, r5
	add	r4, fp, #2
	str	r3, [r7, #4]
	mov	r0, r10
	str	r4, [r7, #36]
	adds	r3, r0, r3
	str	ip, [r7, #52]
	str	r3, [r7, #16]
.L4:
	ldr	r0, [r7, #32]
	movw	r9, #11570
	vldr	d0, .L92
	vldr	d1, .L92+8
	movw	r10, #25710
	movt	r9, 31074
	movt	r10, 13088
	ldr	r3, [r0]
	movw	r8, #25972
	movt	r8, 27424
	str	r2, [r7, #100]
	str	r8, [r7, #72]
	vadd.i32	q10, q13, q3
	str	r3, [r7, #40]
	mov	r3, r0
	ldr	r5, [r3, #8]
	mov	r8, r10
	ldr	r6, [r3, #12]
	mov	r10, r9
	vmov	q8, q15  @ v4si
	ldr	r0, [r0, #4]
	str	r5, [r7, #84]
	mov	r5, r3
	ldr	r3, [r3, #16]
	vmov	q6, q0  @ v4si
	ldr	r4, [r5, #28]
	vmov	q4, q14  @ v4si
	ldr	lr, [r5, #20]
	vmov	q1, q13  @ v4si
	str	r3, [r7, #48]
	vmov	q9, q15  @ v4si
	ldr	r3, [r5, #24]
	vmov	q5, q14  @ v4si
	str	r4, [r7, #96]
	movw	fp, #30821
	ldr	r4, [r7, #36]
	movt	fp, 24944
	str	r3, [r7, #92]
	mov	r3, r1
	str	r6, [r7, #44]
	movs	r5, #10
	str	r4, [r7, #88]
	ldr	r1, [r7, #84]
	ldr	ip, [r7, #48]
	ldr	r2, [r7, #44]
	ldr	r9, [r7, #92]
	ldr	r4, [r7, #40]
	str	lr, [r7, #80]
	mov	lr, r3
	str	r5, [r7, #68]
	movs	r5, #0
	str	r5, [r7, #76]
	b	.L93
.L94:
	.align	3
.L92:
	.word	1634760805
	.word	857760878
	.word	2036477234
	.word	1797285236
	.word	1
	.word	0
	.word	0
	.word	0
.L93:
.L3:
	vadd.i32	q6, q6, q5
	add	r3, r8, r0
	vadd.i32	q0, q0, q4
	add	r5, fp, r4
	veor	q1, q1, q6
	mov	r6, r3
	veor	q10, q10, q0
	ldr	r3, [r7, #72]
	str	r5, [r7, #92]
	add	r10, r10, r1
	vrev32.16	q1, q1
	eor	lr, lr, r10
	vrev32.16	q10, q10
	vadd.i32	q9, q9, q1
	vadd.i32	q8, q8, q10
	mov	fp, r3
	ldr	r3, [r7, #88]
	veor	q11, q9, q5
	str	r6, [r7, #88]
	veor	q4, q8, q4
	eors	r3, r3, r5
	mov	r5, r6
	ldr	r6, [r7, #76]
	vshl.i32	q5, q11, #12
	vshl.i32	q12, q4, #12
	add	fp, fp, r2
	eors	r6, r6, r5
	ror	r3, r3, #16
	vsri.32	q5, q11, #20
	ror	lr, lr, #16
	mov	r5, r6
	ldr	r6, [r7, #100]
	vsri.32	q12, q4, #20
	str	r3, [r7, #100]
	eor	r6, r6, fp
	ror	r5, r5, #16
	vadd.i32	q6, q6, q5
	add	r9, r9, lr
	ror	r3, r6, #16
	ldr	r6, [r7, #100]
	vadd.i32	q0, q0, q12
	str	r3, [r7, #84]
	add	ip, ip, r6
	ldr	r6, [r7, #80]
	veor	q4, q6, q1
	eor	r4, ip, r4
	veor	q11, q0, q10
	eor	r1, r9, r1
	mov	r8, r6
	ldr	r6, [r7, #96]
	vshl.i32	q1, q4, #8
	ror	r4, r4, #20
	vshl.i32	q10, q11, #8
	add	r6, r6, r3
	eors	r2, r2, r6
	str	r6, [r7, #80]
	ldr	r6, [r7, #92]
	vsri.32	q1, q4, #24
	add	r8, r8, r5
	ror	r2, r2, #20
	vsri.32	q10, q11, #24
	adds	r6, r4, r6
	eor	r0, r8, r0
	add	fp, fp, r2
	mov	r3, r6
	vadd.i32	q9, q9, q1
	ldr	r6, [r7, #88]
	vadd.i32	q8, q8, q10
	ror	r0, r0, #20
	str	r3, [r7, #88]
	veor	q11, q9, q5
	adds	r6, r0, r6
	veor	q12, q8, q12
	str	r6, [r7, #96]
	mov	r6, r3
	ldr	r3, [r7, #100]
	vshl.i32	q5, q11, #7
	ror	r1, r1, #20
	eors	r3, r3, r6
	ldr	r6, [r7, #96]
	vshl.i32	q4, q12, #7
	add	r10, r10, r1
	eors	r5, r5, r6
	vsri.32	q5, q11, #25
	ldr	r6, [r7, #84]
	ror	r3, r3, #24
	ror	r5, r5, #24
	vsri.32	q4, q12, #25
	str	r5, [r7, #92]
	eor	r6, fp, r6
	ldr	r5, [r7, #92]
	add	ip, r3, ip
	str	ip, [r7, #100]
	vext.32	q5, q5, q5, #1
	add	ip, r5, r8
	ldr	r5, [r7, #80]
	eor	lr, r10, lr
	ror	r6, r6, #24
	vext.32	q4, q4, q4, #1
	add	r8, r6, r5
	ldr	r5, [r7, #100]
	vadd.i32	q6, q6, q5
	vadd.i32	q0, q0, q4
	ror	lr, lr, #24
	eor	r0, ip, r0
	vext.32	q1, q1, q1, #3
	add	r9, r9, lr
	eors	r4, r4, r5
	vext.32	q10, q10, q10, #3
	ldr	r5, [r7, #88]
	eor	r1, r9, r1
	ror	r0, r0, #25
	veor	q1, q6, q1
	adds	r5, r0, r5
	veor	q10, q0, q10
	str	r5, [r7, #88]
	ldr	r5, [r7, #96]
	ror	r1, r1, #25
	vext.32	q9, q9, q9, #2
	eor	r2, r8, r2
	adds	r5, r1, r5
	ror	r4, r4, #25
	vext.32	q8, q8, q8, #2
	str	r5, [r7, #76]
	eors	r3, r3, r5
	ldr	r5, [r7, #88]
	vrev32.16	q1, q1
	add	fp, fp, r4
	vrev32.16	q10, q10
	vadd.i32	q9, q9, q1
	vadd.i32	q8, q8, q10
	ror	r2, r2, #25
	veor	q11, q9, q5
	eor	lr, fp, lr
	veor	q4, q8, q4
	eors	r6, r6, r5
	ror	r3, r3, #16
	ldr	r5, [r7, #92]
	str	r3, [r7, #96]
	add	r10, r10, r2
	ldr	r3, [r7, #96]
	eor	r5, r10, r5
	ror	lr, lr, #16
	vshl.i32	q12, q4, #12
	add	ip, lr, ip
	vshl.i32	q5, q11, #12
	str	ip, [r7, #80]
	add	ip, r3, r8
	str	ip, [r7, #92]
	ror	r5, r5, #16
	ldr	r3, [r7, #100]
	vsri.32	q5, q11, #20
	ror	r6, r6, #16
	add	ip, r5, r3
	ldr	r3, [r7, #80]
	vsri.32	q12, q4, #20
	add	r9, r9, r6
	eor	r2, ip, r2
	eors	r4, r4, r3
	ldr	r3, [r7, #92]
	eor	r0, r9, r0
	vadd.i32	q6, q6, q5
	ror	r4, r4, #20
	eors	r1, r1, r3
	vadd.i32	q0, q0, q12
	ror	r3, r2, #20
	str	r3, [r7, #84]
	ldr	r3, [r7, #88]
	ror	r0, r0, #20
	add	r8, r4, fp
	veor	q7, q6, q1
	add	fp, r0, r3
	veor	q4, q0, q10
	ldr	r3, [r7, #76]
	ror	r1, r1, #20
	mov	r2, r8
	str	r8, [r7, #72]
	add	r8, r1, r3
	ldr	r3, [r7, #84]
	vshl.i32	q1, q7, #8
	eor	r6, fp, r6
	vshl.i32	q10, q4, #8
	add	r10, r10, r3
	ldr	r3, [r7, #96]
	eor	lr, r2, lr
	vsri.32	q1, q7, #24
	ror	r2, r6, #24
	eor	r3, r8, r3
	eor	r5, r10, r5
	vsri.32	q10, q4, #24
	str	r2, [r7, #100]
	ror	r2, r3, #24
	ldr	r3, [r7, #80]
	vadd.i32	q9, q9, q1
	ror	lr, lr, #24
	vadd.i32	q8, q8, q10
	ror	r6, r5, #24
	add	r5, lr, r3
	ldr	r3, [r7, #100]
	veor	q7, q9, q5
	add	ip, ip, r6
	veor	q12, q8, q12
	add	r9, r9, r3
	ldr	r3, [r7, #92]
	eors	r4, r4, r5
	vshl.i32	q5, q7, #7
	str	r2, [r7, #88]
	add	r3, r3, r2
	vshl.i32	q4, q12, #7
	eors	r1, r1, r3
	str	r3, [r7, #96]
	ldr	r3, [r7, #84]
	vsri.32	q5, q7, #25
	eor	r0, r9, r0
	str	r6, [r7, #76]
	eor	r2, ip, r3
	vsri.32	q4, q12, #25
	ldr	r3, [r7, #68]
	ror	r4, r4, #25
	str	r5, [r7, #80]
	ror	r0, r0, #25
	subs	r3, r3, #1
	ror	r1, r1, #25
	ror	r2, r2, #25
	vext.32	q9, q9, q9, #2
	str	r3, [r7, #68]
	vext.32	q1, q1, q1, #1
	vext.32	q8, q8, q8, #2
	vext.32	q10, q10, q10, #1
	vext.32	q5, q5, q5, #3
	vext.32	q4, q4, q4, #3
	bne	.L3
	ldr	r6, [r7, #52]
	vadd.i32	q6, q2, q6
	ldr	r3, [r7, #56]
	vadd.i32	q5, q14, q5
	str	r9, [r7, #92]
	mov	r9, r10
	str	r0, [r7, #68]
	mov	r10, r8
	ldr	r0, [r6]	@ unaligned
	vadd.i32	q1, q13, q1
	ldr	r8, [r7, #72]
	vadd.i32	q13, q13, q3
	str	r1, [r7, #84]
	vadd.i32	q9, q15, q9
	ldr	r1, [r6, #4]	@ unaligned
	vadd.i32	q0, q2, q0
	str	ip, [r7, #48]
	mov	ip, r3
	str	r2, [r7, #44]
	vadd.i32	q4, q14, q4
	ldr	r2, [r6, #8]	@ unaligned
	vadd.i32	q8, q15, q8
	ldr	r3, [r6, #12]	@ unaligned
	str	lr, [r7, #72]
	mov	lr, r5
	str	r4, [r7, #40]
	ldr	r5, [r7, #36]
	ldr	r4, [r7, #88]
	add	r4, r4, r5
	mov	r5, ip
	stmia	r5!, {r0, r1, r2, r3}
	mov	r5, ip
	ldr	r1, [r7, #64]
	str	r4, [r7, #24]
	mov	r4, ip
	vldr	d24, [r1, #64]
	vldr	d25, [r1, #72]
	veor	q12, q12, q6
	vadd.i32	q6, q13, q10
	vadd.i32	q13, q13, q3
	vstr	d24, [r1, #64]
	vstr	d25, [r1, #72]
	ldmia	r4!, {r0, r1, r2, r3}
	vadd.i32	q13, q13, q3
	ldr	r4, [r7, #60]
	str	r1, [r4, #4]	@ unaligned
	str	r2, [r4, #8]	@ unaligned
	add	r2, r4, #16
	str	r3, [r4, #12]	@ unaligned
	str	r0, [r4]	@ unaligned
	ldr	r0, [r6, #16]!	@ unaligned
	str	r2, [r7, #88]
	str	ip, [r7, #80]
	ldr	r1, [r6, #4]	@ unaligned
	ldr	r2, [r6, #8]	@ unaligned
	ldr	r3, [r6, #12]	@ unaligned
	ldr	r6, [r7, #52]
	stmia	r5!, {r0, r1, r2, r3}
	mov	r5, ip
	ldr	r1, [r7, #64]
	str	r5, [r7, #28]
	vldr	d24, [r1, #64]
	vldr	d25, [r1, #72]
	veor	q12, q12, q5
	vstr	d24, [r1, #64]
	vstr	d25, [r1, #72]
	ldmia	ip!, {r0, r1, r2, r3}
	str	r0, [r4, #16]	@ unaligned
	ldr	r4, [r7, #88]
	str	r2, [r4, #8]	@ unaligned
	str	r1, [r4, #4]	@ unaligned
	str	r3, [r4, #12]	@ unaligned
	ldr	r0, [r6, #32]!	@ unaligned
	ldr	r4, [r7, #60]
	ldr	r1, [r6, #4]	@ unaligned
	add	r2, r4, #32
	ldr	r3, [r6, #12]	@ unaligned
	str	r2, [r7, #88]
	ldr	r2, [r6, #8]	@ unaligned
	ldr	r6, [r7, #52]
	stmia	r5!, {r0, r1, r2, r3}
	ldr	r1, [r7, #64]
	ldr	r3, [r7, #28]
	ldr	r5, [r7, #56]
	vldr	d20, [r1, #64]
	vldr	d21, [r1, #72]
	veor	q9, q10, q9
	mov	ip, r3
	vstr	d18, [r1, #64]
	vstr	d19, [r1, #72]
	ldmia	ip!, {r0, r1, r2, r3}
	str	r0, [r4, #32]	@ unaligned
	ldr	r4, [r7, #88]
	str	r2, [r4, #8]	@ unaligned
	str	r1, [r4, #4]	@ unaligned
	str	r3, [r4, #12]	@ unaligned
	ldr	r0, [r6, #48]!	@ unaligned
	ldr	r4, [r7, #60]
	ldr	r1, [r6, #4]	@ unaligned
	add	r2, r4, #48
	ldr	r3, [r6, #12]	@ unaligned
	str	r2, [r7, #88]
	ldr	r2, [r6, #8]	@ unaligned
	ldr	r6, [r7, #52]
	stmia	r5!, {r0, r1, r2, r3}
	ldr	r1, [r7, #64]
	ldr	r3, [r7, #28]
	vldr	d22, [r1, #64]
	vldr	d23, [r1, #72]
	veor	q1, q11, q1
	mov	ip, r3
	vstr	d2, [r1, #64]
	vstr	d3, [r1, #72]
	ldmia	ip!, {r0, r1, r2, r3}
	ldr	r5, [r7, #56]
	str	r0, [r4, #48]	@ unaligned
	ldr	r4, [r7, #88]
	str	r2, [r4, #8]	@ unaligned
	str	r1, [r4, #4]	@ unaligned
	str	r3, [r4, #12]	@ unaligned
	ldr	r0, [r6, #64]!	@ unaligned
	ldr	r4, [r7, #60]
	ldr	r1, [r6, #4]	@ unaligned
	add	r2, r4, #64
	ldr	r3, [r6, #12]	@ unaligned
	str	r2, [r7, #88]
	ldr	r2, [r6, #8]	@ unaligned
	ldr	r6, [r7, #52]
	stmia	r5!, {r0, r1, r2, r3}
	ldr	r1, [r7, #64]
	ldr	r3, [r7, #28]
	ldr	r5, [r7, #56]
	vldr	d22, [r1, #64]
	vldr	d23, [r1, #72]
	veor	q11, q11, q0
	mov	ip, r3
	str	r5, [r7, #20]
	vstr	d22, [r1, #64]
	vstr	d23, [r1, #72]
	ldmia	ip!, {r0, r1, r2, r3}
	str	r0, [r4, #64]	@ unaligned
	ldr	r4, [r7, #88]
	str	r1, [r4, #4]	@ unaligned
	str	r2, [r4, #8]	@ unaligned
	str	r3, [r4, #12]	@ unaligned
	ldr	r0, [r6, #80]!	@ unaligned
	ldr	r4, [r7, #60]
	ldr	r1, [r6, #4]	@ unaligned
	add	r2, r4, #80
	ldr	r3, [r6, #12]	@ unaligned
	str	r2, [r7, #28]
	ldr	r2, [r6, #8]	@ unaligned
	ldr	r6, [r7, #52]
	stmia	r5!, {r0, r1, r2, r3}
	ldr	r1, [r7, #64]
	ldr	r3, [r7, #20]
	ldr	r5, [r7, #56]
	vldr	d22, [r1, #64]
	vldr	d23, [r1, #72]
	veor	q4, q11, q4
	mov	ip, r3
	str	r5, [r7, #88]
	vstr	d8, [r1, #64]
	vstr	d9, [r1, #72]
	ldmia	ip!, {r0, r1, r2, r3}
	mov	ip, r5
	str	r0, [r4, #80]	@ unaligned
	ldr	r4, [r7, #28]
	str	r2, [r4, #8]	@ unaligned
	str	r1, [r4, #4]	@ unaligned
	str	r3, [r4, #12]	@ unaligned
	ldr	r0, [r6, #96]!	@ unaligned
	ldr	r4, [r7, #60]
	ldr	r1, [r6, #4]	@ unaligned
	add	r2, r4, #96
	ldr	r3, [r6, #12]	@ unaligned
	str	r2, [r7, #60]
	ldr	r2, [r6, #8]	@ unaligned
	ldr	r6, [r7, #52]
	stmia	r5!, {r0, r1, r2, r3}
	mov	r5, r4
	ldr	r1, [r7, #64]
	vldr	d18, [r1, #64]
	vldr	d19, [r1, #72]
	veor	q8, q9, q8
	vstr	d16, [r1, #64]
	vstr	d17, [r1, #72]
	ldmia	ip!, {r0, r1, r2, r3}
	str	r0, [r4, #96]	@ unaligned
	ldr	r4, [r7, #60]
	str	r2, [r4, #8]	@ unaligned
	add	r2, r5, #112
	str	r1, [r4, #4]	@ unaligned
	str	r3, [r4, #12]	@ unaligned
	mov	ip, r2
	ldr	r0, [r6, #112]!	@ unaligned
	mov	r4, r5
	ldr	r5, [r7, #88]
	str	r4, [r7, #60]
	ldr	r1, [r6, #4]	@ unaligned
	ldr	r2, [r6, #8]	@ unaligned
	ldr	r3, [r6, #12]	@ unaligned
	movw	r6, #30821
	movt	r6, 24944
	add	r6, r6, fp
	stmia	r5!, {r0, r1, r2, r3}
	movw	r5, #25710
	ldr	r1, [r7, #64]
	movt	r5, 13088
	ldr	r3, [r7, #80]
	add	r5, r5, r10
	movw	r10, #11570
	vldr	d16, [r1, #64]
	vldr	d17, [r1, #72]
	movt	r10, 31074
	veor	q10, q8, q6
	mov	fp, r3
	add	r10, r10, r9
	movw	r9, #25972
	movt	r9, 27424
	add	r9, r9, r8
	vstr	d20, [r1, #64]
	vstr	d21, [r1, #72]
	ldmia	fp!, {r0, r1, r2, r3}
	str	r0, [r4, #112]	@ unaligned
	ldr	r4, [r7, #52]
	ldr	r0, [r7, #60]
	str	r1, [ip, #4]	@ unaligned
	str	r2, [ip, #8]	@ unaligned
	str	r3, [ip, #12]	@ unaligned
	ldr	r3, [r4, #128]
	ldr	r1, [r7, #40]
	eors	r6, r6, r3
	str	r6, [r0, #128]
	ldr	r3, [r4, #132]
	ldr	r6, [r7, #32]
	mov	r8, r1
	eors	r5, r5, r3
	str	r5, [r0, #132]
	ldr	r3, [r4, #136]
	mov	r1, r0
	mov	r5, r4
	eor	r3, r10, r3
	str	r3, [r0, #136]
	ldr	r3, [r4, #140]
	mov	ip, r5
	eor	r3, r9, r3
	str	r3, [r0, #140]
	ldr	r3, [r6]
	ldr	r2, [r4, #144]
	add	r8, r8, r3
	mov	r3, r8
	eors	r2, r2, r3
	str	r2, [r0, #144]
	ldr	r0, [r7, #68]
	ldr	r2, [r6, #4]
	ldr	r3, [r4, #148]
	mov	r4, r1
	add	r0, r0, r2
	eors	r3, r3, r0
	str	r3, [r1, #148]
	ldr	r1, [r7, #84]
	mov	r0, r4
	ldr	r2, [r6, #8]
	ldr	r3, [r5, #152]
	mov	r8, r1
	add	r8, r8, r2
	ldr	r1, [r7, #44]
	mov	r2, r8
	eors	r3, r3, r2
	str	r3, [r4, #152]
	mov	r8, r6
	ldr	r2, [r6, #12]
	ldr	r3, [r5, #156]
	add	r1, r1, r2
	eors	r3, r3, r1
	ldr	r1, [r7, #48]
	str	r3, [r4, #156]
	ldr	r2, [r6, #16]
	ldr	r3, [r5, #160]
	add	r1, r1, r2
	eors	r3, r3, r1
	str	r3, [r4, #160]
	ldr	r2, [r6, #20]
	mov	r1, r4
	ldr	r3, [r5, #164]
	mov	r4, r5
	add	lr, lr, r2
	ldr	r2, [r7, #92]
	eor	r3, lr, r3
	str	r3, [r1, #164]
	ldr	r6, [r6, #24]
	mov	r1, r0
	ldr	r3, [r4, #168]
	add	r2, r2, r6
	eors	r3, r3, r2
	ldr	r2, [r7, #96]
	str	r3, [r0, #168]
	ldr	r5, [r8, #28]
	ldr	r3, [r4, #172]
	add	r2, r2, r5
	ldr	r0, [r7, #24]
	eors	r3, r3, r2
	str	r3, [r1, #172]
	ldr	r3, [r7, #36]
	mov	r2, r1
	ldr	r4, [r4, #176]
	eors	r4, r4, r0
	adds	r0, r3, #3
	str	r4, [r1, #176]
	mov	r4, r2
	str	r0, [r7, #36]
	ldr	r3, [r7, #76]
	ldr	r0, [ip, #180]
	eors	r3, r3, r0
	mov	r0, r3
	ldr	r3, [r7, #224]
	str	r0, [r2, #180]
	ldr	r2, [r7, #72]
	ldr	r1, [r3]
	ldr	r3, [ip, #184]
	add	r2, r2, r1
	mov	r1, ip
	eors	r3, r3, r2
	str	r3, [r4, #184]
	ldr	r3, [r7, #224]
	adds	r1, r1, #192
	str	r1, [r7, #52]
	ldr	r1, [r7, #100]
	ldr	r2, [r3, #4]
	ldr	r3, [ip, #188]
	add	r1, r1, r2
	mov	r2, r1
	eors	r2, r2, r3
	str	r2, [r4, #188]
	mov	r3, r4
	ldr	r2, [r7, #16]
	adds	r3, r3, #192
	str	r3, [r7, #60]
	cmp	r2, r3
	beq	.L85
	ldr	r3, [r7, #224]
	ldmia	r3, {r1, r2}
	b	.L4
.L85:
	ldr	r3, [r7, #12]
	ldr	r2, [r7, #4]
	add	r3, r3, r2
	str	r3, [r7, #12]
.L2:
	ldr	r1, [r7, #8]
	movw	r2, #43691
	movt	r2, 43690
	umull	r2, r3, r1, r2
	lsr	fp, r3, #7
	lsl	r3, fp, #8
	sub	fp, r3, fp, lsl #6
	rsb	fp, fp, r1
	lsrs	fp, fp, #6
	beq	.L6
	ldr	r5, [r7, #12]
	ldr	r4, [r7, #16]
	ldr	r6, [r7, #64]
	ldr	lr, [r7, #56]
	vldr	d20, .L95
	vldr	d21, .L95+8
	str	fp, [r7, #96]
	str	fp, [r7, #100]
.L8:
	vmov	q1, q13  @ v4si
	vldr	d24, .L95+16
	vldr	d25, .L95+24
	vmov	q8, q15  @ v4si
	movs	r3, #10
	vmov	q11, q14  @ v4si
.L7:
	vadd.i32	q12, q12, q11
	subs	r3, r3, #1
	veor	q1, q1, q12
	vrev32.16	q1, q1
	vadd.i32	q8, q8, q1
	veor	q11, q8, q11
	vshl.i32	q3, q11, #12
	vsri.32	q3, q11, #20
	vadd.i32	q12, q12, q3
	veor	q1, q12, q1
	vshl.i32	q9, q1, #8
	vsri.32	q9, q1, #24
	vadd.i32	q8, q8, q9
	vext.32	q9, q9, q9, #3
	veor	q3, q8, q3
	vext.32	q11, q8, q8, #2
	vshl.i32	q8, q3, #7
	vsri.32	q8, q3, #25
	vext.32	q8, q8, q8, #1
	vadd.i32	q12, q12, q8
	veor	q9, q12, q9
	vrev32.16	q9, q9
	vadd.i32	q11, q11, q9
	veor	q8, q11, q8
	vshl.i32	q3, q8, #12
	vsri.32	q3, q8, #20
	vadd.i32	q12, q12, q3
	veor	q9, q12, q9
	vshl.i32	q1, q9, #8
	vsri.32	q1, q9, #24
	vadd.i32	q8, q11, q1
	vext.32	q1, q1, q1, #1
	veor	q3, q8, q3
	vext.32	q8, q8, q8, #2
	vshl.i32	q11, q3, #7
	vsri.32	q11, q3, #25
	vext.32	q11, q11, q11, #3
	bne	.L7
	ldr	r0, [r5]	@ unaligned
	vadd.i32	q12, q2, q12
	ldr	r1, [r5, #4]	@ unaligned
	mov	ip, lr
	ldr	r2, [r5, #8]	@ unaligned
	mov	r9, lr
	ldr	r3, [r5, #12]	@ unaligned
	mov	r10, r5
	vadd.i32	q11, q14, q11
	mov	r8, lr
	vadd.i32	q3, q15, q8
	stmia	ip!, {r0, r1, r2, r3}
	mov	ip, lr
	vldr	d18, [r6, #64]
	vldr	d19, [r6, #72]
	vadd.i32	q1, q13, q1
	veor	q9, q9, q12
	vadd.i32	q13, q13, q10
	vstr	d18, [r6, #64]
	vstr	d19, [r6, #72]
	ldmia	r9!, {r0, r1, r2, r3}
	mov	r9, r5
	str	r0, [r4]	@ unaligned
	str	r1, [r4, #4]	@ unaligned
	str	r2, [r4, #8]	@ unaligned
	str	r3, [r4, #12]	@ unaligned
	ldr	r0, [r10, #16]!	@ unaligned
	ldr	r1, [r10, #4]	@ unaligned
	ldr	r2, [r10, #8]	@ unaligned
	ldr	r3, [r10, #12]	@ unaligned
	add	r10, r4, #48
	adds	r4, r4, #64
	stmia	r8!, {r0, r1, r2, r3}
	mov	r8, lr
	vldr	d18, [r6, #64]
	vldr	d19, [r6, #72]
	veor	q9, q9, q11
	vstr	d18, [r6, #64]
	vstr	d19, [r6, #72]
	ldmia	ip!, {r0, r1, r2, r3}
	mov	ip, lr
	str	r0, [r4, #-48]	@ unaligned
	str	r1, [r4, #-44]	@ unaligned
	str	r2, [r4, #-40]	@ unaligned
	str	r3, [r4, #-36]	@ unaligned
	ldr	r0, [r9, #32]!	@ unaligned
	ldr	r1, [r9, #4]	@ unaligned
	ldr	r2, [r9, #8]	@ unaligned
	ldr	r3, [r9, #12]	@ unaligned
	mov	r9, r5
	adds	r5, r5, #64
	stmia	r8!, {r0, r1, r2, r3}
	mov	r8, lr
	vldr	d16, [r6, #64]
	vldr	d17, [r6, #72]
	veor	q8, q8, q3
	vstr	d16, [r6, #64]
	vstr	d17, [r6, #72]
	ldmia	ip!, {r0, r1, r2, r3}
	mov	ip, lr
	str	r0, [r4, #-32]	@ unaligned
	str	r1, [r4, #-28]	@ unaligned
	str	r2, [r4, #-24]	@ unaligned
	str	r3, [r4, #-20]	@ unaligned
	ldr	r0, [r9, #48]!	@ unaligned
	ldr	r1, [r9, #4]	@ unaligned
	ldr	r2, [r9, #8]	@ unaligned
	ldr	r3, [r9, #12]	@ unaligned
	stmia	r8!, {r0, r1, r2, r3}
	vldr	d16, [r6, #64]
	vldr	d17, [r6, #72]
	veor	q8, q8, q1
	vstr	d16, [r6, #64]
	vstr	d17, [r6, #72]
	ldmia	ip!, {r0, r1, r2, r3}
	str	r0, [r4, #-16]	@ unaligned
	str	r1, [r4, #-12]	@ unaligned
	str	r3, [r10, #12]	@ unaligned
	ldr	r3, [r7, #100]
	str	r2, [r10, #8]	@ unaligned
	cmp	r3, #1
	beq	.L88
	movs	r3, #1
	str	r3, [r7, #100]
	b	.L8
.L96:
	.align	3
.L95:
	.word	1
	.word	0
	.word	0
	.word	0
	.word	1634760805
	.word	857760878
	.word	2036477234
	.word	1797285236
.L88:
	ldr	fp, [r7, #96]
	ldr	r3, [r7, #12]
	lsl	fp, fp, #6
	add	r3, r3, fp
	str	r3, [r7, #12]
	ldr	r3, [r7, #16]
	add	r3, r3, fp
	str	r3, [r7, #16]
.L6:
	ldr	r3, [r7, #8]
	ands	r9, r3, #63
	beq	.L1
	vmov	q12, q13  @ v4si
	vldr	d20, .L95+16
	vldr	d21, .L95+24
	vmov	q8, q15  @ v4si
	movs	r3, #10
	vmov	q11, q14  @ v4si
	mov	r5, r9
.L10:
	vadd.i32	q10, q10, q11
	subs	r3, r3, #1
	veor	q9, q12, q10
	vrev32.16	q9, q9
	vadd.i32	q8, q8, q9
	veor	q11, q8, q11
	vshl.i32	q12, q11, #12
	vsri.32	q12, q11, #20
	vadd.i32	q10, q10, q12
	veor	q11, q10, q9
	vshl.i32	q9, q11, #8
	vsri.32	q9, q11, #24
	vadd.i32	q8, q8, q9
	vext.32	q9, q9, q9, #3
	veor	q12, q8, q12
	vext.32	q8, q8, q8, #2
	vshl.i32	q11, q12, #7
	vsri.32	q11, q12, #25
	vext.32	q11, q11, q11, #1
	vadd.i32	q10, q10, q11
	veor	q9, q10, q9
	vrev32.16	q9, q9
	vadd.i32	q8, q8, q9
	veor	q11, q8, q11
	vshl.i32	q12, q11, #12
	vsri.32	q12, q11, #20
	vadd.i32	q10, q10, q12
	vmov	q11, q12  @ v4si
	veor	q9, q10, q9
	vshl.i32	q12, q9, #8
	vsri.32	q12, q9, #24
	vadd.i32	q8, q8, q12
	vext.32	q12, q12, q12, #1
	veor	q9, q8, q11
	vext.32	q8, q8, q8, #2
	vshl.i32	q11, q9, #7
	vsri.32	q11, q9, #25
	vext.32	q11, q11, q11, #3
	bne	.L10
	cmp	r5, #15
	mov	r9, r5
	bhi	.L89
	vldr	d16, .L95+16
	vldr	d17, .L95+24
	vadd.i32	q10, q8, q10
	ldr	r3, [r7, #64]
	vst1.64	{d20-d21}, [r3:128]
.L14:
	ldr	r3, [r7, #8]
	and	r2, r3, #48
	cmp	r9, r2
	bls	.L1
	ldr	r6, [r7, #16]
	add	r3, r2, #16
	ldr	r1, [r7, #12]
	rsb	ip, r2, r9
	adds	r0, r1, r2
	mov	r4, r6
	add	r1, r1, r3
	add	r4, r4, r2
	add	r3, r3, r6
	cmp	r0, r3
	it	cc
	cmpcc	r4, r1
	ite	cs
	movcs	r3, #1
	movcc	r3, #0
	cmp	ip, #18
	ite	ls
	movls	r3, #0
	andhi	r3, r3, #1
	cmp	r3, #0
	beq	.L16
	and	r1, r0, #7
	mov	r3, r2
	negs	r1, r1
	and	r1, r1, #15
	cmp	r1, ip
	it	cs
	movcs	r1, ip
	cmp	r1, #0
	beq	.L17
	ldr	r5, [r7, #64]
	cmp	r1, #1
	ldrb	r0, [r0]	@ zero_extendqisi2
	add	r3, r2, #1
	ldrb	lr, [r5, r2]	@ zero_extendqisi2
	mov	r6, r5
	eor	r0, lr, r0
	strb	r0, [r4]
	beq	.L17
	ldr	r0, [r7, #12]
	cmp	r1, #2
	ldrb	r4, [r5, r3]	@ zero_extendqisi2
	ldr	r5, [r7, #16]
	ldrb	r0, [r0, r3]	@ zero_extendqisi2
	eor	r0, r0, r4
	strb	r0, [r5, r3]
	add	r3, r2, #2
	beq	.L17
	ldr	r0, [r7, #12]
	cmp	r1, #3
	ldrb	r4, [r6, r3]	@ zero_extendqisi2
	ldrb	r0, [r0, r3]	@ zero_extendqisi2
	eor	r0, r0, r4
	strb	r0, [r5, r3]
	add	r3, r2, #3
	beq	.L17
	ldr	r0, [r7, #12]
	cmp	r1, #4
	ldrb	r4, [r6, r3]	@ zero_extendqisi2
	ldrb	r0, [r0, r3]	@ zero_extendqisi2
	eor	r0, r0, r4
	strb	r0, [r5, r3]
	add	r3, r2, #4
	beq	.L17
	ldr	r0, [r7, #12]
	cmp	r1, #5
	ldrb	r4, [r6, r3]	@ zero_extendqisi2
	ldrb	r0, [r0, r3]	@ zero_extendqisi2
	eor	r0, r0, r4
	strb	r0, [r5, r3]
	add	r3, r2, #5
	beq	.L17
	ldr	r0, [r7, #12]
	cmp	r1, #6
	ldrb	r4, [r6, r3]	@ zero_extendqisi2
	ldrb	r0, [r0, r3]	@ zero_extendqisi2
	eor	r0, r0, r4
	strb	r0, [r5, r3]
	add	r3, r2, #6
	beq	.L17
	ldr	r0, [r7, #12]
	cmp	r1, #7
	ldrb	r4, [r6, r3]	@ zero_extendqisi2
	ldrb	r0, [r0, r3]	@ zero_extendqisi2
	eor	r0, r0, r4
	strb	r0, [r5, r3]
	add	r3, r2, #7
	beq	.L17
	ldr	r0, [r7, #12]
	cmp	r1, #8
	ldrb	r4, [r6, r3]	@ zero_extendqisi2
	ldrb	r0, [r0, r3]	@ zero_extendqisi2
	eor	r0, r0, r4
	strb	r0, [r5, r3]
	add	r3, r2, #8
	beq	.L17
	ldr	r0, [r7, #12]
	cmp	r1, #9
	ldrb	r4, [r6, r3]	@ zero_extendqisi2
	ldrb	r0, [r0, r3]	@ zero_extendqisi2
	eor	r0, r0, r4
	strb	r0, [r5, r3]
	add	r3, r2, #9
	beq	.L17
	ldr	r0, [r7, #12]
	cmp	r1, #10
	ldrb	r4, [r6, r3]	@ zero_extendqisi2
	ldrb	r0, [r0, r3]	@ zero_extendqisi2
	eor	r0, r0, r4
	strb	r0, [r5, r3]
	add	r3, r2, #10
	beq	.L17
	ldr	r0, [r7, #12]
	cmp	r1, #11
	ldrb	r4, [r6, r3]	@ zero_extendqisi2
	ldrb	r0, [r0, r3]	@ zero_extendqisi2
	eor	r0, r0, r4
	strb	r0, [r5, r3]
	add	r3, r2, #11
	beq	.L17
	ldr	r0, [r7, #12]
	cmp	r1, #12
	ldrb	r4, [r6, r3]	@ zero_extendqisi2
	ldrb	r0, [r0, r3]	@ zero_extendqisi2
	eor	r0, r0, r4
	strb	r0, [r5, r3]
	add	r3, r2, #12
	beq	.L17
	ldr	r0, [r7, #12]
	cmp	r1, #13
	ldrb	r4, [r6, r3]	@ zero_extendqisi2
	ldrb	r0, [r0, r3]	@ zero_extendqisi2
	eor	r0, r0, r4
	strb	r0, [r5, r3]
	add	r3, r2, #13
	beq	.L17
	ldr	r0, [r7, #12]
	cmp	r1, #15
	ldrb	r4, [r6, r3]	@ zero_extendqisi2
	ldrb	r0, [r0, r3]	@ zero_extendqisi2
	eor	r0, r0, r4
	strb	r0, [r5, r3]
	add	r3, r2, #14
	bne	.L17
	ldr	r0, [r7, #12]
	ldrb	r4, [r6, r3]	@ zero_extendqisi2
	ldrb	r0, [r0, r3]	@ zero_extendqisi2
	eors	r0, r0, r4
	strb	r0, [r5, r3]
	add	r3, r2, #15
.L17:
	rsb	r4, r1, ip
	add	r0, ip, #-1
	sub	r6, r4, #16
	subs	r0, r0, r1
	cmp	r0, #14
	lsr	r6, r6, #4
	add	r6, r6, #1
	lsl	lr, r6, #4
	bls	.L19
	add	r2, r2, r1
	ldr	r1, [r7, #12]
	ldr	r5, [r7, #16]
	cmp	r6, #1
	add	r0, r1, r2
	ldr	r1, [r7, #64]
	add	r1, r1, r2
	vld1.64	{d18-d19}, [r0:64]
	add	r2, r2, r5
	vld1.8	{q8}, [r1]
	veor	q8, q8, q9
	vst1.8	{q8}, [r2]
	beq	.L20
	add	r8, r1, #16
	add	ip, r2, #16
	vldr	d18, [r0, #16]
	vldr	d19, [r0, #24]
	cmp	r6, #2
	vld1.8	{q8}, [r8]
	veor	q8, q8, q9
	vst1.8	{q8}, [ip]
	beq	.L20
	add	r8, r1, #32
	add	ip, r2, #32
	vldr	d18, [r0, #32]
	vldr	d19, [r0, #40]
	cmp	r6, #3
	vld1.8	{q8}, [r8]
	veor	q8, q8, q9
	vst1.8	{q8}, [ip]
	beq	.L20
	adds	r1, r1, #48
	adds	r2, r2, #48
	vldr	d18, [r0, #48]
	vldr	d19, [r0, #56]
	vld1.8	{q8}, [r1]
	veor	q8, q8, q9
	vst1.8	{q8}, [r2]
.L20:
	cmp	lr, r4
	add	r3, r3, lr
	beq	.L1
.L19:
	ldr	r4, [r7, #64]
	adds	r2, r3, #1
	ldr	r1, [r7, #12]
	cmp	r2, r9
	ldr	r5, [r7, #16]
	ldrb	r0, [r4, r3]	@ zero_extendqisi2
	ldrb	r1, [r1, r3]	@ zero_extendqisi2
	eor	r1, r1, r0
	strb	r1, [r5, r3]
	bcs	.L1
	ldr	r0, [r7, #12]
	adds	r1, r3, #2
	mov	r6, r4
	cmp	r9, r1
	ldrb	r4, [r4, r2]	@ zero_extendqisi2
	ldrb	r0, [r0, r2]	@ zero_extendqisi2
	eor	r0, r0, r4
	strb	r0, [r5, r2]
	bls	.L1
	ldr	r0, [r7, #12]
	adds	r2, r3, #3
	ldrb	r4, [r6, r1]	@ zero_extendqisi2
	cmp	r9, r2
	ldrb	r0, [r0, r1]	@ zero_extendqisi2
	eor	r0, r0, r4
	strb	r0, [r5, r1]
	bls	.L1
	ldr	r0, [r7, #12]
	adds	r1, r3, #4
	ldrb	r4, [r6, r2]	@ zero_extendqisi2
	cmp	r9, r1
	ldrb	r0, [r0, r2]	@ zero_extendqisi2
	eor	r0, r0, r4
	strb	r0, [r5, r2]
	bls	.L1
	ldr	r0, [r7, #12]
	adds	r2, r3, #5
	ldrb	r4, [r6, r1]	@ zero_extendqisi2
	cmp	r9, r2
	ldrb	r0, [r0, r1]	@ zero_extendqisi2
	eor	r0, r0, r4
	strb	r0, [r5, r1]
	bls	.L1
	ldr	r0, [r7, #12]
	adds	r1, r3, #6
	ldrb	r4, [r6, r2]	@ zero_extendqisi2
	cmp	r9, r1
	ldrb	r0, [r0, r2]	@ zero_extendqisi2
	eor	r0, r0, r4
	strb	r0, [r5, r2]
	bls	.L1
	ldr	r0, [r7, #12]
	adds	r2, r3, #7
	ldrb	r4, [r6, r1]	@ zero_extendqisi2
	cmp	r9, r2
	ldrb	r0, [r0, r1]	@ zero_extendqisi2
	eor	r0, r0, r4
	strb	r0, [r5, r1]
	bls	.L1
	ldr	r0, [r7, #12]
	add	r1, r3, #8
	ldrb	r4, [r6, r2]	@ zero_extendqisi2
	cmp	r9, r1
	ldrb	r0, [r0, r2]	@ zero_extendqisi2
	eor	r0, r0, r4
	strb	r0, [r5, r2]
	bls	.L1
	ldr	r0, [r7, #12]
	add	r2, r3, #9
	ldrb	r4, [r6, r1]	@ zero_extendqisi2
	cmp	r9, r2
	ldrb	r0, [r0, r1]	@ zero_extendqisi2
	eor	r0, r0, r4
	strb	r0, [r5, r1]
	bls	.L1
	ldr	r0, [r7, #12]
	add	r1, r3, #10
	ldrb	r4, [r6, r2]	@ zero_extendqisi2
	cmp	r9, r1
	ldrb	r0, [r0, r2]	@ zero_extendqisi2
	eor	r0, r0, r4
	strb	r0, [r5, r2]
	bls	.L1
	ldr	r0, [r7, #12]
	add	r2, r3, #11
	ldrb	r4, [r6, r1]	@ zero_extendqisi2
	cmp	r9, r2
	ldrb	r0, [r0, r1]	@ zero_extendqisi2
	eor	r0, r0, r4
	strb	r0, [r5, r1]
	bls	.L1
	ldr	r0, [r7, #12]
	add	r1, r3, #12
	ldrb	r4, [r6, r2]	@ zero_extendqisi2
	cmp	r9, r1
	ldrb	r0, [r0, r2]	@ zero_extendqisi2
	eor	r0, r0, r4
	strb	r0, [r5, r2]
	bls	.L1
	ldr	r0, [r7, #12]
	add	r2, r3, #13
	ldrb	r4, [r6, r1]	@ zero_extendqisi2
	cmp	r9, r2
	ldrb	r0, [r0, r1]	@ zero_extendqisi2
	eor	r0, r0, r4
	strb	r0, [r5, r1]
	bls	.L1
	ldr	r1, [r7, #12]
	adds	r3, r3, #14
	ldrb	r0, [r6, r2]	@ zero_extendqisi2
	cmp	r9, r3
	ldrb	r1, [r1, r2]	@ zero_extendqisi2
	eor	r1, r1, r0
	strb	r1, [r5, r2]
	bls	.L1
	ldr	r2, [r7, #64]
	ldrb	r1, [r2, r3]	@ zero_extendqisi2
	ldr	r2, [r7, #12]
	ldrb	r2, [r2, r3]	@ zero_extendqisi2
	eors	r2, r2, r1
	ldr	r1, [r7, #16]
	strb	r2, [r1, r3]
.L1:
	adds	r7, r7, #124
	mov	sp, r7
	@ sp needed
	vldm	sp!, {d8-d15}
	pop	{r4, r5, r6, r7, r8, r9, r10, fp, pc}
.L89:
	ldr	r5, [r7, #12]
	cmp	r9, #31
	ldr	r4, [r7, #56]
	vldr	d18, .L97
	vldr	d19, .L97+8
	ldr	r0, [r5]	@ unaligned
	vadd.i32	q10, q9, q10
	ldr	r1, [r5, #4]	@ unaligned
	mov	r6, r4
	ldr	r2, [r5, #8]	@ unaligned
	ldr	r3, [r5, #12]	@ unaligned
	stmia	r6!, {r0, r1, r2, r3}
	ldr	r2, [r7, #64]
	ldr	r6, [r7, #16]
	vldr	d18, [r2, #64]
	vldr	d19, [r2, #72]
	veor	q9, q9, q10
	vstr	d18, [r2, #64]
	vstr	d19, [r2, #72]
	ldmia	r4!, {r0, r1, r2, r3}
	str	r1, [r6, #4]	@ unaligned
	mov	r1, r6
	str	r0, [r6]	@ unaligned
	str	r2, [r6, #8]	@ unaligned
	str	r3, [r6, #12]	@ unaligned
	bhi	.L90
	vadd.i32	q11, q14, q11
	ldr	r3, [r7, #64]
	vstr	d22, [r3, #16]
	vstr	d23, [r3, #24]
	b	.L14
.L16:
	subs	r3, r2, #1
	ldr	r2, [r7, #12]
	add	r2, r2, r9
	mov	r5, r2
	ldr	r2, [r7, #64]
	add	r2, r2, r3
	mov	r3, r2
.L24:
	ldrb	r1, [r0], #1	@ zero_extendqisi2
	ldrb	r2, [r3, #1]!	@ zero_extendqisi2
	cmp	r0, r5
	eor	r2, r2, r1
	strb	r2, [r4], #1
	bne	.L24
	adds	r7, r7, #124
	mov	sp, r7
	@ sp needed
	vldm	sp!, {d8-d15}
	pop	{r4, r5, r6, r7, r8, r9, r10, fp, pc}
.L26:
	str	r10, [r7, #16]
	b	.L2
.L90:
	mov	r3, r5
	ldr	r4, [r7, #56]
	ldr	r0, [r3, #16]!	@ unaligned
	add	lr, r1, #16
	mov	r5, r1
	vadd.i32	q11, q14, q11
	mov	r6, r4
	cmp	r9, #47
	ldr	r1, [r3, #4]	@ unaligned
	ldr	r2, [r3, #8]	@ unaligned
	ldr	r3, [r3, #12]	@ unaligned
	stmia	r6!, {r0, r1, r2, r3}
	ldr	r2, [r7, #64]
	vldr	d18, [r2, #64]
	vldr	d19, [r2, #72]
	veor	q11, q9, q11
	vstr	d22, [r2, #64]
	vstr	d23, [r2, #72]
	ldmia	r4!, {r0, r1, r2, r3}
	str	r0, [r5, #16]	@ unaligned
	str	r1, [lr, #4]	@ unaligned
	str	r2, [lr, #8]	@ unaligned
	str	r3, [lr, #12]	@ unaligned
	bhi	.L91
	vadd.i32	q8, q15, q8
	ldr	r3, [r7, #64]
	vstr	d16, [r3, #32]
	vstr	d17, [r3, #40]
	b	.L14
.L91:
	ldr	r3, [r7, #12]
	add	lr, r5, #32
	ldr	r4, [r7, #56]
	vadd.i32	q8, q15, q8
	ldr	r5, [r7, #64]
	vadd.i32	q13, q13, q12
	ldr	r0, [r3, #32]!	@ unaligned
	mov	r6, r4
	vstr	d26, [r5, #48]
	vstr	d27, [r5, #56]
	ldr	r1, [r3, #4]	@ unaligned
	ldr	r2, [r3, #8]	@ unaligned
	ldr	r3, [r3, #12]	@ unaligned
	stmia	r4!, {r0, r1, r2, r3}
	vldr	d18, [r5, #64]
	vldr	d19, [r5, #72]
	veor	q9, q9, q8
	ldr	r4, [r7, #16]
	vstr	d18, [r5, #64]
	vstr	d19, [r5, #72]
	ldmia	r6!, {r0, r1, r2, r3}
	str	r0, [r4, #32]	@ unaligned
	str	r1, [lr, #4]	@ unaligned
	str	r2, [lr, #8]	@ unaligned
	str	r3, [lr, #12]	@ unaligned
	b	.L14
.L98:
	.align	3
.L97:
	.word	1634760805
	.word	857760878
	.word	2036477234
	.word	1797285236
	.size	CRYPTO_chacha_20_neon, .-CRYPTO_chacha_20_neon
	.ident	"GCC: (Linaro GCC 2014.11) 4.9.3 20141031 (prerelease)"
	.section	.note.GNU-stack,"",%progbits
