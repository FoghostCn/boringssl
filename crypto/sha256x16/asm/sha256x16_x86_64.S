/* Copyright (c) 2018, Google Inc.
 *
 * Permission to use, copy, modify, and/or distribute this software for any
 * purpose with or without fee is hereby granted, provided that the above
 * copyright notice and this permission notice appear in all copies.
 *
 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY
 * SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN ACTION
 * OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF OR IN
 * CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE. */

/* Do not edit this file directly. Instead, see sha256_x86_64.c.disabled. */

	.text
	.file	"sha256x16_x86_64.c"
	.section	.rodata.cst16,"aM",@progbits,16
	.p2align	4               # -- Begin function sha256x16_avx_init
.LCPI0_0:
	.long	1779033703              # 0x6a09e667
	.long	1779033703              # 0x6a09e667
	.long	1779033703              # 0x6a09e667
	.long	1779033703              # 0x6a09e667
.LCPI0_1:
	.long	3144134277              # 0xbb67ae85
	.long	3144134277              # 0xbb67ae85
	.long	3144134277              # 0xbb67ae85
	.long	3144134277              # 0xbb67ae85
.LCPI0_2:
	.long	1013904242              # 0x3c6ef372
	.long	1013904242              # 0x3c6ef372
	.long	1013904242              # 0x3c6ef372
	.long	1013904242              # 0x3c6ef372
.LCPI0_3:
	.long	2773480762              # 0xa54ff53a
	.long	2773480762              # 0xa54ff53a
	.long	2773480762              # 0xa54ff53a
	.long	2773480762              # 0xa54ff53a
.LCPI0_4:
	.long	1359893119              # 0x510e527f
	.long	1359893119              # 0x510e527f
	.long	1359893119              # 0x510e527f
	.long	1359893119              # 0x510e527f
.LCPI0_5:
	.long	2600822924              # 0x9b05688c
	.long	2600822924              # 0x9b05688c
	.long	2600822924              # 0x9b05688c
	.long	2600822924              # 0x9b05688c
.LCPI0_6:
	.long	528734635               # 0x1f83d9ab
	.long	528734635               # 0x1f83d9ab
	.long	528734635               # 0x1f83d9ab
	.long	528734635               # 0x1f83d9ab
.LCPI0_7:
	.long	1541459225              # 0x5be0cd19
	.long	1541459225              # 0x5be0cd19
	.long	1541459225              # 0x5be0cd19
	.long	1541459225              # 0x5be0cd19
	.text
	.hidden	sha256x16_avx_init
	.globl	sha256x16_avx_init
	.p2align	4, 0x90
	.type	sha256x16_avx_init,@function
sha256x16_avx_init:                     # @sha256x16_avx_init
	.cfi_startproc
# %bb.0:
	vmovaps	.LCPI0_0(%rip), %xmm0   # xmm0 = [1779033703,1779033703,1779033703,1779033703]
	vmovaps	%xmm0, (%rdi)
	vmovaps	%xmm0, 128(%rdi)
	vmovaps	%xmm0, 256(%rdi)
	vmovaps	%xmm0, 384(%rdi)
	vmovaps	.LCPI0_1(%rip), %xmm0   # xmm0 = [3144134277,3144134277,3144134277,3144134277]
	vmovaps	%xmm0, 16(%rdi)
	vmovaps	%xmm0, 144(%rdi)
	vmovaps	%xmm0, 272(%rdi)
	vmovaps	%xmm0, 400(%rdi)
	vmovaps	.LCPI0_2(%rip), %xmm0   # xmm0 = [1013904242,1013904242,1013904242,1013904242]
	vmovaps	%xmm0, 32(%rdi)
	vmovaps	%xmm0, 160(%rdi)
	vmovaps	%xmm0, 288(%rdi)
	vmovaps	%xmm0, 416(%rdi)
	vmovaps	.LCPI0_3(%rip), %xmm0   # xmm0 = [2773480762,2773480762,2773480762,2773480762]
	vmovaps	%xmm0, 48(%rdi)
	vmovaps	%xmm0, 176(%rdi)
	vmovaps	%xmm0, 304(%rdi)
	vmovaps	%xmm0, 432(%rdi)
	vmovaps	.LCPI0_4(%rip), %xmm0   # xmm0 = [1359893119,1359893119,1359893119,1359893119]
	vmovaps	%xmm0, 64(%rdi)
	vmovaps	%xmm0, 192(%rdi)
	vmovaps	%xmm0, 320(%rdi)
	vmovaps	%xmm0, 448(%rdi)
	vmovaps	.LCPI0_5(%rip), %xmm0   # xmm0 = [2600822924,2600822924,2600822924,2600822924]
	vmovaps	%xmm0, 80(%rdi)
	vmovaps	%xmm0, 208(%rdi)
	vmovaps	%xmm0, 336(%rdi)
	vmovaps	%xmm0, 464(%rdi)
	vmovaps	.LCPI0_6(%rip), %xmm0   # xmm0 = [528734635,528734635,528734635,528734635]
	vmovaps	%xmm0, 96(%rdi)
	vmovaps	%xmm0, 224(%rdi)
	vmovaps	%xmm0, 352(%rdi)
	vmovaps	%xmm0, 480(%rdi)
	vmovaps	.LCPI0_7(%rip), %xmm0   # xmm0 = [1541459225,1541459225,1541459225,1541459225]
	vmovaps	%xmm0, 112(%rdi)
	vmovaps	%xmm0, 240(%rdi)
	vmovaps	%xmm0, 368(%rdi)
	vmovaps	%xmm0, 496(%rdi)
	retq
.Lfunc_end0:
	.size	sha256x16_avx_init, .Lfunc_end0-sha256x16_avx_init
	.cfi_endproc
                                        # -- End function
	.section	.rodata.cst16,"aM",@progbits,16
	.p2align	4               # -- Begin function sha256x16_avx_update
.LCPI1_0:
	.byte	3                       # 0x3
	.byte	2                       # 0x2
	.byte	1                       # 0x1
	.byte	0                       # 0x0
	.byte	7                       # 0x7
	.byte	6                       # 0x6
	.byte	5                       # 0x5
	.byte	4                       # 0x4
	.byte	11                      # 0xb
	.byte	10                      # 0xa
	.byte	9                       # 0x9
	.byte	8                       # 0x8
	.byte	15                      # 0xf
	.byte	14                      # 0xe
	.byte	13                      # 0xd
	.byte	12                      # 0xc
.LCPI1_1:
	.long	1116352408              # 0x428a2f98
	.long	1116352408              # 0x428a2f98
	.long	1116352408              # 0x428a2f98
	.long	1116352408              # 0x428a2f98
.LCPI1_2:
	.long	1899447441              # 0x71374491
	.long	1899447441              # 0x71374491
	.long	1899447441              # 0x71374491
	.long	1899447441              # 0x71374491
.LCPI1_3:
	.long	3049323471              # 0xb5c0fbcf
	.long	3049323471              # 0xb5c0fbcf
	.long	3049323471              # 0xb5c0fbcf
	.long	3049323471              # 0xb5c0fbcf
.LCPI1_4:
	.long	3921009573              # 0xe9b5dba5
	.long	3921009573              # 0xe9b5dba5
	.long	3921009573              # 0xe9b5dba5
	.long	3921009573              # 0xe9b5dba5
.LCPI1_5:
	.long	961987163               # 0x3956c25b
	.long	961987163               # 0x3956c25b
	.long	961987163               # 0x3956c25b
	.long	961987163               # 0x3956c25b
.LCPI1_6:
	.long	1508970993              # 0x59f111f1
	.long	1508970993              # 0x59f111f1
	.long	1508970993              # 0x59f111f1
	.long	1508970993              # 0x59f111f1
.LCPI1_7:
	.long	2453635748              # 0x923f82a4
	.long	2453635748              # 0x923f82a4
	.long	2453635748              # 0x923f82a4
	.long	2453635748              # 0x923f82a4
.LCPI1_8:
	.long	2870763221              # 0xab1c5ed5
	.long	2870763221              # 0xab1c5ed5
	.long	2870763221              # 0xab1c5ed5
	.long	2870763221              # 0xab1c5ed5
.LCPI1_9:
	.long	3624381080              # 0xd807aa98
	.long	3624381080              # 0xd807aa98
	.long	3624381080              # 0xd807aa98
	.long	3624381080              # 0xd807aa98
.LCPI1_10:
	.long	310598401               # 0x12835b01
	.long	310598401               # 0x12835b01
	.long	310598401               # 0x12835b01
	.long	310598401               # 0x12835b01
.LCPI1_11:
	.long	607225278               # 0x243185be
	.long	607225278               # 0x243185be
	.long	607225278               # 0x243185be
	.long	607225278               # 0x243185be
.LCPI1_12:
	.long	1426881987              # 0x550c7dc3
	.long	1426881987              # 0x550c7dc3
	.long	1426881987              # 0x550c7dc3
	.long	1426881987              # 0x550c7dc3
.LCPI1_13:
	.long	1925078388              # 0x72be5d74
	.long	1925078388              # 0x72be5d74
	.long	1925078388              # 0x72be5d74
	.long	1925078388              # 0x72be5d74
.LCPI1_14:
	.long	2162078206              # 0x80deb1fe
	.long	2162078206              # 0x80deb1fe
	.long	2162078206              # 0x80deb1fe
	.long	2162078206              # 0x80deb1fe
.LCPI1_15:
	.long	2614888103              # 0x9bdc06a7
	.long	2614888103              # 0x9bdc06a7
	.long	2614888103              # 0x9bdc06a7
	.long	2614888103              # 0x9bdc06a7
.LCPI1_16:
	.long	3248222580              # 0xc19bf174
	.long	3248222580              # 0xc19bf174
	.long	3248222580              # 0xc19bf174
	.long	3248222580              # 0xc19bf174
.LCPI1_17:
	.long	3835390401              # 0xe49b69c1
	.long	3835390401              # 0xe49b69c1
	.long	3835390401              # 0xe49b69c1
	.long	3835390401              # 0xe49b69c1
.LCPI1_18:
	.long	4022224774              # 0xefbe4786
	.long	4022224774              # 0xefbe4786
	.long	4022224774              # 0xefbe4786
	.long	4022224774              # 0xefbe4786
.LCPI1_19:
	.long	264347078               # 0xfc19dc6
	.long	264347078               # 0xfc19dc6
	.long	264347078               # 0xfc19dc6
	.long	264347078               # 0xfc19dc6
.LCPI1_20:
	.long	604807628               # 0x240ca1cc
	.long	604807628               # 0x240ca1cc
	.long	604807628               # 0x240ca1cc
	.long	604807628               # 0x240ca1cc
.LCPI1_21:
	.long	770255983               # 0x2de92c6f
	.long	770255983               # 0x2de92c6f
	.long	770255983               # 0x2de92c6f
	.long	770255983               # 0x2de92c6f
.LCPI1_22:
	.long	1249150122              # 0x4a7484aa
	.long	1249150122              # 0x4a7484aa
	.long	1249150122              # 0x4a7484aa
	.long	1249150122              # 0x4a7484aa
.LCPI1_23:
	.long	1555081692              # 0x5cb0a9dc
	.long	1555081692              # 0x5cb0a9dc
	.long	1555081692              # 0x5cb0a9dc
	.long	1555081692              # 0x5cb0a9dc
.LCPI1_24:
	.long	1996064986              # 0x76f988da
	.long	1996064986              # 0x76f988da
	.long	1996064986              # 0x76f988da
	.long	1996064986              # 0x76f988da
.LCPI1_25:
	.long	2554220882              # 0x983e5152
	.long	2554220882              # 0x983e5152
	.long	2554220882              # 0x983e5152
	.long	2554220882              # 0x983e5152
.LCPI1_26:
	.long	2821834349              # 0xa831c66d
	.long	2821834349              # 0xa831c66d
	.long	2821834349              # 0xa831c66d
	.long	2821834349              # 0xa831c66d
.LCPI1_27:
	.long	2952996808              # 0xb00327c8
	.long	2952996808              # 0xb00327c8
	.long	2952996808              # 0xb00327c8
	.long	2952996808              # 0xb00327c8
.LCPI1_28:
	.long	3210313671              # 0xbf597fc7
	.long	3210313671              # 0xbf597fc7
	.long	3210313671              # 0xbf597fc7
	.long	3210313671              # 0xbf597fc7
.LCPI1_29:
	.long	3336571891              # 0xc6e00bf3
	.long	3336571891              # 0xc6e00bf3
	.long	3336571891              # 0xc6e00bf3
	.long	3336571891              # 0xc6e00bf3
.LCPI1_30:
	.long	3584528711              # 0xd5a79147
	.long	3584528711              # 0xd5a79147
	.long	3584528711              # 0xd5a79147
	.long	3584528711              # 0xd5a79147
.LCPI1_31:
	.long	113926993               # 0x6ca6351
	.long	113926993               # 0x6ca6351
	.long	113926993               # 0x6ca6351
	.long	113926993               # 0x6ca6351
.LCPI1_32:
	.long	338241895               # 0x14292967
	.long	338241895               # 0x14292967
	.long	338241895               # 0x14292967
	.long	338241895               # 0x14292967
.LCPI1_33:
	.long	666307205               # 0x27b70a85
	.long	666307205               # 0x27b70a85
	.long	666307205               # 0x27b70a85
	.long	666307205               # 0x27b70a85
.LCPI1_34:
	.long	773529912               # 0x2e1b2138
	.long	773529912               # 0x2e1b2138
	.long	773529912               # 0x2e1b2138
	.long	773529912               # 0x2e1b2138
.LCPI1_35:
	.long	1294757372              # 0x4d2c6dfc
	.long	1294757372              # 0x4d2c6dfc
	.long	1294757372              # 0x4d2c6dfc
	.long	1294757372              # 0x4d2c6dfc
.LCPI1_36:
	.long	1396182291              # 0x53380d13
	.long	1396182291              # 0x53380d13
	.long	1396182291              # 0x53380d13
	.long	1396182291              # 0x53380d13
.LCPI1_37:
	.long	1695183700              # 0x650a7354
	.long	1695183700              # 0x650a7354
	.long	1695183700              # 0x650a7354
	.long	1695183700              # 0x650a7354
.LCPI1_38:
	.long	1986661051              # 0x766a0abb
	.long	1986661051              # 0x766a0abb
	.long	1986661051              # 0x766a0abb
	.long	1986661051              # 0x766a0abb
.LCPI1_39:
	.long	2177026350              # 0x81c2c92e
	.long	2177026350              # 0x81c2c92e
	.long	2177026350              # 0x81c2c92e
	.long	2177026350              # 0x81c2c92e
.LCPI1_40:
	.long	2456956037              # 0x92722c85
	.long	2456956037              # 0x92722c85
	.long	2456956037              # 0x92722c85
	.long	2456956037              # 0x92722c85
.LCPI1_41:
	.long	2730485921              # 0xa2bfe8a1
	.long	2730485921              # 0xa2bfe8a1
	.long	2730485921              # 0xa2bfe8a1
	.long	2730485921              # 0xa2bfe8a1
.LCPI1_42:
	.long	2820302411              # 0xa81a664b
	.long	2820302411              # 0xa81a664b
	.long	2820302411              # 0xa81a664b
	.long	2820302411              # 0xa81a664b
.LCPI1_43:
	.long	3259730800              # 0xc24b8b70
	.long	3259730800              # 0xc24b8b70
	.long	3259730800              # 0xc24b8b70
	.long	3259730800              # 0xc24b8b70
.LCPI1_44:
	.long	3345764771              # 0xc76c51a3
	.long	3345764771              # 0xc76c51a3
	.long	3345764771              # 0xc76c51a3
	.long	3345764771              # 0xc76c51a3
.LCPI1_45:
	.long	3516065817              # 0xd192e819
	.long	3516065817              # 0xd192e819
	.long	3516065817              # 0xd192e819
	.long	3516065817              # 0xd192e819
.LCPI1_46:
	.long	3600352804              # 0xd6990624
	.long	3600352804              # 0xd6990624
	.long	3600352804              # 0xd6990624
	.long	3600352804              # 0xd6990624
.LCPI1_47:
	.long	4094571909              # 0xf40e3585
	.long	4094571909              # 0xf40e3585
	.long	4094571909              # 0xf40e3585
	.long	4094571909              # 0xf40e3585
.LCPI1_48:
	.long	275423344               # 0x106aa070
	.long	275423344               # 0x106aa070
	.long	275423344               # 0x106aa070
	.long	275423344               # 0x106aa070
.LCPI1_49:
	.long	430227734               # 0x19a4c116
	.long	430227734               # 0x19a4c116
	.long	430227734               # 0x19a4c116
	.long	430227734               # 0x19a4c116
.LCPI1_50:
	.long	506948616               # 0x1e376c08
	.long	506948616               # 0x1e376c08
	.long	506948616               # 0x1e376c08
	.long	506948616               # 0x1e376c08
.LCPI1_51:
	.long	659060556               # 0x2748774c
	.long	659060556               # 0x2748774c
	.long	659060556               # 0x2748774c
	.long	659060556               # 0x2748774c
.LCPI1_52:
	.long	883997877               # 0x34b0bcb5
	.long	883997877               # 0x34b0bcb5
	.long	883997877               # 0x34b0bcb5
	.long	883997877               # 0x34b0bcb5
.LCPI1_53:
	.long	958139571               # 0x391c0cb3
	.long	958139571               # 0x391c0cb3
	.long	958139571               # 0x391c0cb3
	.long	958139571               # 0x391c0cb3
.LCPI1_54:
	.long	1322822218              # 0x4ed8aa4a
	.long	1322822218              # 0x4ed8aa4a
	.long	1322822218              # 0x4ed8aa4a
	.long	1322822218              # 0x4ed8aa4a
.LCPI1_55:
	.long	1537002063              # 0x5b9cca4f
	.long	1537002063              # 0x5b9cca4f
	.long	1537002063              # 0x5b9cca4f
	.long	1537002063              # 0x5b9cca4f
.LCPI1_56:
	.long	1747873779              # 0x682e6ff3
	.long	1747873779              # 0x682e6ff3
	.long	1747873779              # 0x682e6ff3
	.long	1747873779              # 0x682e6ff3
.LCPI1_57:
	.long	1955562222              # 0x748f82ee
	.long	1955562222              # 0x748f82ee
	.long	1955562222              # 0x748f82ee
	.long	1955562222              # 0x748f82ee
.LCPI1_58:
	.long	2024104815              # 0x78a5636f
	.long	2024104815              # 0x78a5636f
	.long	2024104815              # 0x78a5636f
	.long	2024104815              # 0x78a5636f
.LCPI1_59:
	.long	2227730452              # 0x84c87814
	.long	2227730452              # 0x84c87814
	.long	2227730452              # 0x84c87814
	.long	2227730452              # 0x84c87814
.LCPI1_60:
	.long	2361852424              # 0x8cc70208
	.long	2361852424              # 0x8cc70208
	.long	2361852424              # 0x8cc70208
	.long	2361852424              # 0x8cc70208
.LCPI1_61:
	.long	2428436474              # 0x90befffa
	.long	2428436474              # 0x90befffa
	.long	2428436474              # 0x90befffa
	.long	2428436474              # 0x90befffa
.LCPI1_62:
	.long	2756734187              # 0xa4506ceb
	.long	2756734187              # 0xa4506ceb
	.long	2756734187              # 0xa4506ceb
	.long	2756734187              # 0xa4506ceb
.LCPI1_63:
	.long	3204031479              # 0xbef9a3f7
	.long	3204031479              # 0xbef9a3f7
	.long	3204031479              # 0xbef9a3f7
	.long	3204031479              # 0xbef9a3f7
.LCPI1_64:
	.long	3329325298              # 0xc67178f2
	.long	3329325298              # 0xc67178f2
	.long	3329325298              # 0xc67178f2
	.long	3329325298              # 0xc67178f2
	.text
	.hidden	sha256x16_avx_update
	.globl	sha256x16_avx_update
	.p2align	4, 0x90
	.type	sha256x16_avx_update,@function
sha256x16_avx_update:                   # @sha256x16_avx_update
	.cfi_startproc
# %bb.0:
	subq	$280, %rsp              # imm = 0x118
	.cfi_def_cfa_offset 288
	testq	%rdx, %rdx
	je	.LBB1_5
# %bb.1:
	addq	$112, %rdi
	xorl	%r8d, %r8d
	vmovdqa	.LCPI1_0(%rip), %xmm11  # xmm11 = [3,2,1,0,7,6,5,4,11,10,9,8,15,14,13,12]
	vmovdqa	.LCPI1_1(%rip), %xmm9   # xmm9 = [1116352408,1116352408,1116352408,1116352408]
	.p2align	4, 0x90
.LBB1_2:                                # =>This Loop Header: Depth=1
                                        #     Child Loop BB1_3 Depth 2
	movq	%rdi, %rcx
	xorl	%eax, %eax
	.p2align	4, 0x90
.LBB1_3:                                #   Parent Loop BB1_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovdqa	-112(%rcx), %xmm8
	vmovdqa	-96(%rcx), %xmm14
	vmovdqa	-80(%rcx), %xmm13
	vmovdqa	-64(%rcx), %xmm10
	vmovdqa	%xmm10, 240(%rsp)       # 16-byte Spill
	vmovdqa	-48(%rcx), %xmm5
	vmovdqa	-32(%rcx), %xmm6
	vmovdqa	-16(%rcx), %xmm15
	vmovdqa	(%rcx), %xmm7
	vmovdqa	%xmm7, 256(%rsp)        # 16-byte Spill
	vmovdqu	(%rsi,%rax,2), %xmm1
	vmovdqu	64(%rsi,%rax,2), %xmm12
	vpshufb	%xmm11, %xmm1, %xmm0
	vmovdqa	%xmm0, -64(%rsp)        # 16-byte Spill
	vpxor	%xmm15, %xmm6, %xmm1
	vpand	%xmm5, %xmm1, %xmm1
	vpxor	%xmm15, %xmm1, %xmm1
	vmovdqa	%xmm15, 224(%rsp)       # 16-byte Spill
	vpslld	$27, %xmm5, %xmm2
	vpsrld	$5, %xmm5, %xmm3
	vpor	%xmm2, %xmm3, %xmm2
	vpxor	%xmm5, %xmm2, %xmm2
	vpslld	$26, %xmm2, %xmm3
	vpsrld	$6, %xmm2, %xmm2
	vpor	%xmm3, %xmm2, %xmm2
	vpslld	$7, %xmm5, %xmm3
	vpsrld	$25, %xmm5, %xmm4
	vpor	%xmm3, %xmm4, %xmm3
	vpxor	%xmm3, %xmm2, %xmm2
	vpaddd	%xmm1, %xmm7, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm1
	vpaddd	%xmm1, %xmm2, %xmm1
	vpaddd	%xmm9, %xmm1, %xmm1
	vpaddd	%xmm10, %xmm1, %xmm9
	vpslld	$30, %xmm8, %xmm2
	vpsrld	$2, %xmm8, %xmm3
	vpor	%xmm2, %xmm3, %xmm2
	vpslld	$19, %xmm8, %xmm3
	vpsrld	$13, %xmm8, %xmm4
	vpor	%xmm3, %xmm4, %xmm3
	vpxor	%xmm3, %xmm2, %xmm2
	vpslld	$10, %xmm8, %xmm3
	vpsrld	$22, %xmm8, %xmm4
	vpor	%xmm3, %xmm4, %xmm3
	vpxor	%xmm3, %xmm2, %xmm2
	vpxor	%xmm13, %xmm14, %xmm3
	vpand	%xmm3, %xmm8, %xmm3
	vpand	%xmm13, %xmm14, %xmm4
	vmovdqa	%xmm13, 208(%rsp)       # 16-byte Spill
	vpxor	%xmm4, %xmm3, %xmm3
	vpaddd	%xmm3, %xmm2, %xmm2
	vpaddd	%xmm1, %xmm2, %xmm1
	vpshufb	%xmm11, %xmm12, %xmm7
	vmovdqa	%xmm7, -80(%rsp)        # 16-byte Spill
	vpxor	%xmm6, %xmm5, %xmm0
	vpand	%xmm9, %xmm0, %xmm0
	vpxor	%xmm6, %xmm0, %xmm0
	vmovdqa	%xmm6, 192(%rsp)        # 16-byte Spill
	vpslld	$27, %xmm9, %xmm2
	vpsrld	$5, %xmm9, %xmm3
	vpor	%xmm2, %xmm3, %xmm2
	vpxor	%xmm9, %xmm2, %xmm2
	vpslld	$26, %xmm2, %xmm3
	vpsrld	$6, %xmm2, %xmm2
	vpor	%xmm3, %xmm2, %xmm2
	vpslld	$7, %xmm9, %xmm3
	vpsrld	$25, %xmm9, %xmm4
	vpor	%xmm3, %xmm4, %xmm3
	vpxor	%xmm3, %xmm2, %xmm2
	vpaddd	%xmm15, %xmm0, %xmm0
	vpaddd	%xmm7, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm2, %xmm0
	vpaddd	.LCPI1_2(%rip), %xmm0, %xmm0
	vpslld	$30, %xmm1, %xmm2
	vpsrld	$2, %xmm1, %xmm3
	vpor	%xmm2, %xmm3, %xmm2
	vpslld	$19, %xmm1, %xmm3
	vpsrld	$13, %xmm1, %xmm4
	vpor	%xmm3, %xmm4, %xmm3
	vpxor	%xmm3, %xmm2, %xmm2
	vpslld	$10, %xmm1, %xmm3
	vpsrld	$22, %xmm1, %xmm4
	vpor	%xmm3, %xmm4, %xmm3
	vpxor	%xmm3, %xmm2, %xmm2
	vpxor	%xmm14, %xmm8, %xmm3
	vpand	%xmm1, %xmm3, %xmm3
	vpand	%xmm14, %xmm8, %xmm4
	vmovdqa	%xmm8, %xmm10
	vmovdqa	%xmm14, %xmm8
	vmovdqa	%xmm8, 176(%rsp)        # 16-byte Spill
	vpxor	%xmm4, %xmm3, %xmm3
	vpaddd	%xmm13, %xmm0, %xmm13
	vpaddd	%xmm3, %xmm2, %xmm2
	vpaddd	%xmm0, %xmm2, %xmm3
	vpslld	$27, %xmm13, %xmm0
	vpsrld	$5, %xmm13, %xmm2
	vpor	%xmm0, %xmm2, %xmm0
	vpxor	%xmm13, %xmm0, %xmm0
	vpslld	$26, %xmm0, %xmm2
	vpsrld	$6, %xmm0, %xmm0
	vpor	%xmm2, %xmm0, %xmm0
	vpslld	$7, %xmm13, %xmm2
	vpsrld	$25, %xmm13, %xmm4
	vpor	%xmm2, %xmm4, %xmm2
	vpxor	%xmm2, %xmm0, %xmm0
	vmovdqu	128(%rsi,%rax,2), %xmm2
	vpshufb	%xmm11, %xmm2, %xmm14
	vpxor	%xmm9, %xmm5, %xmm2
	vpand	%xmm13, %xmm2, %xmm2
	vpxor	%xmm5, %xmm2, %xmm2
	vmovdqa	%xmm5, %xmm15
	vmovdqa	%xmm15, 160(%rsp)       # 16-byte Spill
	vpaddd	%xmm6, %xmm2, %xmm2
	vpaddd	%xmm14, %xmm2, %xmm2
	vpaddd	%xmm2, %xmm0, %xmm0
	vpslld	$30, %xmm3, %xmm2
	vpsrld	$2, %xmm3, %xmm4
	vpor	%xmm2, %xmm4, %xmm2
	vpslld	$19, %xmm3, %xmm4
	vpsrld	$13, %xmm3, %xmm7
	vpor	%xmm4, %xmm7, %xmm4
	vpxor	%xmm4, %xmm2, %xmm2
	vpslld	$10, %xmm3, %xmm4
	vpsrld	$22, %xmm3, %xmm7
	vpor	%xmm4, %xmm7, %xmm4
	vpxor	%xmm4, %xmm2, %xmm2
	vpxor	%xmm1, %xmm10, %xmm4
	vpand	%xmm3, %xmm4, %xmm4
	vpand	%xmm1, %xmm10, %xmm7
	vmovdqa	%xmm10, %xmm6
	vmovdqa	%xmm6, 144(%rsp)        # 16-byte Spill
	vpxor	%xmm7, %xmm4, %xmm4
	vpaddd	%xmm4, %xmm2, %xmm2
	vpaddd	.LCPI1_3(%rip), %xmm0, %xmm0
	vpaddd	%xmm0, %xmm2, %xmm2
	vpaddd	%xmm8, %xmm0, %xmm4
	vpslld	$27, %xmm4, %xmm0
	vpsrld	$5, %xmm4, %xmm7
	vpor	%xmm0, %xmm7, %xmm0
	vpxor	%xmm4, %xmm0, %xmm0
	vpslld	$26, %xmm0, %xmm7
	vpsrld	$6, %xmm0, %xmm0
	vpor	%xmm7, %xmm0, %xmm0
	vpslld	$7, %xmm4, %xmm7
	vpsrld	$25, %xmm4, %xmm5
	vpor	%xmm7, %xmm5, %xmm5
	vpxor	%xmm5, %xmm0, %xmm0
	vmovdqu	192(%rsi,%rax,2), %xmm5
	vpshufb	%xmm11, %xmm5, %xmm7
	vmovdqa	%xmm7, -112(%rsp)       # 16-byte Spill
	vpxor	%xmm9, %xmm13, %xmm5
	vpand	%xmm4, %xmm5, %xmm5
	vpxor	%xmm9, %xmm5, %xmm5
	vpaddd	%xmm15, %xmm5, %xmm5
	vpaddd	%xmm7, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm0, %xmm10
	vpslld	$30, %xmm2, %xmm5
	vpsrld	$2, %xmm2, %xmm7
	vpor	%xmm5, %xmm7, %xmm5
	vpslld	$19, %xmm2, %xmm7
	vpsrld	$13, %xmm2, %xmm0
	vpor	%xmm7, %xmm0, %xmm0
	vpxor	%xmm0, %xmm5, %xmm0
	vpslld	$10, %xmm2, %xmm5
	vpsrld	$22, %xmm2, %xmm7
	vpor	%xmm5, %xmm7, %xmm5
	vpxor	%xmm5, %xmm0, %xmm0
	vpxor	%xmm1, %xmm3, %xmm5
	vpand	%xmm5, %xmm2, %xmm5
	vpand	%xmm1, %xmm3, %xmm7
	vpxor	%xmm7, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm0, %xmm0
	vpaddd	.LCPI1_4(%rip), %xmm10, %xmm5
	vpaddd	%xmm5, %xmm0, %xmm0
	vpaddd	%xmm6, %xmm5, %xmm12
	vpslld	$27, %xmm12, %xmm5
	vpsrld	$5, %xmm12, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm12, %xmm5, %xmm5
	vpslld	$26, %xmm5, %xmm6
	vpsrld	$6, %xmm5, %xmm5
	vpor	%xmm6, %xmm5, %xmm10
	vpslld	$7, %xmm12, %xmm6
	vpsrld	$25, %xmm12, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpxor	%xmm5, %xmm10, %xmm10
	vpxor	%xmm13, %xmm4, %xmm6
	vpand	%xmm12, %xmm6, %xmm6
	vpxor	%xmm13, %xmm6, %xmm6
	vpaddd	%xmm9, %xmm6, %xmm6
	vmovdqu	256(%rsi,%rax,2), %xmm5
	vpshufb	%xmm11, %xmm5, %xmm5
	vmovdqa	%xmm5, -96(%rsp)        # 16-byte Spill
	vpaddd	%xmm5, %xmm6, %xmm5
	vpaddd	%xmm5, %xmm10, %xmm9
	vpslld	$30, %xmm0, %xmm6
	vpsrld	$2, %xmm0, %xmm5
	vpor	%xmm6, %xmm5, %xmm10
	vpslld	$19, %xmm0, %xmm6
	vpsrld	$13, %xmm0, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpxor	%xmm5, %xmm10, %xmm10
	vpslld	$10, %xmm0, %xmm6
	vpsrld	$22, %xmm0, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpxor	%xmm5, %xmm10, %xmm10
	vpxor	%xmm3, %xmm2, %xmm6
	vpand	%xmm6, %xmm0, %xmm6
	vpand	%xmm3, %xmm2, %xmm5
	vpxor	%xmm5, %xmm6, %xmm5
	vpaddd	%xmm5, %xmm10, %xmm6
	vpaddd	.LCPI1_5(%rip), %xmm9, %xmm5
	vpaddd	%xmm1, %xmm5, %xmm9
	vpaddd	%xmm5, %xmm6, %xmm1
	vpslld	$27, %xmm9, %xmm5
	vpsrld	$5, %xmm9, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm9, %xmm5, %xmm5
	vpslld	$26, %xmm5, %xmm6
	vpsrld	$6, %xmm5, %xmm5
	vpor	%xmm6, %xmm5, %xmm10
	vpslld	$7, %xmm9, %xmm6
	vpsrld	$25, %xmm9, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpxor	%xmm5, %xmm10, %xmm10
	vpxor	%xmm4, %xmm12, %xmm6
	vpand	%xmm9, %xmm6, %xmm6
	vpxor	%xmm4, %xmm6, %xmm6
	vpaddd	%xmm13, %xmm6, %xmm6
	vmovdqu	320(%rsi,%rax,2), %xmm5
	vpshufb	%xmm11, %xmm5, %xmm5
	vmovdqa	%xmm5, -128(%rsp)       # 16-byte Spill
	vpaddd	%xmm5, %xmm6, %xmm5
	vpaddd	%xmm5, %xmm10, %xmm10
	vpslld	$30, %xmm1, %xmm6
	vpsrld	$2, %xmm1, %xmm5
	vpor	%xmm6, %xmm5, %xmm13
	vpslld	$19, %xmm1, %xmm6
	vpsrld	$13, %xmm1, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpxor	%xmm5, %xmm13, %xmm13
	vpslld	$10, %xmm1, %xmm6
	vpsrld	$22, %xmm1, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpxor	%xmm5, %xmm13, %xmm13
	vpxor	%xmm2, %xmm0, %xmm6
	vpand	%xmm6, %xmm1, %xmm6
	vpand	%xmm2, %xmm0, %xmm5
	vpxor	%xmm5, %xmm6, %xmm5
	vpaddd	%xmm5, %xmm13, %xmm5
	vpaddd	.LCPI1_6(%rip), %xmm10, %xmm6
	vpaddd	%xmm3, %xmm6, %xmm10
	vpaddd	%xmm6, %xmm5, %xmm3
	vpslld	$27, %xmm10, %xmm5
	vpsrld	$5, %xmm10, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm10, %xmm5, %xmm5
	vpslld	$26, %xmm5, %xmm6
	vpsrld	$6, %xmm5, %xmm5
	vpor	%xmm6, %xmm5, %xmm13
	vpslld	$7, %xmm10, %xmm6
	vpsrld	$25, %xmm10, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpxor	%xmm5, %xmm13, %xmm5
	vpxor	%xmm12, %xmm9, %xmm6
	vpand	%xmm10, %xmm6, %xmm6
	vpxor	%xmm12, %xmm6, %xmm6
	vpaddd	%xmm4, %xmm6, %xmm4
	vmovdqu	384(%rsi,%rax,2), %xmm6
	vpshufb	%xmm11, %xmm6, %xmm6
	vmovdqa	%xmm6, 96(%rsp)         # 16-byte Spill
	vpaddd	%xmm6, %xmm4, %xmm4
	vpaddd	%xmm4, %xmm5, %xmm13
	vpslld	$30, %xmm3, %xmm5
	vpsrld	$2, %xmm3, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpslld	$19, %xmm3, %xmm6
	vpsrld	$13, %xmm3, %xmm4
	vpor	%xmm6, %xmm4, %xmm4
	vpxor	%xmm4, %xmm5, %xmm4
	vpslld	$10, %xmm3, %xmm5
	vpsrld	$22, %xmm3, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm0, %xmm1, %xmm5
	vpand	%xmm5, %xmm3, %xmm5
	vpand	%xmm0, %xmm1, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm4
	vpaddd	.LCPI1_7(%rip), %xmm13, %xmm5
	vpaddd	%xmm2, %xmm5, %xmm8
	vpaddd	%xmm5, %xmm4, %xmm4
	vpslld	$27, %xmm8, %xmm5
	vpsrld	$5, %xmm8, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm8, %xmm5, %xmm5
	vpslld	$26, %xmm5, %xmm6
	vpsrld	$6, %xmm5, %xmm5
	vpor	%xmm6, %xmm5, %xmm13
	vpslld	$7, %xmm8, %xmm6
	vpsrld	$25, %xmm8, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpxor	%xmm5, %xmm13, %xmm5
	vpxor	%xmm9, %xmm10, %xmm6
	vpand	%xmm8, %xmm6, %xmm6
	vpxor	%xmm9, %xmm6, %xmm6
	vpaddd	%xmm12, %xmm6, %xmm6
	vmovdqu	448(%rsi,%rax,2), %xmm7
	vpshufb	%xmm11, %xmm7, %xmm2
	vmovdqa	%xmm2, 80(%rsp)         # 16-byte Spill
	vpaddd	%xmm2, %xmm6, %xmm6
	vpaddd	%xmm6, %xmm5, %xmm13
	vpslld	$30, %xmm4, %xmm6
	vpsrld	$2, %xmm4, %xmm7
	vpor	%xmm6, %xmm7, %xmm6
	vpslld	$19, %xmm4, %xmm7
	vpsrld	$13, %xmm4, %xmm5
	vpor	%xmm7, %xmm5, %xmm5
	vpxor	%xmm5, %xmm6, %xmm5
	vpslld	$10, %xmm4, %xmm6
	vpsrld	$22, %xmm4, %xmm7
	vpor	%xmm6, %xmm7, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpxor	%xmm1, %xmm3, %xmm6
	vpand	%xmm6, %xmm4, %xmm6
	vpand	%xmm1, %xmm3, %xmm7
	vpxor	%xmm7, %xmm6, %xmm6
	vpaddd	%xmm6, %xmm5, %xmm5
	vpaddd	.LCPI1_8(%rip), %xmm13, %xmm6
	vpaddd	%xmm0, %xmm6, %xmm7
	vpaddd	%xmm6, %xmm5, %xmm0
	vpslld	$27, %xmm7, %xmm5
	vpsrld	$5, %xmm7, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm7, %xmm5, %xmm5
	vpslld	$26, %xmm5, %xmm6
	vpsrld	$6, %xmm5, %xmm5
	vpor	%xmm6, %xmm5, %xmm13
	vpslld	$7, %xmm7, %xmm6
	vpsrld	$25, %xmm7, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpxor	%xmm5, %xmm13, %xmm13
	vpxor	%xmm10, %xmm8, %xmm6
	vpand	%xmm7, %xmm6, %xmm6
	vpxor	%xmm10, %xmm6, %xmm6
	vpaddd	%xmm9, %xmm6, %xmm6
	vmovdqu	512(%rsi,%rax,2), %xmm5
	vpshufb	%xmm11, %xmm5, %xmm2
	vmovdqa	%xmm2, 64(%rsp)         # 16-byte Spill
	vpaddd	%xmm2, %xmm6, %xmm5
	vpaddd	%xmm5, %xmm13, %xmm9
	vpslld	$30, %xmm0, %xmm6
	vpsrld	$2, %xmm0, %xmm5
	vpor	%xmm6, %xmm5, %xmm13
	vpslld	$19, %xmm0, %xmm6
	vpsrld	$13, %xmm0, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpxor	%xmm5, %xmm13, %xmm13
	vpslld	$10, %xmm0, %xmm6
	vpsrld	$22, %xmm0, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpxor	%xmm5, %xmm13, %xmm13
	vpxor	%xmm3, %xmm4, %xmm6
	vpand	%xmm6, %xmm0, %xmm6
	vpand	%xmm3, %xmm4, %xmm5
	vpxor	%xmm5, %xmm6, %xmm5
	vpaddd	%xmm5, %xmm13, %xmm5
	vpaddd	.LCPI1_9(%rip), %xmm9, %xmm6
	vpaddd	%xmm1, %xmm6, %xmm9
	vpaddd	%xmm6, %xmm5, %xmm1
	vpslld	$27, %xmm9, %xmm5
	vpsrld	$5, %xmm9, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm9, %xmm5, %xmm5
	vpslld	$26, %xmm5, %xmm6
	vpsrld	$6, %xmm5, %xmm5
	vpor	%xmm6, %xmm5, %xmm13
	vpslld	$7, %xmm9, %xmm6
	vpsrld	$25, %xmm9, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpxor	%xmm5, %xmm13, %xmm15
	vpxor	%xmm8, %xmm7, %xmm6
	vpand	%xmm9, %xmm6, %xmm6
	vpxor	%xmm8, %xmm6, %xmm6
	vpaddd	%xmm10, %xmm6, %xmm6
	vmovdqu	576(%rsi,%rax,2), %xmm5
	vpshufb	%xmm11, %xmm5, %xmm2
	vmovdqa	%xmm2, 16(%rsp)         # 16-byte Spill
	vpaddd	%xmm2, %xmm6, %xmm5
	vpaddd	%xmm5, %xmm15, %xmm10
	vpslld	$30, %xmm1, %xmm6
	vpsrld	$2, %xmm1, %xmm5
	vpor	%xmm6, %xmm5, %xmm15
	vpslld	$19, %xmm1, %xmm6
	vpsrld	$13, %xmm1, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpxor	%xmm5, %xmm15, %xmm15
	vpslld	$10, %xmm1, %xmm6
	vpsrld	$22, %xmm1, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpxor	%xmm5, %xmm15, %xmm15
	vpxor	%xmm4, %xmm0, %xmm6
	vpand	%xmm6, %xmm1, %xmm6
	vpand	%xmm4, %xmm0, %xmm5
	vpxor	%xmm5, %xmm6, %xmm5
	vpaddd	%xmm5, %xmm15, %xmm5
	vpaddd	.LCPI1_10(%rip), %xmm10, %xmm6
	vpaddd	%xmm3, %xmm6, %xmm15
	vpaddd	%xmm6, %xmm5, %xmm3
	vpslld	$27, %xmm15, %xmm5
	vpsrld	$5, %xmm15, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm15, %xmm5, %xmm5
	vpslld	$26, %xmm5, %xmm6
	vpsrld	$6, %xmm5, %xmm5
	vpor	%xmm6, %xmm5, %xmm10
	vpslld	$7, %xmm15, %xmm6
	vpsrld	$25, %xmm15, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpxor	%xmm5, %xmm10, %xmm5
	vpxor	%xmm7, %xmm9, %xmm6
	vpand	%xmm15, %xmm6, %xmm6
	vpxor	%xmm7, %xmm6, %xmm6
	vpaddd	%xmm8, %xmm6, %xmm2
	vmovdqu	640(%rsi,%rax,2), %xmm6
	vpshufb	%xmm11, %xmm6, %xmm6
	vmovdqa	%xmm6, (%rsp)           # 16-byte Spill
	vpaddd	%xmm6, %xmm2, %xmm2
	vpaddd	%xmm2, %xmm5, %xmm10
	vpslld	$30, %xmm3, %xmm5
	vpsrld	$2, %xmm3, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpslld	$19, %xmm3, %xmm6
	vpsrld	$13, %xmm3, %xmm2
	vpor	%xmm6, %xmm2, %xmm2
	vpxor	%xmm2, %xmm5, %xmm2
	vpslld	$10, %xmm3, %xmm5
	vpsrld	$22, %xmm3, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm2, %xmm2
	vpxor	%xmm0, %xmm1, %xmm5
	vpand	%xmm5, %xmm3, %xmm5
	vpand	%xmm0, %xmm1, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm2, %xmm5
	vpaddd	.LCPI1_11(%rip), %xmm10, %xmm6
	vpaddd	%xmm4, %xmm6, %xmm2
	vpaddd	%xmm6, %xmm5, %xmm4
	vpslld	$27, %xmm2, %xmm5
	vpsrld	$5, %xmm2, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm2, %xmm5, %xmm5
	vpslld	$26, %xmm5, %xmm6
	vpsrld	$6, %xmm5, %xmm5
	vpor	%xmm6, %xmm5, %xmm10
	vpslld	$7, %xmm2, %xmm6
	vpsrld	$25, %xmm2, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpxor	%xmm5, %xmm10, %xmm5
	vpxor	%xmm9, %xmm15, %xmm6
	vpand	%xmm2, %xmm6, %xmm6
	vpxor	%xmm9, %xmm6, %xmm6
	vpaddd	%xmm7, %xmm6, %xmm6
	vmovdqu	704(%rsi,%rax,2), %xmm7
	vpshufb	%xmm11, %xmm7, %xmm7
	vmovdqa	%xmm7, 32(%rsp)         # 16-byte Spill
	vpaddd	%xmm7, %xmm6, %xmm6
	vpaddd	%xmm6, %xmm5, %xmm10
	vpslld	$30, %xmm4, %xmm6
	vpsrld	$2, %xmm4, %xmm7
	vpor	%xmm6, %xmm7, %xmm6
	vpslld	$19, %xmm4, %xmm7
	vpsrld	$13, %xmm4, %xmm5
	vpor	%xmm7, %xmm5, %xmm5
	vpxor	%xmm5, %xmm6, %xmm5
	vpslld	$10, %xmm4, %xmm6
	vpsrld	$22, %xmm4, %xmm7
	vpor	%xmm6, %xmm7, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpxor	%xmm1, %xmm3, %xmm6
	vpand	%xmm6, %xmm4, %xmm6
	vpand	%xmm1, %xmm3, %xmm7
	vpxor	%xmm7, %xmm6, %xmm6
	vpaddd	%xmm6, %xmm5, %xmm5
	vpaddd	.LCPI1_12(%rip), %xmm10, %xmm6
	vpaddd	%xmm0, %xmm6, %xmm13
	vpaddd	%xmm6, %xmm5, %xmm5
	vpslld	$27, %xmm13, %xmm6
	vpsrld	$5, %xmm13, %xmm7
	vpor	%xmm6, %xmm7, %xmm6
	vpxor	%xmm13, %xmm6, %xmm6
	vpslld	$26, %xmm6, %xmm7
	vpsrld	$6, %xmm6, %xmm6
	vpor	%xmm7, %xmm6, %xmm10
	vpslld	$7, %xmm13, %xmm7
	vpsrld	$25, %xmm13, %xmm6
	vpor	%xmm7, %xmm6, %xmm6
	vpxor	%xmm6, %xmm10, %xmm10
	vpxor	%xmm15, %xmm2, %xmm7
	vpand	%xmm13, %xmm7, %xmm7
	vpxor	%xmm15, %xmm7, %xmm7
	vpaddd	%xmm9, %xmm7, %xmm7
	vmovdqu	768(%rsi,%rax,2), %xmm6
	vpshufb	%xmm11, %xmm6, %xmm0
	vmovdqa	%xmm0, -48(%rsp)        # 16-byte Spill
	vpaddd	%xmm0, %xmm7, %xmm6
	vpaddd	%xmm6, %xmm10, %xmm9
	vpslld	$30, %xmm5, %xmm7
	vpsrld	$2, %xmm5, %xmm6
	vpor	%xmm7, %xmm6, %xmm10
	vpslld	$19, %xmm5, %xmm7
	vpsrld	$13, %xmm5, %xmm6
	vpor	%xmm7, %xmm6, %xmm6
	vpxor	%xmm6, %xmm10, %xmm10
	vpslld	$10, %xmm5, %xmm7
	vpsrld	$22, %xmm5, %xmm6
	vpor	%xmm7, %xmm6, %xmm6
	vpxor	%xmm6, %xmm10, %xmm10
	vpxor	%xmm3, %xmm4, %xmm7
	vpand	%xmm7, %xmm5, %xmm7
	vpand	%xmm3, %xmm4, %xmm6
	vpxor	%xmm6, %xmm7, %xmm6
	vpaddd	%xmm6, %xmm10, %xmm6
	vpaddd	.LCPI1_13(%rip), %xmm9, %xmm7
	vpaddd	%xmm1, %xmm7, %xmm10
	vpaddd	%xmm7, %xmm6, %xmm8
	vpslld	$27, %xmm10, %xmm6
	vpsrld	$5, %xmm10, %xmm7
	vpor	%xmm6, %xmm7, %xmm6
	vpxor	%xmm10, %xmm6, %xmm6
	vpslld	$26, %xmm6, %xmm7
	vpsrld	$6, %xmm6, %xmm6
	vpor	%xmm7, %xmm6, %xmm6
	vpslld	$7, %xmm10, %xmm7
	vpsrld	$25, %xmm10, %xmm1
	vpor	%xmm7, %xmm1, %xmm1
	vpxor	%xmm1, %xmm6, %xmm1
	vpxor	%xmm2, %xmm13, %xmm6
	vpand	%xmm10, %xmm6, %xmm6
	vpxor	%xmm2, %xmm6, %xmm6
	vpaddd	%xmm15, %xmm6, %xmm6
	vmovdqu	832(%rsi,%rax,2), %xmm7
	vpshufb	%xmm11, %xmm7, %xmm0
	vmovdqa	%xmm0, -16(%rsp)        # 16-byte Spill
	vpaddd	%xmm0, %xmm6, %xmm6
	vpaddd	%xmm6, %xmm1, %xmm9
	vpslld	$30, %xmm8, %xmm6
	vpsrld	$2, %xmm8, %xmm7
	vpor	%xmm6, %xmm7, %xmm6
	vpslld	$19, %xmm8, %xmm7
	vpsrld	$13, %xmm8, %xmm1
	vpor	%xmm7, %xmm1, %xmm1
	vpxor	%xmm1, %xmm6, %xmm1
	vpslld	$10, %xmm8, %xmm6
	vpsrld	$22, %xmm8, %xmm7
	vpor	%xmm6, %xmm7, %xmm6
	vpxor	%xmm6, %xmm1, %xmm1
	vpxor	%xmm4, %xmm5, %xmm6
	vpand	%xmm6, %xmm8, %xmm6
	vpand	%xmm4, %xmm5, %xmm7
	vpxor	%xmm7, %xmm6, %xmm6
	vpaddd	%xmm6, %xmm1, %xmm1
	vpaddd	.LCPI1_14(%rip), %xmm9, %xmm7
	vpaddd	%xmm3, %xmm7, %xmm9
	vpaddd	%xmm7, %xmm1, %xmm7
	vpslld	$27, %xmm9, %xmm1
	vpsrld	$5, %xmm9, %xmm3
	vpor	%xmm1, %xmm3, %xmm1
	vpxor	%xmm9, %xmm1, %xmm1
	vpslld	$26, %xmm1, %xmm3
	vpsrld	$6, %xmm1, %xmm1
	vpor	%xmm3, %xmm1, %xmm1
	vpslld	$7, %xmm9, %xmm3
	vpsrld	$25, %xmm9, %xmm6
	vpor	%xmm3, %xmm6, %xmm3
	vpxor	%xmm3, %xmm1, %xmm1
	vpxor	%xmm13, %xmm10, %xmm3
	vpand	%xmm9, %xmm3, %xmm3
	vpxor	%xmm13, %xmm3, %xmm3
	vpaddd	%xmm2, %xmm3, %xmm2
	vmovdqu	896(%rsi,%rax,2), %xmm3
	vpshufb	%xmm11, %xmm3, %xmm12
	vpaddd	%xmm12, %xmm2, %xmm2
	vpaddd	%xmm2, %xmm1, %xmm1
	vpslld	$30, %xmm7, %xmm2
	vpsrld	$2, %xmm7, %xmm3
	vpor	%xmm2, %xmm3, %xmm2
	vpslld	$19, %xmm7, %xmm3
	vpsrld	$13, %xmm7, %xmm6
	vpor	%xmm3, %xmm6, %xmm3
	vpxor	%xmm3, %xmm2, %xmm2
	vpslld	$10, %xmm7, %xmm3
	vpsrld	$22, %xmm7, %xmm6
	vpor	%xmm3, %xmm6, %xmm3
	vpxor	%xmm3, %xmm2, %xmm2
	vpxor	%xmm5, %xmm8, %xmm3
	vpand	%xmm3, %xmm7, %xmm3
	vpand	%xmm5, %xmm8, %xmm6
	vpxor	%xmm6, %xmm3, %xmm3
	vpaddd	%xmm3, %xmm2, %xmm2
	vpaddd	.LCPI1_15(%rip), %xmm1, %xmm1
	vpaddd	%xmm4, %xmm1, %xmm3
	vpaddd	%xmm1, %xmm2, %xmm2
	vpslld	$27, %xmm3, %xmm1
	vpsrld	$5, %xmm3, %xmm4
	vpor	%xmm1, %xmm4, %xmm1
	vpxor	%xmm3, %xmm1, %xmm1
	vpslld	$26, %xmm1, %xmm4
	vpsrld	$6, %xmm1, %xmm1
	vpor	%xmm4, %xmm1, %xmm1
	vpslld	$7, %xmm3, %xmm4
	vpsrld	$25, %xmm3, %xmm6
	vpor	%xmm4, %xmm6, %xmm4
	vpxor	%xmm4, %xmm1, %xmm1
	vpxor	%xmm10, %xmm9, %xmm4
	vpand	%xmm3, %xmm4, %xmm4
	vpxor	%xmm10, %xmm4, %xmm4
	vpaddd	%xmm13, %xmm4, %xmm0
	vmovdqu	960(%rsi,%rax,2), %xmm4
	vpshufb	%xmm11, %xmm4, %xmm13
	vpaddd	%xmm13, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm1, %xmm0
	vpslld	$30, %xmm2, %xmm1
	vpsrld	$2, %xmm2, %xmm4
	vpor	%xmm1, %xmm4, %xmm1
	vpslld	$19, %xmm2, %xmm4
	vpsrld	$13, %xmm2, %xmm6
	vpor	%xmm4, %xmm6, %xmm4
	vpxor	%xmm4, %xmm1, %xmm1
	vpslld	$10, %xmm2, %xmm4
	vpsrld	$22, %xmm2, %xmm6
	vpor	%xmm4, %xmm6, %xmm4
	vpxor	%xmm4, %xmm1, %xmm1
	vpxor	%xmm8, %xmm7, %xmm4
	vpand	%xmm4, %xmm2, %xmm4
	vpand	%xmm8, %xmm7, %xmm6
	vpxor	%xmm6, %xmm4, %xmm4
	vpaddd	%xmm4, %xmm1, %xmm1
	vpaddd	.LCPI1_16(%rip), %xmm0, %xmm4
	vpaddd	%xmm5, %xmm4, %xmm11
	vpaddd	%xmm4, %xmm1, %xmm4
	vmovdqa	-80(%rsp), %xmm0        # 16-byte Reload
	vpslld	$21, %xmm0, %xmm1
	vpsrld	$11, %xmm0, %xmm5
	vpor	%xmm1, %xmm5, %xmm1
	vpxor	%xmm0, %xmm1, %xmm1
	vpslld	$25, %xmm1, %xmm5
	vpsrld	$7, %xmm1, %xmm1
	vpor	%xmm5, %xmm1, %xmm1
	vpsrld	$3, %xmm0, %xmm5
	vmovdqa	%xmm0, %xmm15
	vpxor	%xmm5, %xmm1, %xmm1
	vmovdqa	%xmm12, -32(%rsp)       # 16-byte Spill
	vpslld	$30, %xmm12, %xmm5
	vpsrld	$2, %xmm12, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm12, %xmm5, %xmm5
	vpslld	$15, %xmm5, %xmm6
	vpsrld	$17, %xmm5, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpsrld	$10, %xmm12, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vmovdqa	-64(%rsp), %xmm0        # 16-byte Reload
	vpaddd	16(%rsp), %xmm0, %xmm6  # 16-byte Folded Reload
	vpaddd	%xmm1, %xmm6, %xmm1
	vpaddd	%xmm5, %xmm1, %xmm12
	vpslld	$27, %xmm11, %xmm1
	vpsrld	$5, %xmm11, %xmm5
	vpor	%xmm1, %xmm5, %xmm1
	vpxor	%xmm11, %xmm1, %xmm1
	vpslld	$26, %xmm1, %xmm5
	vpsrld	$6, %xmm1, %xmm1
	vpor	%xmm5, %xmm1, %xmm1
	vpslld	$7, %xmm11, %xmm5
	vpsrld	$25, %xmm11, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm1, %xmm1
	vpxor	%xmm9, %xmm3, %xmm5
	vpand	%xmm11, %xmm5, %xmm5
	vpxor	%xmm9, %xmm5, %xmm5
	vpaddd	%xmm10, %xmm5, %xmm5
	vpaddd	%xmm12, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm1, %xmm10
	vpslld	$30, %xmm4, %xmm5
	vpsrld	$2, %xmm4, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpslld	$19, %xmm4, %xmm6
	vpsrld	$13, %xmm4, %xmm1
	vpor	%xmm6, %xmm1, %xmm1
	vpxor	%xmm1, %xmm5, %xmm1
	vpslld	$10, %xmm4, %xmm5
	vpsrld	$22, %xmm4, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm1, %xmm1
	vpxor	%xmm7, %xmm2, %xmm5
	vpand	%xmm5, %xmm4, %xmm5
	vpand	%xmm7, %xmm2, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm1, %xmm1
	vpaddd	.LCPI1_17(%rip), %xmm10, %xmm5
	vpaddd	%xmm8, %xmm5, %xmm10
	vpaddd	%xmm5, %xmm1, %xmm5
	vpslld	$21, %xmm14, %xmm1
	vpsrld	$11, %xmm14, %xmm6
	vpor	%xmm1, %xmm6, %xmm1
	vpxor	%xmm14, %xmm1, %xmm1
	vpslld	$25, %xmm1, %xmm6
	vpsrld	$7, %xmm1, %xmm1
	vpor	%xmm6, %xmm1, %xmm1
	vpsrld	$3, %xmm14, %xmm6
	vpxor	%xmm6, %xmm1, %xmm1
	vmovdqa	%xmm13, 48(%rsp)        # 16-byte Spill
	vpslld	$30, %xmm13, %xmm6
	vpsrld	$2, %xmm13, %xmm0
	vpor	%xmm6, %xmm0, %xmm0
	vpxor	%xmm13, %xmm0, %xmm0
	vpslld	$15, %xmm0, %xmm6
	vpsrld	$17, %xmm0, %xmm0
	vpor	%xmm6, %xmm0, %xmm0
	vpsrld	$10, %xmm13, %xmm6
	vpxor	%xmm6, %xmm0, %xmm0
	vpaddd	(%rsp), %xmm15, %xmm6   # 16-byte Folded Reload
	vpaddd	%xmm1, %xmm6, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm13
	vpslld	$27, %xmm10, %xmm0
	vpsrld	$5, %xmm10, %xmm1
	vpor	%xmm0, %xmm1, %xmm0
	vpxor	%xmm10, %xmm0, %xmm0
	vpslld	$26, %xmm0, %xmm1
	vpsrld	$6, %xmm0, %xmm0
	vpor	%xmm1, %xmm0, %xmm0
	vpslld	$7, %xmm10, %xmm1
	vpsrld	$25, %xmm10, %xmm6
	vpor	%xmm1, %xmm6, %xmm1
	vpxor	%xmm1, %xmm0, %xmm0
	vpxor	%xmm3, %xmm11, %xmm1
	vpand	%xmm10, %xmm1, %xmm1
	vpxor	%xmm3, %xmm1, %xmm1
	vpaddd	%xmm9, %xmm1, %xmm1
	vpaddd	%xmm13, %xmm1, %xmm1
	vpaddd	%xmm1, %xmm0, %xmm8
	vpslld	$30, %xmm5, %xmm1
	vpsrld	$2, %xmm5, %xmm6
	vpor	%xmm1, %xmm6, %xmm1
	vpslld	$19, %xmm5, %xmm6
	vpsrld	$13, %xmm5, %xmm0
	vpor	%xmm6, %xmm0, %xmm0
	vpxor	%xmm0, %xmm1, %xmm0
	vpslld	$10, %xmm5, %xmm1
	vpsrld	$22, %xmm5, %xmm6
	vpor	%xmm1, %xmm6, %xmm1
	vpxor	%xmm1, %xmm0, %xmm0
	vpxor	%xmm2, %xmm4, %xmm1
	vpand	%xmm1, %xmm5, %xmm1
	vpand	%xmm2, %xmm4, %xmm6
	vpxor	%xmm6, %xmm1, %xmm1
	vpaddd	%xmm1, %xmm0, %xmm0
	vpaddd	.LCPI1_18(%rip), %xmm8, %xmm1
	vpaddd	%xmm7, %xmm1, %xmm9
	vpaddd	%xmm1, %xmm0, %xmm6
	vmovdqa	-112(%rsp), %xmm7       # 16-byte Reload
	vpslld	$21, %xmm7, %xmm0
	vpsrld	$11, %xmm7, %xmm1
	vpor	%xmm0, %xmm1, %xmm0
	vpxor	%xmm7, %xmm0, %xmm0
	vpslld	$25, %xmm0, %xmm1
	vpsrld	$7, %xmm0, %xmm0
	vpor	%xmm1, %xmm0, %xmm0
	vpsrld	$3, %xmm7, %xmm1
	vpxor	%xmm1, %xmm0, %xmm0
	vmovdqa	%xmm12, -64(%rsp)       # 16-byte Spill
	vpslld	$30, %xmm12, %xmm1
	vpsrld	$2, %xmm12, %xmm7
	vpor	%xmm1, %xmm7, %xmm1
	vpxor	%xmm12, %xmm1, %xmm1
	vpslld	$15, %xmm1, %xmm7
	vpsrld	$17, %xmm1, %xmm1
	vpor	%xmm7, %xmm1, %xmm1
	vpsrld	$10, %xmm12, %xmm7
	vpxor	%xmm7, %xmm1, %xmm1
	vpaddd	32(%rsp), %xmm14, %xmm7 # 16-byte Folded Reload
	vpaddd	%xmm0, %xmm7, %xmm0
	vpaddd	%xmm1, %xmm0, %xmm14
	vpslld	$27, %xmm9, %xmm0
	vpsrld	$5, %xmm9, %xmm1
	vpor	%xmm0, %xmm1, %xmm0
	vpxor	%xmm9, %xmm0, %xmm0
	vpslld	$26, %xmm0, %xmm1
	vpsrld	$6, %xmm0, %xmm0
	vpor	%xmm1, %xmm0, %xmm0
	vpslld	$7, %xmm9, %xmm1
	vpsrld	$25, %xmm9, %xmm7
	vpor	%xmm1, %xmm7, %xmm1
	vpxor	%xmm1, %xmm0, %xmm0
	vpxor	%xmm11, %xmm10, %xmm1
	vpand	%xmm9, %xmm1, %xmm1
	vpxor	%xmm11, %xmm1, %xmm1
	vpaddd	%xmm3, %xmm1, %xmm1
	vpaddd	%xmm14, %xmm1, %xmm1
	vpaddd	%xmm1, %xmm0, %xmm0
	vpslld	$30, %xmm6, %xmm1
	vpsrld	$2, %xmm6, %xmm3
	vpor	%xmm1, %xmm3, %xmm1
	vpslld	$19, %xmm6, %xmm3
	vpsrld	$13, %xmm6, %xmm7
	vpor	%xmm3, %xmm7, %xmm3
	vpxor	%xmm3, %xmm1, %xmm1
	vpslld	$10, %xmm6, %xmm3
	vpsrld	$22, %xmm6, %xmm7
	vpor	%xmm3, %xmm7, %xmm3
	vpxor	%xmm3, %xmm1, %xmm1
	vpxor	%xmm4, %xmm5, %xmm3
	vpand	%xmm3, %xmm6, %xmm3
	vpand	%xmm4, %xmm5, %xmm7
	vpxor	%xmm7, %xmm3, %xmm3
	vpaddd	%xmm3, %xmm1, %xmm1
	vpaddd	.LCPI1_19(%rip), %xmm0, %xmm0
	vpaddd	%xmm2, %xmm0, %xmm8
	vpaddd	%xmm0, %xmm1, %xmm3
	vmovdqa	-96(%rsp), %xmm2        # 16-byte Reload
	vpslld	$21, %xmm2, %xmm0
	vpsrld	$11, %xmm2, %xmm1
	vpor	%xmm0, %xmm1, %xmm0
	vpxor	%xmm2, %xmm0, %xmm0
	vpslld	$25, %xmm0, %xmm1
	vpsrld	$7, %xmm0, %xmm0
	vpor	%xmm1, %xmm0, %xmm0
	vpsrld	$3, %xmm2, %xmm1
	vmovdqa	%xmm2, %xmm15
	vpxor	%xmm1, %xmm0, %xmm0
	vmovdqa	%xmm13, -80(%rsp)       # 16-byte Spill
	vpslld	$30, %xmm13, %xmm1
	vpsrld	$2, %xmm13, %xmm2
	vpor	%xmm1, %xmm2, %xmm1
	vpxor	%xmm13, %xmm1, %xmm1
	vpslld	$15, %xmm1, %xmm2
	vpsrld	$17, %xmm1, %xmm1
	vpor	%xmm2, %xmm1, %xmm1
	vpsrld	$10, %xmm13, %xmm2
	vpxor	%xmm2, %xmm1, %xmm1
	vmovdqa	-112(%rsp), %xmm2       # 16-byte Reload
	vpaddd	-48(%rsp), %xmm2, %xmm2 # 16-byte Folded Reload
	vpaddd	%xmm0, %xmm2, %xmm0
	vpaddd	%xmm1, %xmm0, %xmm13
	vpslld	$27, %xmm8, %xmm0
	vpsrld	$5, %xmm8, %xmm1
	vpor	%xmm0, %xmm1, %xmm0
	vpxor	%xmm8, %xmm0, %xmm0
	vpslld	$26, %xmm0, %xmm1
	vpsrld	$6, %xmm0, %xmm0
	vpor	%xmm1, %xmm0, %xmm0
	vpslld	$7, %xmm8, %xmm1
	vpsrld	$25, %xmm8, %xmm2
	vpor	%xmm1, %xmm2, %xmm1
	vpxor	%xmm1, %xmm0, %xmm0
	vpxor	%xmm10, %xmm9, %xmm1
	vpand	%xmm8, %xmm1, %xmm1
	vpxor	%xmm10, %xmm1, %xmm1
	vpaddd	%xmm11, %xmm1, %xmm1
	vpaddd	%xmm13, %xmm1, %xmm1
	vpaddd	%xmm1, %xmm0, %xmm0
	vpslld	$30, %xmm3, %xmm1
	vpsrld	$2, %xmm3, %xmm2
	vpor	%xmm1, %xmm2, %xmm1
	vpslld	$19, %xmm3, %xmm2
	vpsrld	$13, %xmm3, %xmm7
	vpor	%xmm2, %xmm7, %xmm2
	vpxor	%xmm2, %xmm1, %xmm1
	vpslld	$10, %xmm3, %xmm2
	vpsrld	$22, %xmm3, %xmm7
	vpor	%xmm2, %xmm7, %xmm2
	vpxor	%xmm2, %xmm1, %xmm1
	vpxor	%xmm5, %xmm6, %xmm2
	vpand	%xmm2, %xmm3, %xmm2
	vpand	%xmm5, %xmm6, %xmm7
	vpxor	%xmm7, %xmm2, %xmm2
	vpaddd	%xmm2, %xmm1, %xmm1
	vpaddd	.LCPI1_20(%rip), %xmm0, %xmm0
	vpaddd	%xmm4, %xmm0, %xmm12
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovdqa	-128(%rsp), %xmm4       # 16-byte Reload
	vpslld	$21, %xmm4, %xmm1
	vpsrld	$11, %xmm4, %xmm2
	vpor	%xmm1, %xmm2, %xmm1
	vpxor	%xmm4, %xmm1, %xmm1
	vpslld	$25, %xmm1, %xmm2
	vpsrld	$7, %xmm1, %xmm1
	vpor	%xmm2, %xmm1, %xmm1
	vpsrld	$3, %xmm4, %xmm2
	vpxor	%xmm2, %xmm1, %xmm1
	vmovdqa	%xmm14, 112(%rsp)       # 16-byte Spill
	vpslld	$30, %xmm14, %xmm2
	vpsrld	$2, %xmm14, %xmm7
	vpor	%xmm2, %xmm7, %xmm2
	vpxor	%xmm14, %xmm2, %xmm2
	vpslld	$15, %xmm2, %xmm7
	vpsrld	$17, %xmm2, %xmm2
	vpor	%xmm7, %xmm2, %xmm2
	vpsrld	$10, %xmm14, %xmm7
	vpxor	%xmm7, %xmm2, %xmm2
	vpaddd	-16(%rsp), %xmm15, %xmm7 # 16-byte Folded Reload
	vpaddd	%xmm1, %xmm7, %xmm1
	vpaddd	%xmm2, %xmm1, %xmm11
	vpslld	$27, %xmm12, %xmm1
	vpsrld	$5, %xmm12, %xmm2
	vpor	%xmm1, %xmm2, %xmm1
	vpxor	%xmm12, %xmm1, %xmm1
	vpslld	$26, %xmm1, %xmm2
	vpsrld	$6, %xmm1, %xmm1
	vpor	%xmm2, %xmm1, %xmm1
	vpslld	$7, %xmm12, %xmm2
	vpsrld	$25, %xmm12, %xmm7
	vpor	%xmm2, %xmm7, %xmm2
	vpxor	%xmm2, %xmm1, %xmm1
	vpxor	%xmm9, %xmm8, %xmm2
	vpand	%xmm12, %xmm2, %xmm2
	vpxor	%xmm9, %xmm2, %xmm2
	vpaddd	%xmm10, %xmm2, %xmm2
	vpaddd	%xmm11, %xmm2, %xmm2
	vpaddd	%xmm2, %xmm1, %xmm10
	vpslld	$30, %xmm0, %xmm2
	vpsrld	$2, %xmm0, %xmm7
	vpor	%xmm2, %xmm7, %xmm2
	vpslld	$19, %xmm0, %xmm7
	vpsrld	$13, %xmm0, %xmm1
	vpor	%xmm7, %xmm1, %xmm1
	vpxor	%xmm1, %xmm2, %xmm1
	vpslld	$10, %xmm0, %xmm2
	vpsrld	$22, %xmm0, %xmm7
	vpor	%xmm2, %xmm7, %xmm2
	vpxor	%xmm2, %xmm1, %xmm1
	vpxor	%xmm6, %xmm3, %xmm2
	vpand	%xmm2, %xmm0, %xmm2
	vpand	%xmm6, %xmm3, %xmm7
	vpxor	%xmm7, %xmm2, %xmm2
	vpaddd	%xmm2, %xmm1, %xmm1
	vpaddd	.LCPI1_21(%rip), %xmm10, %xmm2
	vpaddd	%xmm5, %xmm2, %xmm10
	vpaddd	%xmm2, %xmm1, %xmm1
	vmovdqa	96(%rsp), %xmm15        # 16-byte Reload
	vpslld	$21, %xmm15, %xmm2
	vpsrld	$11, %xmm15, %xmm7
	vpor	%xmm2, %xmm7, %xmm2
	vpxor	%xmm15, %xmm2, %xmm2
	vpslld	$25, %xmm2, %xmm7
	vpsrld	$7, %xmm2, %xmm2
	vpor	%xmm7, %xmm2, %xmm2
	vpsrld	$3, %xmm15, %xmm7
	vpxor	%xmm7, %xmm2, %xmm2
	vmovdqa	%xmm13, -112(%rsp)      # 16-byte Spill
	vpslld	$30, %xmm13, %xmm7
	vpsrld	$2, %xmm13, %xmm4
	vpor	%xmm7, %xmm4, %xmm4
	vpxor	%xmm13, %xmm4, %xmm4
	vpslld	$15, %xmm4, %xmm7
	vpsrld	$17, %xmm4, %xmm4
	vpor	%xmm7, %xmm4, %xmm4
	vpsrld	$10, %xmm13, %xmm7
	vpxor	%xmm7, %xmm4, %xmm4
	vmovdqa	-128(%rsp), %xmm5       # 16-byte Reload
	vpaddd	-32(%rsp), %xmm5, %xmm7 # 16-byte Folded Reload
	vpaddd	%xmm2, %xmm7, %xmm2
	vpaddd	%xmm4, %xmm2, %xmm13
	vpslld	$27, %xmm10, %xmm2
	vpsrld	$5, %xmm10, %xmm4
	vpor	%xmm2, %xmm4, %xmm2
	vpxor	%xmm10, %xmm2, %xmm2
	vpslld	$26, %xmm2, %xmm4
	vpsrld	$6, %xmm2, %xmm2
	vpor	%xmm4, %xmm2, %xmm2
	vpslld	$7, %xmm10, %xmm4
	vpsrld	$25, %xmm10, %xmm7
	vpor	%xmm4, %xmm7, %xmm4
	vpxor	%xmm4, %xmm2, %xmm2
	vpxor	%xmm8, %xmm12, %xmm4
	vpand	%xmm10, %xmm4, %xmm4
	vpxor	%xmm8, %xmm4, %xmm4
	vpaddd	%xmm9, %xmm4, %xmm4
	vpaddd	%xmm13, %xmm4, %xmm4
	vpaddd	%xmm4, %xmm2, %xmm9
	vpslld	$30, %xmm1, %xmm4
	vpsrld	$2, %xmm1, %xmm7
	vpor	%xmm4, %xmm7, %xmm4
	vpslld	$19, %xmm1, %xmm7
	vpsrld	$13, %xmm1, %xmm2
	vpor	%xmm7, %xmm2, %xmm2
	vpxor	%xmm2, %xmm4, %xmm2
	vpslld	$10, %xmm1, %xmm4
	vpsrld	$22, %xmm1, %xmm7
	vpor	%xmm4, %xmm7, %xmm4
	vpxor	%xmm4, %xmm2, %xmm2
	vpxor	%xmm3, %xmm0, %xmm4
	vpand	%xmm4, %xmm1, %xmm4
	vpand	%xmm3, %xmm0, %xmm7
	vpxor	%xmm7, %xmm4, %xmm4
	vpaddd	%xmm4, %xmm2, %xmm2
	vpaddd	.LCPI1_22(%rip), %xmm9, %xmm4
	vpaddd	%xmm6, %xmm4, %xmm9
	vpaddd	%xmm4, %xmm2, %xmm2
	vmovdqa	80(%rsp), %xmm5         # 16-byte Reload
	vpslld	$21, %xmm5, %xmm4
	vpsrld	$11, %xmm5, %xmm7
	vpor	%xmm4, %xmm7, %xmm4
	vpxor	%xmm5, %xmm4, %xmm4
	vpslld	$25, %xmm4, %xmm7
	vpsrld	$7, %xmm4, %xmm4
	vpor	%xmm7, %xmm4, %xmm4
	vpsrld	$3, %xmm5, %xmm7
	vmovdqa	%xmm5, %xmm14
	vpxor	%xmm7, %xmm4, %xmm4
	vmovdqa	%xmm11, -96(%rsp)       # 16-byte Spill
	vpslld	$30, %xmm11, %xmm7
	vpsrld	$2, %xmm11, %xmm5
	vpor	%xmm7, %xmm5, %xmm5
	vpxor	%xmm11, %xmm5, %xmm5
	vpslld	$15, %xmm5, %xmm7
	vpsrld	$17, %xmm5, %xmm5
	vpor	%xmm7, %xmm5, %xmm5
	vpsrld	$10, %xmm11, %xmm7
	vpxor	%xmm7, %xmm5, %xmm5
	vpaddd	48(%rsp), %xmm15, %xmm7 # 16-byte Folded Reload
	vpaddd	%xmm4, %xmm7, %xmm4
	vpaddd	%xmm5, %xmm4, %xmm11
	vpslld	$27, %xmm9, %xmm4
	vpsrld	$5, %xmm9, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm9, %xmm4, %xmm4
	vpslld	$26, %xmm4, %xmm5
	vpsrld	$6, %xmm4, %xmm4
	vpor	%xmm5, %xmm4, %xmm4
	vpslld	$7, %xmm9, %xmm5
	vpsrld	$25, %xmm9, %xmm7
	vpor	%xmm5, %xmm7, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm12, %xmm10, %xmm5
	vpand	%xmm9, %xmm5, %xmm5
	vpxor	%xmm12, %xmm5, %xmm5
	vpaddd	%xmm8, %xmm5, %xmm5
	vpaddd	%xmm11, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm8
	vpslld	$30, %xmm2, %xmm5
	vpsrld	$2, %xmm2, %xmm7
	vpor	%xmm5, %xmm7, %xmm5
	vpslld	$19, %xmm2, %xmm7
	vpsrld	$13, %xmm2, %xmm4
	vpor	%xmm7, %xmm4, %xmm4
	vpxor	%xmm4, %xmm5, %xmm4
	vpslld	$10, %xmm2, %xmm5
	vpsrld	$22, %xmm2, %xmm7
	vpor	%xmm5, %xmm7, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm0, %xmm1, %xmm5
	vpand	%xmm5, %xmm2, %xmm5
	vpand	%xmm0, %xmm1, %xmm7
	vpxor	%xmm7, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm4
	vpaddd	.LCPI1_23(%rip), %xmm8, %xmm5
	vpaddd	%xmm3, %xmm5, %xmm7
	vpaddd	%xmm5, %xmm4, %xmm3
	vmovdqa	64(%rsp), %xmm6         # 16-byte Reload
	vpslld	$21, %xmm6, %xmm4
	vpsrld	$11, %xmm6, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm6, %xmm4, %xmm4
	vpslld	$25, %xmm4, %xmm5
	vpsrld	$7, %xmm4, %xmm4
	vpor	%xmm5, %xmm4, %xmm4
	vpsrld	$3, %xmm6, %xmm5
	vmovdqa	%xmm6, %xmm15
	vpxor	%xmm5, %xmm4, %xmm4
	vmovdqa	%xmm13, -128(%rsp)      # 16-byte Spill
	vpslld	$30, %xmm13, %xmm5
	vpsrld	$2, %xmm13, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm13, %xmm5, %xmm5
	vpslld	$15, %xmm5, %xmm6
	vpsrld	$17, %xmm5, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpsrld	$10, %xmm13, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm4, %xmm14, %xmm4
	vpaddd	-64(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vpaddd	%xmm5, %xmm4, %xmm13
	vpslld	$27, %xmm7, %xmm4
	vpsrld	$5, %xmm7, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm7, %xmm4, %xmm4
	vpslld	$26, %xmm4, %xmm5
	vpsrld	$6, %xmm4, %xmm4
	vpor	%xmm5, %xmm4, %xmm4
	vpslld	$7, %xmm7, %xmm5
	vpsrld	$25, %xmm7, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm10, %xmm9, %xmm5
	vpand	%xmm7, %xmm5, %xmm5
	vpxor	%xmm10, %xmm5, %xmm5
	vpaddd	%xmm12, %xmm5, %xmm5
	vpaddd	%xmm13, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm8
	vpslld	$30, %xmm3, %xmm5
	vpsrld	$2, %xmm3, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpslld	$19, %xmm3, %xmm6
	vpsrld	$13, %xmm3, %xmm4
	vpor	%xmm6, %xmm4, %xmm4
	vpxor	%xmm4, %xmm5, %xmm4
	vpslld	$10, %xmm3, %xmm5
	vpsrld	$22, %xmm3, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm1, %xmm2, %xmm5
	vpand	%xmm5, %xmm3, %xmm5
	vpand	%xmm1, %xmm2, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm4
	vpaddd	.LCPI1_24(%rip), %xmm8, %xmm5
	vpaddd	%xmm0, %xmm5, %xmm8
	vpaddd	%xmm5, %xmm4, %xmm0
	vmovdqa	16(%rsp), %xmm6         # 16-byte Reload
	vpslld	$21, %xmm6, %xmm4
	vpsrld	$11, %xmm6, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm6, %xmm4, %xmm4
	vpslld	$25, %xmm4, %xmm5
	vpsrld	$7, %xmm4, %xmm4
	vpor	%xmm5, %xmm4, %xmm4
	vpsrld	$3, %xmm6, %xmm5
	vmovdqa	%xmm6, %xmm14
	vpxor	%xmm5, %xmm4, %xmm4
	vmovdqa	%xmm11, 96(%rsp)        # 16-byte Spill
	vpslld	$30, %xmm11, %xmm5
	vpsrld	$2, %xmm11, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm11, %xmm5, %xmm5
	vpslld	$15, %xmm5, %xmm6
	vpsrld	$17, %xmm5, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpsrld	$10, %xmm11, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm4, %xmm15, %xmm4
	vpaddd	-80(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vpaddd	%xmm5, %xmm4, %xmm11
	vpslld	$27, %xmm8, %xmm4
	vpsrld	$5, %xmm8, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm8, %xmm4, %xmm4
	vpslld	$26, %xmm4, %xmm5
	vpsrld	$6, %xmm4, %xmm4
	vpor	%xmm5, %xmm4, %xmm4
	vpslld	$7, %xmm8, %xmm5
	vpsrld	$25, %xmm8, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm9, %xmm7, %xmm5
	vpand	%xmm8, %xmm5, %xmm5
	vpxor	%xmm9, %xmm5, %xmm5
	vpaddd	%xmm10, %xmm5, %xmm5
	vpaddd	%xmm11, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm10
	vpslld	$30, %xmm0, %xmm5
	vpsrld	$2, %xmm0, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpslld	$19, %xmm0, %xmm6
	vpsrld	$13, %xmm0, %xmm4
	vpor	%xmm6, %xmm4, %xmm4
	vpxor	%xmm4, %xmm5, %xmm4
	vpslld	$10, %xmm0, %xmm5
	vpsrld	$22, %xmm0, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm2, %xmm3, %xmm5
	vpand	%xmm5, %xmm0, %xmm5
	vpand	%xmm2, %xmm3, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm4
	vpaddd	.LCPI1_25(%rip), %xmm10, %xmm6
	vpaddd	%xmm1, %xmm6, %xmm10
	vpaddd	%xmm6, %xmm4, %xmm1
	vmovdqa	(%rsp), %xmm5           # 16-byte Reload
	vpslld	$21, %xmm5, %xmm4
	vpsrld	$11, %xmm5, %xmm6
	vpor	%xmm4, %xmm6, %xmm4
	vpxor	%xmm5, %xmm4, %xmm4
	vpslld	$25, %xmm4, %xmm6
	vpsrld	$7, %xmm4, %xmm4
	vpor	%xmm6, %xmm4, %xmm4
	vpsrld	$3, %xmm5, %xmm6
	vmovdqa	%xmm5, %xmm12
	vpxor	%xmm6, %xmm4, %xmm4
	vmovdqa	%xmm13, 80(%rsp)        # 16-byte Spill
	vpslld	$30, %xmm13, %xmm6
	vpsrld	$2, %xmm13, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpxor	%xmm13, %xmm5, %xmm5
	vpslld	$15, %xmm5, %xmm6
	vpsrld	$17, %xmm5, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpsrld	$10, %xmm13, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm4, %xmm14, %xmm4
	vpaddd	112(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vpaddd	%xmm5, %xmm4, %xmm13
	vpslld	$27, %xmm10, %xmm4
	vpsrld	$5, %xmm10, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm10, %xmm4, %xmm4
	vpslld	$26, %xmm4, %xmm5
	vpsrld	$6, %xmm4, %xmm4
	vpor	%xmm5, %xmm4, %xmm4
	vpslld	$7, %xmm10, %xmm5
	vpsrld	$25, %xmm10, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm7, %xmm8, %xmm5
	vpand	%xmm10, %xmm5, %xmm5
	vpxor	%xmm7, %xmm5, %xmm5
	vpaddd	%xmm9, %xmm5, %xmm5
	vpaddd	%xmm13, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm9
	vpslld	$30, %xmm1, %xmm5
	vpsrld	$2, %xmm1, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpslld	$19, %xmm1, %xmm6
	vpsrld	$13, %xmm1, %xmm4
	vpor	%xmm6, %xmm4, %xmm4
	vpxor	%xmm4, %xmm5, %xmm4
	vpslld	$10, %xmm1, %xmm5
	vpsrld	$22, %xmm1, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm3, %xmm0, %xmm5
	vpand	%xmm5, %xmm1, %xmm5
	vpand	%xmm3, %xmm0, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm4
	vpaddd	.LCPI1_26(%rip), %xmm9, %xmm5
	vpaddd	%xmm2, %xmm5, %xmm9
	vpaddd	%xmm5, %xmm4, %xmm2
	vmovdqa	32(%rsp), %xmm15        # 16-byte Reload
	vpslld	$21, %xmm15, %xmm4
	vpsrld	$11, %xmm15, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm15, %xmm4, %xmm4
	vpslld	$25, %xmm4, %xmm5
	vpsrld	$7, %xmm4, %xmm4
	vpor	%xmm5, %xmm4, %xmm4
	vpsrld	$3, %xmm15, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vmovdqa	%xmm11, 64(%rsp)        # 16-byte Spill
	vpslld	$30, %xmm11, %xmm5
	vpsrld	$2, %xmm11, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm11, %xmm5, %xmm5
	vpslld	$15, %xmm5, %xmm6
	vpsrld	$17, %xmm5, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpsrld	$10, %xmm11, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm4, %xmm12, %xmm4
	vpaddd	-112(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vpaddd	%xmm5, %xmm4, %xmm14
	vpslld	$27, %xmm9, %xmm4
	vpsrld	$5, %xmm9, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm9, %xmm4, %xmm4
	vpslld	$26, %xmm4, %xmm5
	vpsrld	$6, %xmm4, %xmm4
	vpor	%xmm5, %xmm4, %xmm4
	vpslld	$7, %xmm9, %xmm5
	vpsrld	$25, %xmm9, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm8, %xmm10, %xmm5
	vpand	%xmm9, %xmm5, %xmm5
	vpxor	%xmm8, %xmm5, %xmm5
	vpaddd	%xmm7, %xmm5, %xmm5
	vpaddd	%xmm14, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm4
	vpslld	$30, %xmm2, %xmm5
	vpsrld	$2, %xmm2, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpslld	$19, %xmm2, %xmm6
	vpsrld	$13, %xmm2, %xmm7
	vpor	%xmm6, %xmm7, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpslld	$10, %xmm2, %xmm6
	vpsrld	$22, %xmm2, %xmm7
	vpor	%xmm6, %xmm7, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpxor	%xmm0, %xmm1, %xmm6
	vpand	%xmm6, %xmm2, %xmm6
	vpand	%xmm0, %xmm1, %xmm7
	vpxor	%xmm7, %xmm6, %xmm6
	vpaddd	%xmm6, %xmm5, %xmm5
	vpaddd	.LCPI1_27(%rip), %xmm4, %xmm4
	vpaddd	%xmm3, %xmm4, %xmm7
	vpaddd	%xmm4, %xmm5, %xmm3
	vmovdqa	-48(%rsp), %xmm6        # 16-byte Reload
	vpslld	$21, %xmm6, %xmm4
	vpsrld	$11, %xmm6, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm6, %xmm4, %xmm4
	vpslld	$25, %xmm4, %xmm5
	vpsrld	$7, %xmm4, %xmm4
	vpor	%xmm5, %xmm4, %xmm4
	vpsrld	$3, %xmm6, %xmm5
	vmovdqa	%xmm6, %xmm12
	vpxor	%xmm5, %xmm4, %xmm4
	vmovdqa	%xmm13, 16(%rsp)        # 16-byte Spill
	vpslld	$30, %xmm13, %xmm5
	vpsrld	$2, %xmm13, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm13, %xmm5, %xmm5
	vpslld	$15, %xmm5, %xmm6
	vpsrld	$17, %xmm5, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpsrld	$10, %xmm13, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm4, %xmm15, %xmm4
	vpaddd	-96(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vpaddd	%xmm5, %xmm4, %xmm13
	vpslld	$27, %xmm7, %xmm4
	vpsrld	$5, %xmm7, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm7, %xmm4, %xmm4
	vpslld	$26, %xmm4, %xmm5
	vpsrld	$6, %xmm4, %xmm4
	vpor	%xmm5, %xmm4, %xmm4
	vpslld	$7, %xmm7, %xmm5
	vpsrld	$25, %xmm7, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm10, %xmm9, %xmm5
	vpand	%xmm7, %xmm5, %xmm5
	vpxor	%xmm10, %xmm5, %xmm5
	vpaddd	%xmm8, %xmm5, %xmm5
	vpaddd	%xmm13, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm8
	vpslld	$30, %xmm3, %xmm5
	vpsrld	$2, %xmm3, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpslld	$19, %xmm3, %xmm6
	vpsrld	$13, %xmm3, %xmm4
	vpor	%xmm6, %xmm4, %xmm4
	vpxor	%xmm4, %xmm5, %xmm4
	vpslld	$10, %xmm3, %xmm5
	vpsrld	$22, %xmm3, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm1, %xmm2, %xmm5
	vpand	%xmm5, %xmm3, %xmm5
	vpand	%xmm1, %xmm2, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm4
	vpaddd	.LCPI1_28(%rip), %xmm8, %xmm5
	vpaddd	%xmm0, %xmm5, %xmm8
	vpaddd	%xmm5, %xmm4, %xmm0
	vmovdqa	-16(%rsp), %xmm6        # 16-byte Reload
	vpslld	$21, %xmm6, %xmm4
	vpsrld	$11, %xmm6, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm6, %xmm4, %xmm4
	vpslld	$25, %xmm4, %xmm5
	vpsrld	$7, %xmm4, %xmm4
	vpor	%xmm5, %xmm4, %xmm4
	vpsrld	$3, %xmm6, %xmm5
	vmovdqa	%xmm6, %xmm11
	vpxor	%xmm5, %xmm4, %xmm4
	vmovdqa	%xmm14, (%rsp)          # 16-byte Spill
	vpslld	$30, %xmm14, %xmm5
	vpsrld	$2, %xmm14, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm14, %xmm5, %xmm5
	vpslld	$15, %xmm5, %xmm6
	vpsrld	$17, %xmm5, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpsrld	$10, %xmm14, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm4, %xmm12, %xmm4
	vpaddd	-128(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vpaddd	%xmm5, %xmm4, %xmm12
	vpslld	$27, %xmm8, %xmm4
	vpsrld	$5, %xmm8, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm8, %xmm4, %xmm4
	vpslld	$26, %xmm4, %xmm5
	vpsrld	$6, %xmm4, %xmm4
	vpor	%xmm5, %xmm4, %xmm4
	vpslld	$7, %xmm8, %xmm5
	vpsrld	$25, %xmm8, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm9, %xmm7, %xmm5
	vpand	%xmm8, %xmm5, %xmm5
	vpxor	%xmm9, %xmm5, %xmm5
	vpaddd	%xmm10, %xmm5, %xmm5
	vpaddd	%xmm12, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm10
	vpslld	$30, %xmm0, %xmm5
	vpsrld	$2, %xmm0, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpslld	$19, %xmm0, %xmm6
	vpsrld	$13, %xmm0, %xmm4
	vpor	%xmm6, %xmm4, %xmm4
	vpxor	%xmm4, %xmm5, %xmm4
	vpslld	$10, %xmm0, %xmm5
	vpsrld	$22, %xmm0, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm2, %xmm3, %xmm5
	vpand	%xmm5, %xmm0, %xmm5
	vpand	%xmm2, %xmm3, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm5
	vpaddd	.LCPI1_29(%rip), %xmm10, %xmm6
	vpaddd	%xmm1, %xmm6, %xmm10
	vpaddd	%xmm6, %xmm5, %xmm1
	vmovdqa	-32(%rsp), %xmm4        # 16-byte Reload
	vpslld	$21, %xmm4, %xmm5
	vpsrld	$11, %xmm4, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm4, %xmm5, %xmm5
	vpslld	$25, %xmm5, %xmm6
	vpsrld	$7, %xmm5, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpsrld	$3, %xmm4, %xmm6
	vmovdqa	%xmm4, %xmm14
	vpxor	%xmm6, %xmm5, %xmm5
	vmovdqa	%xmm13, 32(%rsp)        # 16-byte Spill
	vpslld	$30, %xmm13, %xmm6
	vpsrld	$2, %xmm13, %xmm4
	vpor	%xmm6, %xmm4, %xmm4
	vpxor	%xmm13, %xmm4, %xmm4
	vpslld	$15, %xmm4, %xmm6
	vpsrld	$17, %xmm4, %xmm4
	vpor	%xmm6, %xmm4, %xmm4
	vpsrld	$10, %xmm13, %xmm6
	vpxor	%xmm6, %xmm4, %xmm4
	vpaddd	%xmm5, %xmm11, %xmm5
	vpaddd	96(%rsp), %xmm5, %xmm5  # 16-byte Folded Reload
	vpaddd	%xmm4, %xmm5, %xmm13
	vpslld	$27, %xmm10, %xmm4
	vpsrld	$5, %xmm10, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm10, %xmm4, %xmm4
	vpslld	$26, %xmm4, %xmm5
	vpsrld	$6, %xmm4, %xmm4
	vpor	%xmm5, %xmm4, %xmm4
	vpslld	$7, %xmm10, %xmm5
	vpsrld	$25, %xmm10, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm7, %xmm8, %xmm5
	vpand	%xmm10, %xmm5, %xmm5
	vpxor	%xmm7, %xmm5, %xmm5
	vpaddd	%xmm9, %xmm5, %xmm5
	vpaddd	%xmm13, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm9
	vpslld	$30, %xmm1, %xmm5
	vpsrld	$2, %xmm1, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpslld	$19, %xmm1, %xmm6
	vpsrld	$13, %xmm1, %xmm4
	vpor	%xmm6, %xmm4, %xmm4
	vpxor	%xmm4, %xmm5, %xmm4
	vpslld	$10, %xmm1, %xmm5
	vpsrld	$22, %xmm1, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm3, %xmm0, %xmm5
	vpand	%xmm5, %xmm1, %xmm5
	vpand	%xmm3, %xmm0, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm4
	vpaddd	.LCPI1_30(%rip), %xmm9, %xmm6
	vpaddd	%xmm2, %xmm6, %xmm11
	vpaddd	%xmm6, %xmm4, %xmm2
	vmovdqa	48(%rsp), %xmm5         # 16-byte Reload
	vpslld	$21, %xmm5, %xmm4
	vpsrld	$11, %xmm5, %xmm6
	vpor	%xmm4, %xmm6, %xmm4
	vpxor	%xmm5, %xmm4, %xmm4
	vpslld	$25, %xmm4, %xmm6
	vpsrld	$7, %xmm4, %xmm4
	vpor	%xmm6, %xmm4, %xmm4
	vpsrld	$3, %xmm5, %xmm6
	vmovdqa	%xmm5, %xmm9
	vpxor	%xmm6, %xmm4, %xmm4
	vmovdqa	%xmm12, 128(%rsp)       # 16-byte Spill
	vpslld	$30, %xmm12, %xmm6
	vpsrld	$2, %xmm12, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpxor	%xmm12, %xmm5, %xmm5
	vpslld	$15, %xmm5, %xmm6
	vpsrld	$17, %xmm5, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpsrld	$10, %xmm12, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm4, %xmm14, %xmm4
	vpaddd	80(%rsp), %xmm4, %xmm4  # 16-byte Folded Reload
	vpaddd	%xmm5, %xmm4, %xmm12
	vpslld	$27, %xmm11, %xmm4
	vpsrld	$5, %xmm11, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm11, %xmm4, %xmm4
	vpslld	$26, %xmm4, %xmm5
	vpsrld	$6, %xmm4, %xmm4
	vpor	%xmm5, %xmm4, %xmm4
	vpslld	$7, %xmm11, %xmm5
	vpsrld	$25, %xmm11, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm8, %xmm10, %xmm5
	vpand	%xmm11, %xmm5, %xmm5
	vpxor	%xmm8, %xmm5, %xmm5
	vpaddd	%xmm7, %xmm5, %xmm5
	vpaddd	%xmm12, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm4
	vpslld	$30, %xmm2, %xmm5
	vpsrld	$2, %xmm2, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpslld	$19, %xmm2, %xmm6
	vpsrld	$13, %xmm2, %xmm7
	vpor	%xmm6, %xmm7, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpslld	$10, %xmm2, %xmm6
	vpsrld	$22, %xmm2, %xmm7
	vpor	%xmm6, %xmm7, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpxor	%xmm0, %xmm1, %xmm6
	vpand	%xmm6, %xmm2, %xmm6
	vpand	%xmm0, %xmm1, %xmm7
	vpxor	%xmm7, %xmm6, %xmm6
	vpaddd	%xmm6, %xmm5, %xmm5
	vpaddd	.LCPI1_31(%rip), %xmm4, %xmm4
	vpaddd	%xmm3, %xmm4, %xmm7
	vpaddd	%xmm4, %xmm5, %xmm3
	vmovdqa	-64(%rsp), %xmm14       # 16-byte Reload
	vpslld	$21, %xmm14, %xmm4
	vpsrld	$11, %xmm14, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm14, %xmm4, %xmm4
	vpslld	$25, %xmm4, %xmm5
	vpsrld	$7, %xmm4, %xmm4
	vpor	%xmm5, %xmm4, %xmm4
	vpsrld	$3, %xmm14, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vmovdqa	%xmm13, -16(%rsp)       # 16-byte Spill
	vpslld	$30, %xmm13, %xmm5
	vpsrld	$2, %xmm13, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm13, %xmm5, %xmm5
	vpslld	$15, %xmm5, %xmm6
	vpsrld	$17, %xmm5, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpsrld	$10, %xmm13, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm4, %xmm9, %xmm4
	vpaddd	64(%rsp), %xmm4, %xmm4  # 16-byte Folded Reload
	vpaddd	%xmm5, %xmm4, %xmm15
	vpslld	$27, %xmm7, %xmm4
	vpsrld	$5, %xmm7, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm7, %xmm4, %xmm4
	vpslld	$26, %xmm4, %xmm5
	vpsrld	$6, %xmm4, %xmm4
	vpor	%xmm5, %xmm4, %xmm4
	vpslld	$7, %xmm7, %xmm5
	vpsrld	$25, %xmm7, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm10, %xmm11, %xmm5
	vpand	%xmm7, %xmm5, %xmm5
	vpxor	%xmm10, %xmm5, %xmm5
	vpaddd	%xmm8, %xmm5, %xmm5
	vpaddd	%xmm15, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm8
	vpslld	$30, %xmm3, %xmm5
	vpsrld	$2, %xmm3, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpslld	$19, %xmm3, %xmm6
	vpsrld	$13, %xmm3, %xmm4
	vpor	%xmm6, %xmm4, %xmm4
	vpxor	%xmm4, %xmm5, %xmm4
	vpslld	$10, %xmm3, %xmm5
	vpsrld	$22, %xmm3, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm1, %xmm2, %xmm5
	vpand	%xmm5, %xmm3, %xmm5
	vpand	%xmm1, %xmm2, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm4
	vpaddd	.LCPI1_32(%rip), %xmm8, %xmm5
	vpaddd	%xmm0, %xmm5, %xmm9
	vpaddd	%xmm5, %xmm4, %xmm0
	vmovdqa	-80(%rsp), %xmm13       # 16-byte Reload
	vpslld	$21, %xmm13, %xmm4
	vpsrld	$11, %xmm13, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm13, %xmm4, %xmm4
	vpslld	$25, %xmm4, %xmm5
	vpsrld	$7, %xmm4, %xmm4
	vpor	%xmm5, %xmm4, %xmm4
	vpsrld	$3, %xmm13, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vmovdqa	%xmm12, -32(%rsp)       # 16-byte Spill
	vpslld	$30, %xmm12, %xmm5
	vpsrld	$2, %xmm12, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm12, %xmm5, %xmm5
	vpslld	$15, %xmm5, %xmm6
	vpsrld	$17, %xmm5, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpsrld	$10, %xmm12, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm4, %xmm14, %xmm4
	vpaddd	16(%rsp), %xmm4, %xmm4  # 16-byte Folded Reload
	vpaddd	%xmm5, %xmm4, %xmm14
	vpslld	$27, %xmm9, %xmm4
	vpsrld	$5, %xmm9, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm9, %xmm4, %xmm4
	vpslld	$26, %xmm4, %xmm5
	vpsrld	$6, %xmm4, %xmm4
	vpor	%xmm5, %xmm4, %xmm4
	vpslld	$7, %xmm9, %xmm5
	vpsrld	$25, %xmm9, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm11, %xmm7, %xmm5
	vpand	%xmm9, %xmm5, %xmm5
	vpxor	%xmm11, %xmm5, %xmm5
	vpaddd	%xmm10, %xmm5, %xmm5
	vpaddd	%xmm14, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm8
	vpslld	$30, %xmm0, %xmm5
	vpsrld	$2, %xmm0, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpslld	$19, %xmm0, %xmm6
	vpsrld	$13, %xmm0, %xmm4
	vpor	%xmm6, %xmm4, %xmm4
	vpxor	%xmm4, %xmm5, %xmm4
	vpslld	$10, %xmm0, %xmm5
	vpsrld	$22, %xmm0, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm2, %xmm3, %xmm5
	vpand	%xmm5, %xmm0, %xmm5
	vpand	%xmm2, %xmm3, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm5
	vpaddd	.LCPI1_33(%rip), %xmm8, %xmm6
	vpaddd	%xmm1, %xmm6, %xmm10
	vpaddd	%xmm6, %xmm5, %xmm1
	vmovdqa	112(%rsp), %xmm4        # 16-byte Reload
	vpslld	$21, %xmm4, %xmm5
	vpsrld	$11, %xmm4, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm4, %xmm5, %xmm5
	vpslld	$25, %xmm5, %xmm6
	vpsrld	$7, %xmm5, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpsrld	$3, %xmm4, %xmm6
	vmovdqa	%xmm4, %xmm12
	vpxor	%xmm6, %xmm5, %xmm5
	vmovdqa	%xmm15, 48(%rsp)        # 16-byte Spill
	vpslld	$30, %xmm15, %xmm6
	vpsrld	$2, %xmm15, %xmm4
	vpor	%xmm6, %xmm4, %xmm4
	vpxor	%xmm15, %xmm4, %xmm4
	vpslld	$15, %xmm4, %xmm6
	vpsrld	$17, %xmm4, %xmm4
	vpor	%xmm6, %xmm4, %xmm4
	vpsrld	$10, %xmm15, %xmm6
	vpxor	%xmm6, %xmm4, %xmm4
	vpaddd	%xmm5, %xmm13, %xmm5
	vpaddd	(%rsp), %xmm5, %xmm5    # 16-byte Folded Reload
	vpaddd	%xmm4, %xmm5, %xmm13
	vpslld	$27, %xmm10, %xmm4
	vpsrld	$5, %xmm10, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm10, %xmm4, %xmm4
	vpslld	$26, %xmm4, %xmm5
	vpsrld	$6, %xmm4, %xmm4
	vpor	%xmm5, %xmm4, %xmm4
	vpslld	$7, %xmm10, %xmm5
	vpsrld	$25, %xmm10, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm7, %xmm9, %xmm5
	vpand	%xmm10, %xmm5, %xmm5
	vpxor	%xmm7, %xmm5, %xmm5
	vpaddd	%xmm11, %xmm5, %xmm5
	vpaddd	%xmm13, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm8
	vpslld	$30, %xmm1, %xmm5
	vpsrld	$2, %xmm1, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpslld	$19, %xmm1, %xmm6
	vpsrld	$13, %xmm1, %xmm4
	vpor	%xmm6, %xmm4, %xmm4
	vpxor	%xmm4, %xmm5, %xmm4
	vpslld	$10, %xmm1, %xmm5
	vpsrld	$22, %xmm1, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm3, %xmm0, %xmm5
	vpand	%xmm5, %xmm1, %xmm5
	vpand	%xmm3, %xmm0, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm4
	vpaddd	.LCPI1_34(%rip), %xmm8, %xmm6
	vpaddd	%xmm2, %xmm6, %xmm11
	vpaddd	%xmm6, %xmm4, %xmm2
	vmovdqa	-112(%rsp), %xmm5       # 16-byte Reload
	vpslld	$21, %xmm5, %xmm4
	vpsrld	$11, %xmm5, %xmm6
	vpor	%xmm4, %xmm6, %xmm4
	vpxor	%xmm5, %xmm4, %xmm4
	vpslld	$25, %xmm4, %xmm6
	vpsrld	$7, %xmm4, %xmm4
	vpor	%xmm6, %xmm4, %xmm4
	vpsrld	$3, %xmm5, %xmm6
	vmovdqa	%xmm5, %xmm8
	vpxor	%xmm6, %xmm4, %xmm4
	vmovdqa	%xmm14, -48(%rsp)       # 16-byte Spill
	vpslld	$30, %xmm14, %xmm6
	vpsrld	$2, %xmm14, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpxor	%xmm14, %xmm5, %xmm5
	vpslld	$15, %xmm5, %xmm6
	vpsrld	$17, %xmm5, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpsrld	$10, %xmm14, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm4, %xmm12, %xmm4
	vpaddd	32(%rsp), %xmm4, %xmm4  # 16-byte Folded Reload
	vpaddd	%xmm5, %xmm4, %xmm12
	vpslld	$27, %xmm11, %xmm4
	vpsrld	$5, %xmm11, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm11, %xmm4, %xmm4
	vpslld	$26, %xmm4, %xmm5
	vpsrld	$6, %xmm4, %xmm4
	vpor	%xmm5, %xmm4, %xmm4
	vpslld	$7, %xmm11, %xmm5
	vpsrld	$25, %xmm11, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm9, %xmm10, %xmm5
	vpand	%xmm11, %xmm5, %xmm5
	vpxor	%xmm9, %xmm5, %xmm5
	vpaddd	%xmm7, %xmm5, %xmm5
	vpaddd	%xmm12, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm4
	vpslld	$30, %xmm2, %xmm5
	vpsrld	$2, %xmm2, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpslld	$19, %xmm2, %xmm6
	vpsrld	$13, %xmm2, %xmm7
	vpor	%xmm6, %xmm7, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpslld	$10, %xmm2, %xmm6
	vpsrld	$22, %xmm2, %xmm7
	vpor	%xmm6, %xmm7, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpxor	%xmm0, %xmm1, %xmm6
	vpand	%xmm6, %xmm2, %xmm6
	vpand	%xmm0, %xmm1, %xmm7
	vpxor	%xmm7, %xmm6, %xmm6
	vpaddd	%xmm6, %xmm5, %xmm5
	vpaddd	.LCPI1_35(%rip), %xmm4, %xmm4
	vpaddd	%xmm3, %xmm4, %xmm7
	vpaddd	%xmm4, %xmm5, %xmm3
	vmovdqa	-96(%rsp), %xmm14       # 16-byte Reload
	vpslld	$21, %xmm14, %xmm4
	vpsrld	$11, %xmm14, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm14, %xmm4, %xmm4
	vpslld	$25, %xmm4, %xmm5
	vpsrld	$7, %xmm4, %xmm4
	vpor	%xmm5, %xmm4, %xmm4
	vpsrld	$3, %xmm14, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vmovdqa	%xmm13, -64(%rsp)       # 16-byte Spill
	vpslld	$30, %xmm13, %xmm5
	vpsrld	$2, %xmm13, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm13, %xmm5, %xmm5
	vpslld	$15, %xmm5, %xmm6
	vpsrld	$17, %xmm5, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpsrld	$10, %xmm13, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm4, %xmm8, %xmm4
	vpaddd	128(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vpaddd	%xmm5, %xmm4, %xmm13
	vpslld	$27, %xmm7, %xmm4
	vpsrld	$5, %xmm7, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm7, %xmm4, %xmm4
	vpslld	$26, %xmm4, %xmm5
	vpsrld	$6, %xmm4, %xmm4
	vpor	%xmm5, %xmm4, %xmm4
	vpslld	$7, %xmm7, %xmm5
	vpsrld	$25, %xmm7, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm10, %xmm11, %xmm5
	vpand	%xmm7, %xmm5, %xmm5
	vpxor	%xmm10, %xmm5, %xmm5
	vpaddd	%xmm9, %xmm5, %xmm5
	vpaddd	%xmm13, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm9
	vpslld	$30, %xmm3, %xmm5
	vpsrld	$2, %xmm3, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpslld	$19, %xmm3, %xmm6
	vpsrld	$13, %xmm3, %xmm4
	vpor	%xmm6, %xmm4, %xmm4
	vpxor	%xmm4, %xmm5, %xmm4
	vpslld	$10, %xmm3, %xmm5
	vpsrld	$22, %xmm3, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm1, %xmm2, %xmm5
	vpand	%xmm5, %xmm3, %xmm5
	vpand	%xmm1, %xmm2, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm4
	vpaddd	.LCPI1_36(%rip), %xmm9, %xmm5
	vpaddd	%xmm0, %xmm5, %xmm8
	vpaddd	%xmm5, %xmm4, %xmm0
	vmovdqa	-128(%rsp), %xmm15      # 16-byte Reload
	vpslld	$21, %xmm15, %xmm4
	vpsrld	$11, %xmm15, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm15, %xmm4, %xmm4
	vpslld	$25, %xmm4, %xmm5
	vpsrld	$7, %xmm4, %xmm4
	vpor	%xmm5, %xmm4, %xmm4
	vpsrld	$3, %xmm15, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vmovdqa	%xmm12, -80(%rsp)       # 16-byte Spill
	vpslld	$30, %xmm12, %xmm5
	vpsrld	$2, %xmm12, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm12, %xmm5, %xmm5
	vpslld	$15, %xmm5, %xmm6
	vpsrld	$17, %xmm5, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpsrld	$10, %xmm12, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm4, %xmm14, %xmm4
	vpaddd	-16(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vpaddd	%xmm5, %xmm4, %xmm14
	vpslld	$27, %xmm8, %xmm4
	vpsrld	$5, %xmm8, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm8, %xmm4, %xmm4
	vpslld	$26, %xmm4, %xmm5
	vpsrld	$6, %xmm4, %xmm4
	vpor	%xmm5, %xmm4, %xmm4
	vpslld	$7, %xmm8, %xmm5
	vpsrld	$25, %xmm8, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm11, %xmm7, %xmm5
	vpand	%xmm8, %xmm5, %xmm5
	vpxor	%xmm11, %xmm5, %xmm5
	vpaddd	%xmm10, %xmm5, %xmm5
	vpaddd	%xmm14, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm10
	vpslld	$30, %xmm0, %xmm5
	vpsrld	$2, %xmm0, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpslld	$19, %xmm0, %xmm6
	vpsrld	$13, %xmm0, %xmm4
	vpor	%xmm6, %xmm4, %xmm4
	vpxor	%xmm4, %xmm5, %xmm4
	vpslld	$10, %xmm0, %xmm5
	vpsrld	$22, %xmm0, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm2, %xmm3, %xmm5
	vpand	%xmm5, %xmm0, %xmm5
	vpand	%xmm2, %xmm3, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm5
	vpaddd	.LCPI1_37(%rip), %xmm10, %xmm6
	vpaddd	%xmm1, %xmm6, %xmm9
	vpaddd	%xmm6, %xmm5, %xmm1
	vmovdqa	96(%rsp), %xmm4         # 16-byte Reload
	vpslld	$21, %xmm4, %xmm5
	vpsrld	$11, %xmm4, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm4, %xmm5, %xmm5
	vpslld	$25, %xmm5, %xmm6
	vpsrld	$7, %xmm5, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpsrld	$3, %xmm4, %xmm6
	vmovdqa	%xmm4, %xmm12
	vpxor	%xmm6, %xmm5, %xmm5
	vmovdqa	%xmm13, 112(%rsp)       # 16-byte Spill
	vpslld	$30, %xmm13, %xmm6
	vpsrld	$2, %xmm13, %xmm4
	vpor	%xmm6, %xmm4, %xmm4
	vpxor	%xmm13, %xmm4, %xmm4
	vpslld	$15, %xmm4, %xmm6
	vpsrld	$17, %xmm4, %xmm4
	vpor	%xmm6, %xmm4, %xmm4
	vpsrld	$10, %xmm13, %xmm6
	vpxor	%xmm6, %xmm4, %xmm4
	vpaddd	%xmm5, %xmm15, %xmm5
	vpaddd	-32(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vpaddd	%xmm4, %xmm5, %xmm15
	vpslld	$27, %xmm9, %xmm4
	vpsrld	$5, %xmm9, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm9, %xmm4, %xmm4
	vpslld	$26, %xmm4, %xmm5
	vpsrld	$6, %xmm4, %xmm4
	vpor	%xmm5, %xmm4, %xmm4
	vpslld	$7, %xmm9, %xmm5
	vpsrld	$25, %xmm9, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm7, %xmm8, %xmm5
	vpand	%xmm9, %xmm5, %xmm5
	vpxor	%xmm7, %xmm5, %xmm5
	vpaddd	%xmm11, %xmm5, %xmm5
	vpaddd	%xmm15, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm11
	vpslld	$30, %xmm1, %xmm5
	vpsrld	$2, %xmm1, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpslld	$19, %xmm1, %xmm6
	vpsrld	$13, %xmm1, %xmm4
	vpor	%xmm6, %xmm4, %xmm4
	vpxor	%xmm4, %xmm5, %xmm4
	vpslld	$10, %xmm1, %xmm5
	vpsrld	$22, %xmm1, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm3, %xmm0, %xmm5
	vpand	%xmm5, %xmm1, %xmm5
	vpand	%xmm3, %xmm0, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm4
	vpaddd	.LCPI1_38(%rip), %xmm11, %xmm6
	vpaddd	%xmm2, %xmm6, %xmm10
	vpaddd	%xmm6, %xmm4, %xmm2
	vmovdqa	80(%rsp), %xmm5         # 16-byte Reload
	vpslld	$21, %xmm5, %xmm4
	vpsrld	$11, %xmm5, %xmm6
	vpor	%xmm4, %xmm6, %xmm4
	vpxor	%xmm5, %xmm4, %xmm4
	vpslld	$25, %xmm4, %xmm6
	vpsrld	$7, %xmm4, %xmm4
	vpor	%xmm6, %xmm4, %xmm4
	vpsrld	$3, %xmm5, %xmm6
	vmovdqa	%xmm5, %xmm13
	vpxor	%xmm6, %xmm4, %xmm4
	vmovdqa	%xmm14, -112(%rsp)      # 16-byte Spill
	vpslld	$30, %xmm14, %xmm6
	vpsrld	$2, %xmm14, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpxor	%xmm14, %xmm5, %xmm5
	vpslld	$15, %xmm5, %xmm6
	vpsrld	$17, %xmm5, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpsrld	$10, %xmm14, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm4, %xmm12, %xmm4
	vpaddd	48(%rsp), %xmm4, %xmm4  # 16-byte Folded Reload
	vpaddd	%xmm5, %xmm4, %xmm11
	vpslld	$27, %xmm10, %xmm4
	vpsrld	$5, %xmm10, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm10, %xmm4, %xmm4
	vpslld	$26, %xmm4, %xmm5
	vpsrld	$6, %xmm4, %xmm4
	vpor	%xmm5, %xmm4, %xmm4
	vpslld	$7, %xmm10, %xmm5
	vpsrld	$25, %xmm10, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm8, %xmm9, %xmm5
	vpand	%xmm10, %xmm5, %xmm5
	vpxor	%xmm8, %xmm5, %xmm5
	vpaddd	%xmm7, %xmm5, %xmm5
	vpaddd	%xmm11, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm4
	vpslld	$30, %xmm2, %xmm5
	vpsrld	$2, %xmm2, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpslld	$19, %xmm2, %xmm6
	vpsrld	$13, %xmm2, %xmm7
	vpor	%xmm6, %xmm7, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpslld	$10, %xmm2, %xmm6
	vpsrld	$22, %xmm2, %xmm7
	vpor	%xmm6, %xmm7, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpxor	%xmm0, %xmm1, %xmm6
	vpand	%xmm6, %xmm2, %xmm6
	vpand	%xmm0, %xmm1, %xmm7
	vpxor	%xmm7, %xmm6, %xmm6
	vpaddd	%xmm6, %xmm5, %xmm5
	vpaddd	.LCPI1_39(%rip), %xmm4, %xmm4
	vpaddd	%xmm3, %xmm4, %xmm7
	vpaddd	%xmm4, %xmm5, %xmm3
	vmovdqa	64(%rsp), %xmm6         # 16-byte Reload
	vpslld	$21, %xmm6, %xmm4
	vpsrld	$11, %xmm6, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm6, %xmm4, %xmm4
	vpslld	$25, %xmm4, %xmm5
	vpsrld	$7, %xmm4, %xmm4
	vpor	%xmm5, %xmm4, %xmm4
	vpsrld	$3, %xmm6, %xmm5
	vmovdqa	%xmm6, %xmm12
	vpxor	%xmm5, %xmm4, %xmm4
	vmovdqa	%xmm15, -96(%rsp)       # 16-byte Spill
	vpslld	$30, %xmm15, %xmm5
	vpsrld	$2, %xmm15, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm15, %xmm5, %xmm5
	vpslld	$15, %xmm5, %xmm6
	vpsrld	$17, %xmm5, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpsrld	$10, %xmm15, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm4, %xmm13, %xmm4
	vpaddd	-48(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vpaddd	%xmm5, %xmm4, %xmm14
	vpslld	$27, %xmm7, %xmm4
	vpsrld	$5, %xmm7, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm7, %xmm4, %xmm4
	vpslld	$26, %xmm4, %xmm5
	vpsrld	$6, %xmm4, %xmm4
	vpor	%xmm5, %xmm4, %xmm4
	vpslld	$7, %xmm7, %xmm5
	vpsrld	$25, %xmm7, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm9, %xmm10, %xmm5
	vpand	%xmm7, %xmm5, %xmm5
	vpxor	%xmm9, %xmm5, %xmm5
	vpaddd	%xmm8, %xmm5, %xmm5
	vpaddd	%xmm14, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm8
	vpslld	$30, %xmm3, %xmm5
	vpsrld	$2, %xmm3, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpslld	$19, %xmm3, %xmm6
	vpsrld	$13, %xmm3, %xmm4
	vpor	%xmm6, %xmm4, %xmm4
	vpxor	%xmm4, %xmm5, %xmm4
	vpslld	$10, %xmm3, %xmm5
	vpsrld	$22, %xmm3, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm1, %xmm2, %xmm5
	vpand	%xmm5, %xmm3, %xmm5
	vpand	%xmm1, %xmm2, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm4
	vpaddd	.LCPI1_40(%rip), %xmm8, %xmm5
	vpaddd	%xmm0, %xmm5, %xmm8
	vpaddd	%xmm5, %xmm4, %xmm0
	vmovdqa	16(%rsp), %xmm6         # 16-byte Reload
	vpslld	$21, %xmm6, %xmm4
	vpsrld	$11, %xmm6, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm6, %xmm4, %xmm4
	vpslld	$25, %xmm4, %xmm5
	vpsrld	$7, %xmm4, %xmm4
	vpor	%xmm5, %xmm4, %xmm4
	vpsrld	$3, %xmm6, %xmm5
	vmovdqa	%xmm6, %xmm13
	vpxor	%xmm5, %xmm4, %xmm4
	vmovdqa	%xmm11, -128(%rsp)      # 16-byte Spill
	vpslld	$30, %xmm11, %xmm5
	vpsrld	$2, %xmm11, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm11, %xmm5, %xmm5
	vpslld	$15, %xmm5, %xmm6
	vpsrld	$17, %xmm5, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpsrld	$10, %xmm11, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm4, %xmm12, %xmm4
	vpaddd	-64(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vpaddd	%xmm5, %xmm4, %xmm11
	vpslld	$27, %xmm8, %xmm4
	vpsrld	$5, %xmm8, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm8, %xmm4, %xmm4
	vpslld	$26, %xmm4, %xmm5
	vpsrld	$6, %xmm4, %xmm4
	vpor	%xmm5, %xmm4, %xmm4
	vpslld	$7, %xmm8, %xmm5
	vpsrld	$25, %xmm8, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm10, %xmm7, %xmm5
	vpand	%xmm8, %xmm5, %xmm5
	vpxor	%xmm10, %xmm5, %xmm5
	vpaddd	%xmm9, %xmm5, %xmm5
	vpaddd	%xmm11, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm9
	vpslld	$30, %xmm0, %xmm5
	vpsrld	$2, %xmm0, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpslld	$19, %xmm0, %xmm6
	vpsrld	$13, %xmm0, %xmm4
	vpor	%xmm6, %xmm4, %xmm4
	vpxor	%xmm4, %xmm5, %xmm4
	vpslld	$10, %xmm0, %xmm5
	vpsrld	$22, %xmm0, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm2, %xmm3, %xmm5
	vpand	%xmm5, %xmm0, %xmm5
	vpand	%xmm2, %xmm3, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm5
	vpaddd	.LCPI1_41(%rip), %xmm9, %xmm6
	vpaddd	%xmm1, %xmm6, %xmm9
	vpaddd	%xmm6, %xmm5, %xmm1
	vmovdqa	(%rsp), %xmm4           # 16-byte Reload
	vpslld	$21, %xmm4, %xmm5
	vpsrld	$11, %xmm4, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm4, %xmm5, %xmm5
	vpslld	$25, %xmm5, %xmm6
	vpsrld	$7, %xmm5, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpsrld	$3, %xmm4, %xmm6
	vmovdqa	%xmm4, %xmm12
	vpxor	%xmm6, %xmm5, %xmm5
	vmovdqa	%xmm14, 96(%rsp)        # 16-byte Spill
	vpslld	$30, %xmm14, %xmm6
	vpsrld	$2, %xmm14, %xmm4
	vpor	%xmm6, %xmm4, %xmm4
	vpxor	%xmm14, %xmm4, %xmm4
	vpslld	$15, %xmm4, %xmm6
	vpsrld	$17, %xmm4, %xmm4
	vpor	%xmm6, %xmm4, %xmm4
	vpsrld	$10, %xmm14, %xmm6
	vpxor	%xmm6, %xmm4, %xmm4
	vpaddd	%xmm5, %xmm13, %xmm5
	vpaddd	-80(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vpaddd	%xmm4, %xmm5, %xmm14
	vpslld	$27, %xmm9, %xmm4
	vpsrld	$5, %xmm9, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm9, %xmm4, %xmm4
	vpslld	$26, %xmm4, %xmm5
	vpsrld	$6, %xmm4, %xmm4
	vpor	%xmm5, %xmm4, %xmm4
	vpslld	$7, %xmm9, %xmm5
	vpsrld	$25, %xmm9, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm7, %xmm8, %xmm5
	vpand	%xmm9, %xmm5, %xmm5
	vpxor	%xmm7, %xmm5, %xmm5
	vpaddd	%xmm10, %xmm5, %xmm5
	vpaddd	%xmm14, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm10
	vpslld	$30, %xmm1, %xmm5
	vpsrld	$2, %xmm1, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpslld	$19, %xmm1, %xmm6
	vpsrld	$13, %xmm1, %xmm4
	vpor	%xmm6, %xmm4, %xmm4
	vpxor	%xmm4, %xmm5, %xmm4
	vpslld	$10, %xmm1, %xmm5
	vpsrld	$22, %xmm1, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm3, %xmm0, %xmm5
	vpand	%xmm5, %xmm1, %xmm5
	vpand	%xmm3, %xmm0, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm4
	vpaddd	.LCPI1_42(%rip), %xmm10, %xmm6
	vpaddd	%xmm2, %xmm6, %xmm10
	vpaddd	%xmm6, %xmm4, %xmm2
	vmovdqa	32(%rsp), %xmm5         # 16-byte Reload
	vpslld	$21, %xmm5, %xmm4
	vpsrld	$11, %xmm5, %xmm6
	vpor	%xmm4, %xmm6, %xmm4
	vpxor	%xmm5, %xmm4, %xmm4
	vpslld	$25, %xmm4, %xmm6
	vpsrld	$7, %xmm4, %xmm4
	vpor	%xmm6, %xmm4, %xmm4
	vpsrld	$3, %xmm5, %xmm6
	vmovdqa	%xmm5, %xmm13
	vpxor	%xmm6, %xmm4, %xmm4
	vmovdqa	%xmm11, 80(%rsp)        # 16-byte Spill
	vpslld	$30, %xmm11, %xmm6
	vpsrld	$2, %xmm11, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpxor	%xmm11, %xmm5, %xmm5
	vpslld	$15, %xmm5, %xmm6
	vpsrld	$17, %xmm5, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpsrld	$10, %xmm11, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm4, %xmm12, %xmm4
	vpaddd	112(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vpaddd	%xmm5, %xmm4, %xmm15
	vpslld	$27, %xmm10, %xmm4
	vpsrld	$5, %xmm10, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm10, %xmm4, %xmm4
	vpslld	$26, %xmm4, %xmm5
	vpsrld	$6, %xmm4, %xmm4
	vpor	%xmm5, %xmm4, %xmm4
	vpslld	$7, %xmm10, %xmm5
	vpsrld	$25, %xmm10, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm8, %xmm9, %xmm5
	vpand	%xmm10, %xmm5, %xmm5
	vpxor	%xmm8, %xmm5, %xmm5
	vpaddd	%xmm7, %xmm5, %xmm5
	vpaddd	%xmm15, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm4
	vpslld	$30, %xmm2, %xmm5
	vpsrld	$2, %xmm2, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpslld	$19, %xmm2, %xmm6
	vpsrld	$13, %xmm2, %xmm7
	vpor	%xmm6, %xmm7, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpslld	$10, %xmm2, %xmm6
	vpsrld	$22, %xmm2, %xmm7
	vpor	%xmm6, %xmm7, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpxor	%xmm0, %xmm1, %xmm6
	vpand	%xmm6, %xmm2, %xmm6
	vpand	%xmm0, %xmm1, %xmm7
	vpxor	%xmm7, %xmm6, %xmm6
	vpaddd	%xmm6, %xmm5, %xmm5
	vpaddd	.LCPI1_43(%rip), %xmm4, %xmm4
	vpaddd	%xmm3, %xmm4, %xmm7
	vpaddd	%xmm4, %xmm5, %xmm3
	vmovdqa	128(%rsp), %xmm6        # 16-byte Reload
	vpslld	$21, %xmm6, %xmm4
	vpsrld	$11, %xmm6, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm6, %xmm4, %xmm4
	vpslld	$25, %xmm4, %xmm5
	vpsrld	$7, %xmm4, %xmm4
	vpor	%xmm5, %xmm4, %xmm4
	vpsrld	$3, %xmm6, %xmm5
	vmovdqa	%xmm6, %xmm11
	vpxor	%xmm5, %xmm4, %xmm4
	vmovdqa	%xmm14, 64(%rsp)        # 16-byte Spill
	vpslld	$30, %xmm14, %xmm5
	vpsrld	$2, %xmm14, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm14, %xmm5, %xmm5
	vpslld	$15, %xmm5, %xmm6
	vpsrld	$17, %xmm5, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpsrld	$10, %xmm14, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm4, %xmm13, %xmm4
	vpaddd	-112(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vpaddd	%xmm5, %xmm4, %xmm14
	vpslld	$27, %xmm7, %xmm4
	vpsrld	$5, %xmm7, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm7, %xmm4, %xmm4
	vpslld	$26, %xmm4, %xmm5
	vpsrld	$6, %xmm4, %xmm4
	vpor	%xmm5, %xmm4, %xmm4
	vpslld	$7, %xmm7, %xmm5
	vpsrld	$25, %xmm7, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm9, %xmm10, %xmm5
	vpand	%xmm7, %xmm5, %xmm5
	vpxor	%xmm9, %xmm5, %xmm5
	vpaddd	%xmm8, %xmm5, %xmm5
	vpaddd	%xmm14, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm8
	vpslld	$30, %xmm3, %xmm5
	vpsrld	$2, %xmm3, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpslld	$19, %xmm3, %xmm6
	vpsrld	$13, %xmm3, %xmm4
	vpor	%xmm6, %xmm4, %xmm4
	vpxor	%xmm4, %xmm5, %xmm4
	vpslld	$10, %xmm3, %xmm5
	vpsrld	$22, %xmm3, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm1, %xmm2, %xmm5
	vpand	%xmm5, %xmm3, %xmm5
	vpand	%xmm1, %xmm2, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm4
	vpaddd	.LCPI1_44(%rip), %xmm8, %xmm5
	vpaddd	%xmm0, %xmm5, %xmm8
	vpaddd	%xmm5, %xmm4, %xmm0
	vmovdqa	-16(%rsp), %xmm6        # 16-byte Reload
	vpslld	$21, %xmm6, %xmm4
	vpsrld	$11, %xmm6, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm6, %xmm4, %xmm4
	vpslld	$25, %xmm4, %xmm5
	vpsrld	$7, %xmm4, %xmm4
	vpor	%xmm5, %xmm4, %xmm4
	vpsrld	$3, %xmm6, %xmm5
	vmovdqa	%xmm6, %xmm12
	vpxor	%xmm5, %xmm4, %xmm4
	vmovdqa	%xmm15, 16(%rsp)        # 16-byte Spill
	vpslld	$30, %xmm15, %xmm5
	vpsrld	$2, %xmm15, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm15, %xmm5, %xmm5
	vpslld	$15, %xmm5, %xmm6
	vpsrld	$17, %xmm5, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpsrld	$10, %xmm15, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm4, %xmm11, %xmm4
	vpaddd	-96(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vpaddd	%xmm5, %xmm4, %xmm11
	vpslld	$27, %xmm8, %xmm4
	vpsrld	$5, %xmm8, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm8, %xmm4, %xmm4
	vpslld	$26, %xmm4, %xmm5
	vpsrld	$6, %xmm4, %xmm4
	vpor	%xmm5, %xmm4, %xmm4
	vpslld	$7, %xmm8, %xmm5
	vpsrld	$25, %xmm8, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm10, %xmm7, %xmm5
	vpand	%xmm8, %xmm5, %xmm5
	vpxor	%xmm10, %xmm5, %xmm5
	vpaddd	%xmm9, %xmm5, %xmm5
	vpaddd	%xmm11, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm9
	vpslld	$30, %xmm0, %xmm5
	vpsrld	$2, %xmm0, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpslld	$19, %xmm0, %xmm6
	vpsrld	$13, %xmm0, %xmm4
	vpor	%xmm6, %xmm4, %xmm4
	vpxor	%xmm4, %xmm5, %xmm4
	vpslld	$10, %xmm0, %xmm5
	vpsrld	$22, %xmm0, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm2, %xmm3, %xmm5
	vpand	%xmm5, %xmm0, %xmm5
	vpand	%xmm2, %xmm3, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm5
	vpaddd	.LCPI1_45(%rip), %xmm9, %xmm6
	vpaddd	%xmm1, %xmm6, %xmm9
	vpaddd	%xmm6, %xmm5, %xmm1
	vmovdqa	-32(%rsp), %xmm4        # 16-byte Reload
	vpslld	$21, %xmm4, %xmm5
	vpsrld	$11, %xmm4, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm4, %xmm5, %xmm5
	vpslld	$25, %xmm5, %xmm6
	vpsrld	$7, %xmm5, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpsrld	$3, %xmm4, %xmm6
	vmovdqa	%xmm4, %xmm13
	vpxor	%xmm6, %xmm5, %xmm5
	vmovdqa	%xmm14, 32(%rsp)        # 16-byte Spill
	vpslld	$30, %xmm14, %xmm6
	vpsrld	$2, %xmm14, %xmm4
	vpor	%xmm6, %xmm4, %xmm4
	vpxor	%xmm14, %xmm4, %xmm4
	vpslld	$15, %xmm4, %xmm6
	vpsrld	$17, %xmm4, %xmm4
	vpor	%xmm6, %xmm4, %xmm4
	vpsrld	$10, %xmm14, %xmm6
	vpxor	%xmm6, %xmm4, %xmm4
	vpaddd	%xmm5, %xmm12, %xmm5
	vpaddd	-128(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vpaddd	%xmm4, %xmm5, %xmm15
	vpslld	$27, %xmm9, %xmm4
	vpsrld	$5, %xmm9, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm9, %xmm4, %xmm4
	vpslld	$26, %xmm4, %xmm5
	vpsrld	$6, %xmm4, %xmm4
	vpor	%xmm5, %xmm4, %xmm4
	vpslld	$7, %xmm9, %xmm5
	vpsrld	$25, %xmm9, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm7, %xmm8, %xmm5
	vpand	%xmm9, %xmm5, %xmm5
	vpxor	%xmm7, %xmm5, %xmm5
	vpaddd	%xmm10, %xmm5, %xmm5
	vpaddd	%xmm15, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm10
	vpslld	$30, %xmm1, %xmm5
	vpsrld	$2, %xmm1, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpslld	$19, %xmm1, %xmm6
	vpsrld	$13, %xmm1, %xmm4
	vpor	%xmm6, %xmm4, %xmm4
	vpxor	%xmm4, %xmm5, %xmm4
	vpslld	$10, %xmm1, %xmm5
	vpsrld	$22, %xmm1, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm3, %xmm0, %xmm5
	vpand	%xmm5, %xmm1, %xmm5
	vpand	%xmm3, %xmm0, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm4
	vpaddd	.LCPI1_46(%rip), %xmm10, %xmm6
	vpaddd	%xmm2, %xmm6, %xmm10
	vpaddd	%xmm6, %xmm4, %xmm2
	vmovdqa	48(%rsp), %xmm5         # 16-byte Reload
	vpslld	$21, %xmm5, %xmm4
	vpsrld	$11, %xmm5, %xmm6
	vpor	%xmm4, %xmm6, %xmm4
	vpxor	%xmm5, %xmm4, %xmm4
	vpslld	$25, %xmm4, %xmm6
	vpsrld	$7, %xmm4, %xmm4
	vpor	%xmm6, %xmm4, %xmm4
	vpsrld	$3, %xmm5, %xmm6
	vmovdqa	%xmm5, %xmm14
	vpxor	%xmm6, %xmm4, %xmm4
	vmovdqa	%xmm11, (%rsp)          # 16-byte Spill
	vpslld	$30, %xmm11, %xmm6
	vpsrld	$2, %xmm11, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpxor	%xmm11, %xmm5, %xmm5
	vpslld	$15, %xmm5, %xmm6
	vpsrld	$17, %xmm5, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpsrld	$10, %xmm11, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm4, %xmm13, %xmm4
	vpaddd	96(%rsp), %xmm4, %xmm4  # 16-byte Folded Reload
	vpaddd	%xmm5, %xmm4, %xmm11
	vpslld	$27, %xmm10, %xmm4
	vpsrld	$5, %xmm10, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm10, %xmm4, %xmm4
	vpslld	$26, %xmm4, %xmm5
	vpsrld	$6, %xmm4, %xmm4
	vpor	%xmm5, %xmm4, %xmm4
	vpslld	$7, %xmm10, %xmm5
	vpsrld	$25, %xmm10, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm8, %xmm9, %xmm5
	vpand	%xmm10, %xmm5, %xmm5
	vpxor	%xmm8, %xmm5, %xmm5
	vpaddd	%xmm7, %xmm5, %xmm5
	vpaddd	%xmm11, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm4
	vpslld	$30, %xmm2, %xmm5
	vpsrld	$2, %xmm2, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpslld	$19, %xmm2, %xmm6
	vpsrld	$13, %xmm2, %xmm7
	vpor	%xmm6, %xmm7, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpslld	$10, %xmm2, %xmm6
	vpsrld	$22, %xmm2, %xmm7
	vpor	%xmm6, %xmm7, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpxor	%xmm0, %xmm1, %xmm6
	vpand	%xmm6, %xmm2, %xmm6
	vpand	%xmm0, %xmm1, %xmm7
	vpxor	%xmm7, %xmm6, %xmm6
	vpaddd	%xmm6, %xmm5, %xmm5
	vpaddd	.LCPI1_47(%rip), %xmm4, %xmm4
	vpaddd	%xmm3, %xmm4, %xmm7
	vpaddd	%xmm4, %xmm5, %xmm3
	vmovdqa	-48(%rsp), %xmm6        # 16-byte Reload
	vpslld	$21, %xmm6, %xmm4
	vpsrld	$11, %xmm6, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm6, %xmm4, %xmm4
	vpslld	$25, %xmm4, %xmm5
	vpsrld	$7, %xmm4, %xmm4
	vpor	%xmm5, %xmm4, %xmm4
	vpsrld	$3, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vmovdqa	%xmm15, -16(%rsp)       # 16-byte Spill
	vpslld	$30, %xmm15, %xmm5
	vpsrld	$2, %xmm15, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm15, %xmm5, %xmm5
	vpslld	$15, %xmm5, %xmm6
	vpsrld	$17, %xmm5, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpsrld	$10, %xmm15, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm4, %xmm14, %xmm4
	vpaddd	80(%rsp), %xmm4, %xmm4  # 16-byte Folded Reload
	vpaddd	%xmm5, %xmm4, %xmm12
	vpslld	$27, %xmm7, %xmm4
	vpsrld	$5, %xmm7, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm7, %xmm4, %xmm4
	vpslld	$26, %xmm4, %xmm5
	vpsrld	$6, %xmm4, %xmm4
	vpor	%xmm5, %xmm4, %xmm4
	vpslld	$7, %xmm7, %xmm5
	vpsrld	$25, %xmm7, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm9, %xmm10, %xmm5
	vpand	%xmm7, %xmm5, %xmm5
	vpxor	%xmm9, %xmm5, %xmm5
	vpaddd	%xmm8, %xmm5, %xmm5
	vpaddd	%xmm12, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm8
	vpslld	$30, %xmm3, %xmm5
	vpsrld	$2, %xmm3, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpslld	$19, %xmm3, %xmm6
	vpsrld	$13, %xmm3, %xmm4
	vpor	%xmm6, %xmm4, %xmm4
	vpxor	%xmm4, %xmm5, %xmm4
	vpslld	$10, %xmm3, %xmm5
	vpsrld	$22, %xmm3, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm1, %xmm2, %xmm5
	vpand	%xmm5, %xmm3, %xmm5
	vpand	%xmm1, %xmm2, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm4
	vpaddd	.LCPI1_48(%rip), %xmm8, %xmm5
	vpaddd	%xmm0, %xmm5, %xmm14
	vpaddd	%xmm5, %xmm4, %xmm0
	vmovdqa	-64(%rsp), %xmm6        # 16-byte Reload
	vpslld	$21, %xmm6, %xmm4
	vpsrld	$11, %xmm6, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm6, %xmm4, %xmm4
	vpslld	$25, %xmm4, %xmm5
	vpsrld	$7, %xmm4, %xmm4
	vpor	%xmm5, %xmm4, %xmm4
	vpsrld	$3, %xmm6, %xmm5
	vmovdqa	%xmm6, %xmm13
	vpxor	%xmm5, %xmm4, %xmm4
	vmovdqa	%xmm11, 128(%rsp)       # 16-byte Spill
	vpslld	$30, %xmm11, %xmm5
	vpsrld	$2, %xmm11, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm11, %xmm5, %xmm5
	vpslld	$15, %xmm5, %xmm6
	vpsrld	$17, %xmm5, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpsrld	$10, %xmm11, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	-48(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vpaddd	64(%rsp), %xmm4, %xmm4  # 16-byte Folded Reload
	vpaddd	%xmm5, %xmm4, %xmm11
	vpslld	$27, %xmm14, %xmm4
	vpsrld	$5, %xmm14, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm14, %xmm4, %xmm4
	vpslld	$26, %xmm4, %xmm5
	vpsrld	$6, %xmm4, %xmm4
	vpor	%xmm5, %xmm4, %xmm4
	vpslld	$7, %xmm14, %xmm5
	vpsrld	$25, %xmm14, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm10, %xmm7, %xmm5
	vpand	%xmm14, %xmm5, %xmm5
	vpxor	%xmm10, %xmm5, %xmm5
	vpaddd	%xmm9, %xmm5, %xmm5
	vpaddd	%xmm11, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm8
	vpslld	$30, %xmm0, %xmm5
	vpsrld	$2, %xmm0, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpslld	$19, %xmm0, %xmm6
	vpsrld	$13, %xmm0, %xmm4
	vpor	%xmm6, %xmm4, %xmm4
	vpxor	%xmm4, %xmm5, %xmm4
	vpslld	$10, %xmm0, %xmm5
	vpsrld	$22, %xmm0, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm2, %xmm3, %xmm5
	vpand	%xmm5, %xmm0, %xmm5
	vpand	%xmm2, %xmm3, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm5
	vpaddd	.LCPI1_49(%rip), %xmm8, %xmm6
	vpaddd	%xmm1, %xmm6, %xmm8
	vpaddd	%xmm6, %xmm5, %xmm1
	vmovdqa	-80(%rsp), %xmm4        # 16-byte Reload
	vpslld	$21, %xmm4, %xmm5
	vpsrld	$11, %xmm4, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm4, %xmm5, %xmm5
	vpslld	$25, %xmm5, %xmm6
	vpsrld	$7, %xmm5, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpsrld	$3, %xmm4, %xmm6
	vmovdqa	%xmm4, %xmm15
	vpxor	%xmm6, %xmm5, %xmm5
	vmovdqa	%xmm12, -32(%rsp)       # 16-byte Spill
	vpslld	$30, %xmm12, %xmm6
	vpsrld	$2, %xmm12, %xmm4
	vpor	%xmm6, %xmm4, %xmm4
	vpxor	%xmm12, %xmm4, %xmm4
	vpslld	$15, %xmm4, %xmm6
	vpsrld	$17, %xmm4, %xmm4
	vpor	%xmm6, %xmm4, %xmm4
	vpsrld	$10, %xmm12, %xmm6
	vpxor	%xmm6, %xmm4, %xmm4
	vpaddd	%xmm5, %xmm13, %xmm5
	vpaddd	16(%rsp), %xmm5, %xmm5  # 16-byte Folded Reload
	vpaddd	%xmm4, %xmm5, %xmm12
	vpslld	$27, %xmm8, %xmm4
	vpsrld	$5, %xmm8, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm8, %xmm4, %xmm4
	vpslld	$26, %xmm4, %xmm5
	vpsrld	$6, %xmm4, %xmm4
	vpor	%xmm5, %xmm4, %xmm4
	vpslld	$7, %xmm8, %xmm5
	vpsrld	$25, %xmm8, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm7, %xmm14, %xmm5
	vpand	%xmm8, %xmm5, %xmm5
	vpxor	%xmm7, %xmm5, %xmm5
	vpaddd	%xmm10, %xmm5, %xmm5
	vpaddd	%xmm12, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm9
	vpslld	$30, %xmm1, %xmm5
	vpsrld	$2, %xmm1, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpslld	$19, %xmm1, %xmm6
	vpsrld	$13, %xmm1, %xmm4
	vpor	%xmm6, %xmm4, %xmm4
	vpxor	%xmm4, %xmm5, %xmm4
	vpslld	$10, %xmm1, %xmm5
	vpsrld	$22, %xmm1, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm3, %xmm0, %xmm5
	vpand	%xmm5, %xmm1, %xmm5
	vpand	%xmm3, %xmm0, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm4
	vpaddd	.LCPI1_50(%rip), %xmm9, %xmm6
	vpaddd	%xmm2, %xmm6, %xmm9
	vpaddd	%xmm6, %xmm4, %xmm2
	vmovdqa	112(%rsp), %xmm5        # 16-byte Reload
	vpslld	$21, %xmm5, %xmm4
	vpsrld	$11, %xmm5, %xmm6
	vpor	%xmm4, %xmm6, %xmm4
	vpxor	%xmm5, %xmm4, %xmm4
	vpslld	$25, %xmm4, %xmm6
	vpsrld	$7, %xmm4, %xmm4
	vpor	%xmm6, %xmm4, %xmm4
	vpsrld	$3, %xmm5, %xmm6
	vmovdqa	%xmm5, %xmm10
	vpxor	%xmm6, %xmm4, %xmm4
	vmovdqa	%xmm11, -48(%rsp)       # 16-byte Spill
	vpslld	$30, %xmm11, %xmm6
	vpsrld	$2, %xmm11, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpxor	%xmm11, %xmm5, %xmm5
	vpslld	$15, %xmm5, %xmm6
	vpsrld	$17, %xmm5, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpsrld	$10, %xmm11, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm4, %xmm15, %xmm4
	vpaddd	32(%rsp), %xmm4, %xmm4  # 16-byte Folded Reload
	vpaddd	%xmm5, %xmm4, %xmm11
	vpslld	$27, %xmm9, %xmm4
	vpsrld	$5, %xmm9, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm9, %xmm4, %xmm4
	vpslld	$26, %xmm4, %xmm5
	vpsrld	$6, %xmm4, %xmm4
	vpor	%xmm5, %xmm4, %xmm4
	vpslld	$7, %xmm9, %xmm5
	vpsrld	$25, %xmm9, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm14, %xmm8, %xmm5
	vpand	%xmm9, %xmm5, %xmm5
	vpxor	%xmm14, %xmm5, %xmm5
	vpaddd	%xmm7, %xmm5, %xmm5
	vpaddd	%xmm11, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm4
	vpslld	$30, %xmm2, %xmm5
	vpsrld	$2, %xmm2, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpslld	$19, %xmm2, %xmm6
	vpsrld	$13, %xmm2, %xmm7
	vpor	%xmm6, %xmm7, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpslld	$10, %xmm2, %xmm6
	vpsrld	$22, %xmm2, %xmm7
	vpor	%xmm6, %xmm7, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpxor	%xmm0, %xmm1, %xmm6
	vpand	%xmm6, %xmm2, %xmm6
	vpand	%xmm0, %xmm1, %xmm7
	vpxor	%xmm7, %xmm6, %xmm6
	vpaddd	%xmm6, %xmm5, %xmm5
	vpaddd	.LCPI1_51(%rip), %xmm4, %xmm4
	vpaddd	%xmm3, %xmm4, %xmm7
	vpaddd	%xmm4, %xmm5, %xmm3
	vmovdqa	-112(%rsp), %xmm13      # 16-byte Reload
	vpslld	$21, %xmm13, %xmm4
	vpsrld	$11, %xmm13, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm13, %xmm4, %xmm4
	vpslld	$25, %xmm4, %xmm5
	vpsrld	$7, %xmm4, %xmm4
	vpor	%xmm5, %xmm4, %xmm4
	vpsrld	$3, %xmm13, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vmovdqa	%xmm12, 48(%rsp)        # 16-byte Spill
	vpslld	$30, %xmm12, %xmm5
	vpsrld	$2, %xmm12, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm12, %xmm5, %xmm5
	vpslld	$15, %xmm5, %xmm6
	vpsrld	$17, %xmm5, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpsrld	$10, %xmm12, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm4, %xmm10, %xmm4
	vpaddd	(%rsp), %xmm4, %xmm4    # 16-byte Folded Reload
	vpaddd	%xmm5, %xmm4, %xmm15
	vpslld	$27, %xmm7, %xmm4
	vpsrld	$5, %xmm7, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm7, %xmm4, %xmm4
	vpslld	$26, %xmm4, %xmm5
	vpsrld	$6, %xmm4, %xmm4
	vpor	%xmm5, %xmm4, %xmm4
	vpslld	$7, %xmm7, %xmm5
	vpsrld	$25, %xmm7, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm8, %xmm9, %xmm5
	vpand	%xmm7, %xmm5, %xmm5
	vpxor	%xmm8, %xmm5, %xmm5
	vpaddd	%xmm14, %xmm5, %xmm5
	vpaddd	%xmm15, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm10
	vpslld	$30, %xmm3, %xmm5
	vpsrld	$2, %xmm3, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpslld	$19, %xmm3, %xmm6
	vpsrld	$13, %xmm3, %xmm4
	vpor	%xmm6, %xmm4, %xmm4
	vpxor	%xmm4, %xmm5, %xmm4
	vpslld	$10, %xmm3, %xmm5
	vpsrld	$22, %xmm3, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm1, %xmm2, %xmm5
	vpand	%xmm5, %xmm3, %xmm5
	vpand	%xmm1, %xmm2, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm4
	vpaddd	.LCPI1_52(%rip), %xmm10, %xmm5
	vpaddd	%xmm0, %xmm5, %xmm10
	vpaddd	%xmm5, %xmm4, %xmm0
	vmovdqa	-96(%rsp), %xmm6        # 16-byte Reload
	vpslld	$21, %xmm6, %xmm4
	vpsrld	$11, %xmm6, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm6, %xmm4, %xmm4
	vpslld	$25, %xmm4, %xmm5
	vpsrld	$7, %xmm4, %xmm4
	vpor	%xmm5, %xmm4, %xmm4
	vpsrld	$3, %xmm6, %xmm5
	vmovdqa	%xmm6, %xmm14
	vpxor	%xmm5, %xmm4, %xmm4
	vmovdqa	%xmm11, -64(%rsp)       # 16-byte Spill
	vpslld	$30, %xmm11, %xmm5
	vpsrld	$2, %xmm11, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm11, %xmm5, %xmm5
	vpslld	$15, %xmm5, %xmm6
	vpsrld	$17, %xmm5, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpsrld	$10, %xmm11, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm4, %xmm13, %xmm4
	vpaddd	-16(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vpaddd	%xmm5, %xmm4, %xmm12
	vpslld	$27, %xmm10, %xmm4
	vpsrld	$5, %xmm10, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm10, %xmm4, %xmm4
	vpslld	$26, %xmm4, %xmm5
	vpsrld	$6, %xmm4, %xmm4
	vpor	%xmm5, %xmm4, %xmm4
	vpslld	$7, %xmm10, %xmm5
	vpsrld	$25, %xmm10, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm9, %xmm7, %xmm5
	vpand	%xmm10, %xmm5, %xmm5
	vpxor	%xmm9, %xmm5, %xmm5
	vpaddd	%xmm8, %xmm5, %xmm5
	vpaddd	%xmm12, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm8
	vpslld	$30, %xmm0, %xmm5
	vpsrld	$2, %xmm0, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpslld	$19, %xmm0, %xmm6
	vpsrld	$13, %xmm0, %xmm4
	vpor	%xmm6, %xmm4, %xmm4
	vpxor	%xmm4, %xmm5, %xmm4
	vpslld	$10, %xmm0, %xmm5
	vpsrld	$22, %xmm0, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm2, %xmm3, %xmm5
	vpand	%xmm5, %xmm0, %xmm5
	vpand	%xmm2, %xmm3, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm5
	vpaddd	.LCPI1_53(%rip), %xmm8, %xmm6
	vpaddd	%xmm1, %xmm6, %xmm8
	vpaddd	%xmm6, %xmm5, %xmm1
	vmovdqa	-128(%rsp), %xmm4       # 16-byte Reload
	vpslld	$21, %xmm4, %xmm5
	vpsrld	$11, %xmm4, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm4, %xmm5, %xmm5
	vpslld	$25, %xmm5, %xmm6
	vpsrld	$7, %xmm5, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpsrld	$3, %xmm4, %xmm6
	vmovdqa	%xmm4, %xmm11
	vpxor	%xmm6, %xmm5, %xmm5
	vpslld	$30, %xmm15, %xmm6
	vpsrld	$2, %xmm15, %xmm4
	vpor	%xmm6, %xmm4, %xmm4
	vpxor	%xmm15, %xmm4, %xmm4
	vpslld	$15, %xmm4, %xmm6
	vpsrld	$17, %xmm4, %xmm4
	vpor	%xmm6, %xmm4, %xmm4
	vpsrld	$10, %xmm15, %xmm6
	vpxor	%xmm6, %xmm4, %xmm4
	vpaddd	%xmm5, %xmm14, %xmm5
	vpaddd	128(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vpaddd	%xmm4, %xmm5, %xmm13
	vpslld	$27, %xmm8, %xmm4
	vpsrld	$5, %xmm8, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm8, %xmm4, %xmm4
	vpslld	$26, %xmm4, %xmm5
	vpsrld	$6, %xmm4, %xmm4
	vpor	%xmm5, %xmm4, %xmm4
	vpslld	$7, %xmm8, %xmm5
	vpsrld	$25, %xmm8, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm7, %xmm10, %xmm5
	vpand	%xmm8, %xmm5, %xmm5
	vpxor	%xmm7, %xmm5, %xmm5
	vpaddd	%xmm9, %xmm5, %xmm5
	vpaddd	%xmm13, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm9
	vpslld	$30, %xmm1, %xmm5
	vpsrld	$2, %xmm1, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpslld	$19, %xmm1, %xmm6
	vpsrld	$13, %xmm1, %xmm4
	vpor	%xmm6, %xmm4, %xmm4
	vpxor	%xmm4, %xmm5, %xmm4
	vpslld	$10, %xmm1, %xmm5
	vpsrld	$22, %xmm1, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm3, %xmm0, %xmm5
	vpand	%xmm5, %xmm1, %xmm5
	vpand	%xmm3, %xmm0, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm4
	vpaddd	.LCPI1_54(%rip), %xmm9, %xmm6
	vpaddd	%xmm2, %xmm6, %xmm9
	vpaddd	%xmm6, %xmm4, %xmm2
	vmovdqa	96(%rsp), %xmm5         # 16-byte Reload
	vpslld	$21, %xmm5, %xmm4
	vpsrld	$11, %xmm5, %xmm6
	vpor	%xmm4, %xmm6, %xmm4
	vpxor	%xmm5, %xmm4, %xmm4
	vpslld	$25, %xmm4, %xmm6
	vpsrld	$7, %xmm4, %xmm4
	vpor	%xmm6, %xmm4, %xmm4
	vpsrld	$3, %xmm5, %xmm6
	vmovdqa	%xmm5, %xmm14
	vpxor	%xmm6, %xmm4, %xmm4
	vmovdqa	%xmm12, -80(%rsp)       # 16-byte Spill
	vpslld	$30, %xmm12, %xmm6
	vpsrld	$2, %xmm12, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpxor	%xmm12, %xmm5, %xmm5
	vpslld	$15, %xmm5, %xmm6
	vpsrld	$17, %xmm5, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpsrld	$10, %xmm12, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm4, %xmm11, %xmm4
	vpaddd	-32(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vpaddd	%xmm5, %xmm4, %xmm12
	vpslld	$27, %xmm9, %xmm4
	vpsrld	$5, %xmm9, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm9, %xmm4, %xmm4
	vpslld	$26, %xmm4, %xmm5
	vpsrld	$6, %xmm4, %xmm4
	vpor	%xmm5, %xmm4, %xmm4
	vpslld	$7, %xmm9, %xmm5
	vpsrld	$25, %xmm9, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm10, %xmm8, %xmm5
	vpand	%xmm9, %xmm5, %xmm5
	vpxor	%xmm10, %xmm5, %xmm5
	vpaddd	%xmm7, %xmm5, %xmm5
	vpaddd	%xmm12, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm4
	vpslld	$30, %xmm2, %xmm5
	vpsrld	$2, %xmm2, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpslld	$19, %xmm2, %xmm6
	vpsrld	$13, %xmm2, %xmm7
	vpor	%xmm6, %xmm7, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpslld	$10, %xmm2, %xmm6
	vpsrld	$22, %xmm2, %xmm7
	vpor	%xmm6, %xmm7, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpxor	%xmm0, %xmm1, %xmm6
	vpand	%xmm6, %xmm2, %xmm6
	vpand	%xmm0, %xmm1, %xmm7
	vpxor	%xmm7, %xmm6, %xmm6
	vpaddd	%xmm6, %xmm5, %xmm5
	vpaddd	.LCPI1_55(%rip), %xmm4, %xmm4
	vpaddd	%xmm3, %xmm4, %xmm7
	vpaddd	%xmm4, %xmm5, %xmm3
	vmovdqa	80(%rsp), %xmm6         # 16-byte Reload
	vpslld	$21, %xmm6, %xmm4
	vpsrld	$11, %xmm6, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm6, %xmm4, %xmm4
	vpslld	$25, %xmm4, %xmm5
	vpsrld	$7, %xmm4, %xmm4
	vpor	%xmm5, %xmm4, %xmm4
	vpsrld	$3, %xmm6, %xmm5
	vmovdqa	%xmm6, %xmm11
	vpxor	%xmm5, %xmm4, %xmm4
	vmovdqa	%xmm13, 112(%rsp)       # 16-byte Spill
	vpslld	$30, %xmm13, %xmm5
	vpsrld	$2, %xmm13, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm13, %xmm5, %xmm5
	vpslld	$15, %xmm5, %xmm6
	vpsrld	$17, %xmm5, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpsrld	$10, %xmm13, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm4, %xmm14, %xmm4
	vpaddd	-48(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vpaddd	%xmm5, %xmm4, %xmm14
	vpslld	$27, %xmm7, %xmm4
	vpsrld	$5, %xmm7, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm7, %xmm4, %xmm4
	vpslld	$26, %xmm4, %xmm5
	vpsrld	$6, %xmm4, %xmm4
	vpor	%xmm5, %xmm4, %xmm4
	vpslld	$7, %xmm7, %xmm5
	vpsrld	$25, %xmm7, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm8, %xmm9, %xmm5
	vpand	%xmm7, %xmm5, %xmm5
	vpxor	%xmm8, %xmm5, %xmm5
	vpaddd	%xmm10, %xmm5, %xmm5
	vpaddd	%xmm14, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm10
	vpslld	$30, %xmm3, %xmm5
	vpsrld	$2, %xmm3, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpslld	$19, %xmm3, %xmm6
	vpsrld	$13, %xmm3, %xmm4
	vpor	%xmm6, %xmm4, %xmm4
	vpxor	%xmm4, %xmm5, %xmm4
	vpslld	$10, %xmm3, %xmm5
	vpsrld	$22, %xmm3, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm1, %xmm2, %xmm5
	vpand	%xmm5, %xmm3, %xmm5
	vpand	%xmm1, %xmm2, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm4
	vpaddd	.LCPI1_56(%rip), %xmm10, %xmm5
	vpaddd	%xmm0, %xmm5, %xmm10
	vpaddd	%xmm5, %xmm4, %xmm0
	vmovdqa	64(%rsp), %xmm13        # 16-byte Reload
	vpslld	$21, %xmm13, %xmm4
	vpsrld	$11, %xmm13, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm13, %xmm4, %xmm4
	vpslld	$25, %xmm4, %xmm5
	vpsrld	$7, %xmm4, %xmm4
	vpor	%xmm5, %xmm4, %xmm4
	vpsrld	$3, %xmm13, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vmovdqa	%xmm12, -112(%rsp)      # 16-byte Spill
	vpslld	$30, %xmm12, %xmm5
	vpsrld	$2, %xmm12, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm12, %xmm5, %xmm5
	vpslld	$15, %xmm5, %xmm6
	vpsrld	$17, %xmm5, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpsrld	$10, %xmm12, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm4, %xmm11, %xmm4
	vpaddd	48(%rsp), %xmm4, %xmm4  # 16-byte Folded Reload
	vpaddd	%xmm5, %xmm4, %xmm11
	vpslld	$27, %xmm10, %xmm4
	vpsrld	$5, %xmm10, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm10, %xmm4, %xmm4
	vpslld	$26, %xmm4, %xmm5
	vpsrld	$6, %xmm4, %xmm4
	vpor	%xmm5, %xmm4, %xmm4
	vpslld	$7, %xmm10, %xmm5
	vpsrld	$25, %xmm10, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm9, %xmm7, %xmm5
	vpand	%xmm10, %xmm5, %xmm5
	vpxor	%xmm9, %xmm5, %xmm5
	vpaddd	%xmm8, %xmm5, %xmm5
	vpaddd	%xmm11, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm8
	vpslld	$30, %xmm0, %xmm5
	vpsrld	$2, %xmm0, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpslld	$19, %xmm0, %xmm6
	vpsrld	$13, %xmm0, %xmm4
	vpor	%xmm6, %xmm4, %xmm4
	vpxor	%xmm4, %xmm5, %xmm4
	vpslld	$10, %xmm0, %xmm5
	vpsrld	$22, %xmm0, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm2, %xmm3, %xmm5
	vpand	%xmm5, %xmm0, %xmm5
	vpand	%xmm2, %xmm3, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm5
	vpaddd	.LCPI1_57(%rip), %xmm8, %xmm6
	vpaddd	%xmm1, %xmm6, %xmm12
	vpaddd	%xmm6, %xmm5, %xmm1
	vmovdqa	16(%rsp), %xmm4         # 16-byte Reload
	vpslld	$21, %xmm4, %xmm5
	vpsrld	$11, %xmm4, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm4, %xmm5, %xmm5
	vpslld	$25, %xmm5, %xmm6
	vpsrld	$7, %xmm5, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpsrld	$3, %xmm4, %xmm6
	vmovdqa	%xmm4, %xmm8
	vpxor	%xmm6, %xmm5, %xmm5
	vmovdqa	%xmm14, -96(%rsp)       # 16-byte Spill
	vpslld	$30, %xmm14, %xmm6
	vpsrld	$2, %xmm14, %xmm4
	vpor	%xmm6, %xmm4, %xmm4
	vpxor	%xmm14, %xmm4, %xmm4
	vpslld	$15, %xmm4, %xmm6
	vpsrld	$17, %xmm4, %xmm4
	vpor	%xmm6, %xmm4, %xmm4
	vpsrld	$10, %xmm14, %xmm6
	vpxor	%xmm6, %xmm4, %xmm4
	vpaddd	%xmm5, %xmm13, %xmm5
	vpaddd	-64(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vpaddd	%xmm4, %xmm5, %xmm13
	vpslld	$27, %xmm12, %xmm4
	vpsrld	$5, %xmm12, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm12, %xmm4, %xmm4
	vpslld	$26, %xmm4, %xmm5
	vpsrld	$6, %xmm4, %xmm4
	vpor	%xmm5, %xmm4, %xmm4
	vpslld	$7, %xmm12, %xmm5
	vpsrld	$25, %xmm12, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm7, %xmm10, %xmm5
	vpand	%xmm12, %xmm5, %xmm5
	vpxor	%xmm7, %xmm5, %xmm5
	vpaddd	%xmm9, %xmm5, %xmm5
	vpaddd	%xmm13, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm9
	vpslld	$30, %xmm1, %xmm5
	vpsrld	$2, %xmm1, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpslld	$19, %xmm1, %xmm6
	vpsrld	$13, %xmm1, %xmm4
	vpor	%xmm6, %xmm4, %xmm4
	vpxor	%xmm4, %xmm5, %xmm4
	vpslld	$10, %xmm1, %xmm5
	vpsrld	$22, %xmm1, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm3, %xmm0, %xmm5
	vpand	%xmm5, %xmm1, %xmm5
	vpand	%xmm3, %xmm0, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm4
	vpaddd	.LCPI1_58(%rip), %xmm9, %xmm6
	vpaddd	%xmm2, %xmm6, %xmm9
	vpaddd	%xmm6, %xmm4, %xmm2
	vmovdqa	32(%rsp), %xmm14        # 16-byte Reload
	vpslld	$21, %xmm14, %xmm4
	vpsrld	$11, %xmm14, %xmm6
	vpor	%xmm4, %xmm6, %xmm4
	vpxor	%xmm14, %xmm4, %xmm4
	vpslld	$25, %xmm4, %xmm6
	vpsrld	$7, %xmm4, %xmm4
	vpor	%xmm6, %xmm4, %xmm4
	vpsrld	$3, %xmm14, %xmm6
	vpxor	%xmm6, %xmm4, %xmm4
	vmovdqa	%xmm11, -128(%rsp)      # 16-byte Spill
	vpslld	$30, %xmm11, %xmm6
	vpsrld	$2, %xmm11, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpxor	%xmm11, %xmm5, %xmm5
	vpslld	$15, %xmm5, %xmm6
	vpsrld	$17, %xmm5, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpsrld	$10, %xmm11, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm4, %xmm8, %xmm4
	vpaddd	%xmm4, %xmm15, %xmm4
	vpaddd	%xmm5, %xmm4, %xmm15
	vpslld	$27, %xmm9, %xmm5
	vpsrld	$5, %xmm9, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm9, %xmm5, %xmm5
	vpslld	$26, %xmm5, %xmm6
	vpsrld	$6, %xmm5, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpslld	$7, %xmm9, %xmm6
	vpsrld	$25, %xmm9, %xmm4
	vpor	%xmm6, %xmm4, %xmm4
	vpxor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm10, %xmm12, %xmm5
	vpand	%xmm9, %xmm5, %xmm5
	vpxor	%xmm10, %xmm5, %xmm5
	vpaddd	%xmm7, %xmm5, %xmm5
	vpaddd	%xmm15, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm4
	vpslld	$30, %xmm2, %xmm5
	vpsrld	$2, %xmm2, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpslld	$19, %xmm2, %xmm6
	vpsrld	$13, %xmm2, %xmm7
	vpor	%xmm6, %xmm7, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpslld	$10, %xmm2, %xmm6
	vpsrld	$22, %xmm2, %xmm7
	vpor	%xmm6, %xmm7, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpxor	%xmm0, %xmm1, %xmm6
	vpand	%xmm6, %xmm2, %xmm6
	vpand	%xmm0, %xmm1, %xmm7
	vpxor	%xmm7, %xmm6, %xmm6
	vpaddd	%xmm6, %xmm5, %xmm5
	vpaddd	.LCPI1_59(%rip), %xmm4, %xmm4
	vpaddd	%xmm3, %xmm4, %xmm11
	vpaddd	%xmm4, %xmm5, %xmm3
	vmovdqa	(%rsp), %xmm6           # 16-byte Reload
	vpslld	$21, %xmm6, %xmm4
	vpsrld	$11, %xmm6, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm6, %xmm4, %xmm4
	vpslld	$25, %xmm4, %xmm5
	vpsrld	$7, %xmm4, %xmm4
	vpor	%xmm5, %xmm4, %xmm4
	vpsrld	$3, %xmm6, %xmm5
	vmovdqa	%xmm6, %xmm7
	vpxor	%xmm5, %xmm4, %xmm4
	vpslld	$30, %xmm13, %xmm5
	vpsrld	$2, %xmm13, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm13, %xmm5, %xmm5
	vpslld	$15, %xmm5, %xmm6
	vpsrld	$17, %xmm5, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpsrld	$10, %xmm13, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm4, %xmm14, %xmm4
	vpaddd	-80(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vpaddd	%xmm5, %xmm4, %xmm13
	vpslld	$27, %xmm11, %xmm4
	vpsrld	$5, %xmm11, %xmm6
	vpor	%xmm4, %xmm6, %xmm4
	vpxor	%xmm11, %xmm4, %xmm4
	vpslld	$26, %xmm4, %xmm6
	vpsrld	$6, %xmm4, %xmm4
	vpor	%xmm6, %xmm4, %xmm4
	vpslld	$7, %xmm11, %xmm6
	vpsrld	$25, %xmm11, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm12, %xmm9, %xmm5
	vpand	%xmm11, %xmm5, %xmm5
	vpxor	%xmm12, %xmm5, %xmm5
	vpaddd	%xmm10, %xmm5, %xmm5
	vpaddd	%xmm13, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm10
	vpslld	$30, %xmm3, %xmm5
	vpsrld	$2, %xmm3, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpslld	$19, %xmm3, %xmm6
	vpsrld	$13, %xmm3, %xmm4
	vpor	%xmm6, %xmm4, %xmm4
	vpxor	%xmm4, %xmm5, %xmm4
	vpslld	$10, %xmm3, %xmm5
	vpsrld	$22, %xmm3, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm1, %xmm2, %xmm5
	vpand	%xmm5, %xmm3, %xmm5
	vpand	%xmm1, %xmm2, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm4
	vpaddd	.LCPI1_60(%rip), %xmm10, %xmm5
	vpaddd	%xmm0, %xmm5, %xmm10
	vpaddd	%xmm5, %xmm4, %xmm8
	vmovdqa	-16(%rsp), %xmm0        # 16-byte Reload
	vpslld	$21, %xmm0, %xmm4
	vpsrld	$11, %xmm0, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm0, %xmm4, %xmm4
	vpslld	$25, %xmm4, %xmm5
	vpsrld	$7, %xmm4, %xmm4
	vpor	%xmm5, %xmm4, %xmm4
	vpsrld	$3, %xmm0, %xmm5
	vmovdqa	%xmm0, %xmm14
	vpxor	%xmm5, %xmm4, %xmm4
	vpslld	$30, %xmm15, %xmm5
	vpsrld	$2, %xmm15, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm15, %xmm5, %xmm5
	vpslld	$15, %xmm5, %xmm6
	vpsrld	$17, %xmm5, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpsrld	$10, %xmm15, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm4, %xmm7, %xmm4
	vpaddd	112(%rsp), %xmm4, %xmm4 # 16-byte Folded Reload
	vpaddd	%xmm5, %xmm4, %xmm15
	vpslld	$27, %xmm10, %xmm4
	vpsrld	$5, %xmm10, %xmm5
	vpor	%xmm4, %xmm5, %xmm4
	vpxor	%xmm10, %xmm4, %xmm4
	vpslld	$26, %xmm4, %xmm5
	vpsrld	$6, %xmm4, %xmm4
	vpor	%xmm5, %xmm4, %xmm4
	vpslld	$7, %xmm10, %xmm5
	vpsrld	$25, %xmm10, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm9, %xmm11, %xmm5
	vpand	%xmm10, %xmm5, %xmm5
	vpxor	%xmm9, %xmm5, %xmm5
	vpaddd	%xmm12, %xmm5, %xmm5
	vpaddd	%xmm15, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm0
	vpslld	$30, %xmm8, %xmm5
	vpsrld	$2, %xmm8, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpslld	$19, %xmm8, %xmm6
	vpsrld	$13, %xmm8, %xmm4
	vpor	%xmm6, %xmm4, %xmm4
	vpxor	%xmm4, %xmm5, %xmm4
	vpslld	$10, %xmm8, %xmm5
	vpsrld	$22, %xmm8, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm5, %xmm4, %xmm4
	vpxor	%xmm2, %xmm3, %xmm5
	vpand	%xmm5, %xmm8, %xmm5
	vpand	%xmm2, %xmm3, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpaddd	%xmm5, %xmm4, %xmm4
	vpaddd	.LCPI1_61(%rip), %xmm0, %xmm5
	vpaddd	%xmm1, %xmm5, %xmm12
	vpaddd	%xmm5, %xmm4, %xmm4
	vmovdqa	128(%rsp), %xmm0        # 16-byte Reload
	vpslld	$21, %xmm0, %xmm5
	vpsrld	$11, %xmm0, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm0, %xmm5, %xmm5
	vpslld	$25, %xmm5, %xmm6
	vpsrld	$7, %xmm5, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpsrld	$3, %xmm0, %xmm6
	vmovdqa	%xmm0, %xmm1
	vpxor	%xmm6, %xmm5, %xmm5
	vpslld	$30, %xmm13, %xmm6
	vpsrld	$2, %xmm13, %xmm7
	vpor	%xmm6, %xmm7, %xmm6
	vpxor	%xmm13, %xmm6, %xmm6
	vpslld	$15, %xmm6, %xmm7
	vpsrld	$17, %xmm6, %xmm6
	vpor	%xmm7, %xmm6, %xmm6
	vpsrld	$10, %xmm13, %xmm7
	vpxor	%xmm7, %xmm6, %xmm6
	vpaddd	%xmm5, %xmm14, %xmm5
	vpaddd	-112(%rsp), %xmm5, %xmm5 # 16-byte Folded Reload
	vpaddd	%xmm6, %xmm5, %xmm13
	vpslld	$27, %xmm12, %xmm5
	vpsrld	$5, %xmm12, %xmm6
	vpor	%xmm5, %xmm6, %xmm5
	vpxor	%xmm12, %xmm5, %xmm5
	vpslld	$26, %xmm5, %xmm6
	vpsrld	$6, %xmm5, %xmm5
	vpor	%xmm6, %xmm5, %xmm5
	vpslld	$7, %xmm12, %xmm6
	vpsrld	$25, %xmm12, %xmm7
	vpor	%xmm6, %xmm7, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpxor	%xmm11, %xmm10, %xmm6
	vpand	%xmm12, %xmm6, %xmm6
	vpxor	%xmm11, %xmm6, %xmm6
	vpaddd	%xmm9, %xmm6, %xmm6
	vpaddd	%xmm13, %xmm6, %xmm6
	vpaddd	%xmm6, %xmm5, %xmm9
	vpslld	$30, %xmm4, %xmm6
	vpsrld	$2, %xmm4, %xmm7
	vpor	%xmm6, %xmm7, %xmm6
	vpslld	$19, %xmm4, %xmm7
	vpsrld	$13, %xmm4, %xmm5
	vpor	%xmm7, %xmm5, %xmm5
	vpxor	%xmm5, %xmm6, %xmm5
	vpslld	$10, %xmm4, %xmm6
	vpsrld	$22, %xmm4, %xmm7
	vpor	%xmm6, %xmm7, %xmm6
	vpxor	%xmm6, %xmm5, %xmm5
	vpxor	%xmm3, %xmm8, %xmm6
	vpand	%xmm6, %xmm4, %xmm6
	vpand	%xmm3, %xmm8, %xmm7
	vpxor	%xmm7, %xmm6, %xmm6
	vpaddd	%xmm6, %xmm5, %xmm5
	vpaddd	.LCPI1_62(%rip), %xmm9, %xmm6
	vpaddd	%xmm2, %xmm6, %xmm2
	vpaddd	%xmm6, %xmm5, %xmm5
	vmovdqa	-32(%rsp), %xmm0        # 16-byte Reload
	vpslld	$21, %xmm0, %xmm6
	vpsrld	$11, %xmm0, %xmm7
	vpor	%xmm6, %xmm7, %xmm6
	vpxor	%xmm0, %xmm6, %xmm6
	vpslld	$25, %xmm6, %xmm7
	vpsrld	$7, %xmm6, %xmm6
	vpor	%xmm7, %xmm6, %xmm6
	vpsrld	$3, %xmm0, %xmm7
	vmovdqa	%xmm0, %xmm14
	vpxor	%xmm7, %xmm6, %xmm6
	vpslld	$30, %xmm15, %xmm7
	vpsrld	$2, %xmm15, %xmm0
	vpor	%xmm7, %xmm0, %xmm0
	vpxor	%xmm15, %xmm0, %xmm0
	vpslld	$15, %xmm0, %xmm7
	vpsrld	$17, %xmm0, %xmm0
	vpor	%xmm7, %xmm0, %xmm0
	vpsrld	$10, %xmm15, %xmm7
	vpxor	%xmm7, %xmm0, %xmm0
	vpaddd	%xmm6, %xmm1, %xmm6
	vpaddd	-96(%rsp), %xmm6, %xmm6 # 16-byte Folded Reload
	vmovdqa	.LCPI1_63(%rip), %xmm1  # xmm1 = [3204031479,3204031479,3204031479,3204031479]
	vpaddd	%xmm0, %xmm6, %xmm9
	vpslld	$27, %xmm2, %xmm6
	vpsrld	$5, %xmm2, %xmm7
	vpor	%xmm6, %xmm7, %xmm6
	vpxor	%xmm2, %xmm6, %xmm6
	vpslld	$26, %xmm6, %xmm7
	vpsrld	$6, %xmm6, %xmm6
	vpor	%xmm7, %xmm6, %xmm6
	vpslld	$7, %xmm2, %xmm7
	vpsrld	$25, %xmm2, %xmm0
	vpor	%xmm7, %xmm0, %xmm0
	vpxor	%xmm0, %xmm6, %xmm0
	vpxor	%xmm10, %xmm12, %xmm6
	vpand	%xmm2, %xmm6, %xmm6
	vpxor	%xmm10, %xmm6, %xmm6
	vpaddd	%xmm11, %xmm6, %xmm6
	vmovdqa	.LCPI1_0(%rip), %xmm11  # xmm11 = [3,2,1,0,7,6,5,4,11,10,9,8,15,14,13,12]
	vpaddd	%xmm9, %xmm6, %xmm6
	vpaddd	%xmm6, %xmm0, %xmm9
	vpslld	$30, %xmm5, %xmm6
	vpsrld	$2, %xmm5, %xmm7
	vpor	%xmm6, %xmm7, %xmm6
	vpslld	$19, %xmm5, %xmm7
	vpsrld	$13, %xmm5, %xmm0
	vpor	%xmm7, %xmm0, %xmm0
	vpxor	%xmm0, %xmm6, %xmm0
	vpslld	$10, %xmm5, %xmm6
	vpsrld	$22, %xmm5, %xmm7
	vpor	%xmm6, %xmm7, %xmm6
	vpxor	%xmm6, %xmm0, %xmm0
	vpxor	%xmm8, %xmm4, %xmm6
	vpand	%xmm6, %xmm5, %xmm6
	vpand	%xmm8, %xmm4, %xmm7
	vpxor	%xmm7, %xmm6, %xmm6
	vpaddd	%xmm6, %xmm0, %xmm0
	vpaddd	%xmm1, %xmm9, %xmm6
	vpaddd	%xmm3, %xmm6, %xmm3
	vpaddd	%xmm6, %xmm0, %xmm7
	vmovdqa	-48(%rsp), %xmm1        # 16-byte Reload
	vpslld	$21, %xmm1, %xmm0
	vpsrld	$11, %xmm1, %xmm6
	vpor	%xmm0, %xmm6, %xmm0
	vpxor	%xmm1, %xmm0, %xmm0
	vpslld	$25, %xmm0, %xmm6
	vpsrld	$7, %xmm0, %xmm0
	vpor	%xmm6, %xmm0, %xmm0
	vpsrld	$3, %xmm1, %xmm6
	vpxor	%xmm6, %xmm0, %xmm0
	vpslld	$30, %xmm13, %xmm6
	vpsrld	$2, %xmm13, %xmm1
	vpor	%xmm6, %xmm1, %xmm1
	vpxor	%xmm13, %xmm1, %xmm1
	vpslld	$15, %xmm1, %xmm6
	vpsrld	$17, %xmm1, %xmm1
	vpor	%xmm6, %xmm1, %xmm1
	vpsrld	$10, %xmm13, %xmm6
	vpxor	%xmm6, %xmm1, %xmm1
	vpaddd	%xmm0, %xmm14, %xmm0
	vpaddd	-128(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpaddd	%xmm1, %xmm0, %xmm9
	vpslld	$27, %xmm3, %xmm1
	vpsrld	$5, %xmm3, %xmm6
	vpor	%xmm1, %xmm6, %xmm1
	vpxor	%xmm3, %xmm1, %xmm1
	vpslld	$26, %xmm1, %xmm6
	vpsrld	$6, %xmm1, %xmm1
	vpor	%xmm6, %xmm1, %xmm1
	vpslld	$7, %xmm3, %xmm6
	vpsrld	$25, %xmm3, %xmm0
	vpor	%xmm6, %xmm0, %xmm0
	vpxor	%xmm0, %xmm1, %xmm0
	vpxor	%xmm12, %xmm2, %xmm1
	vpand	%xmm3, %xmm1, %xmm1
	vpxor	%xmm12, %xmm1, %xmm1
	vpaddd	%xmm10, %xmm1, %xmm1
	vpaddd	%xmm9, %xmm1, %xmm1
	vpaddd	%xmm1, %xmm0, %xmm9
	vpslld	$30, %xmm7, %xmm1
	vpsrld	$2, %xmm7, %xmm6
	vpor	%xmm1, %xmm6, %xmm1
	vpslld	$19, %xmm7, %xmm6
	vpsrld	$13, %xmm7, %xmm0
	vpor	%xmm6, %xmm0, %xmm0
	vpxor	%xmm0, %xmm1, %xmm0
	vpslld	$10, %xmm7, %xmm1
	vpsrld	$22, %xmm7, %xmm6
	vpor	%xmm1, %xmm6, %xmm1
	vpxor	%xmm1, %xmm0, %xmm0
	vpxor	%xmm4, %xmm5, %xmm1
	vpand	%xmm1, %xmm7, %xmm1
	vpand	%xmm4, %xmm5, %xmm6
	vpxor	%xmm6, %xmm1, %xmm1
	vpaddd	%xmm1, %xmm0, %xmm0
	vpaddd	144(%rsp), %xmm0, %xmm0 # 16-byte Folded Reload
	vpaddd	.LCPI1_64(%rip), %xmm9, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovdqa	%xmm0, -112(%rcx)
	vpaddd	176(%rsp), %xmm7, %xmm0 # 16-byte Folded Reload
	vmovdqa	%xmm0, -96(%rcx)
	vpaddd	208(%rsp), %xmm5, %xmm0 # 16-byte Folded Reload
	vmovdqa	.LCPI1_1(%rip), %xmm5   # xmm5 = [1116352408,1116352408,1116352408,1116352408]
	vmovdqa	%xmm5, %xmm9
	vmovdqa	%xmm0, -80(%rcx)
	vpaddd	240(%rsp), %xmm4, %xmm0 # 16-byte Folded Reload
	vmovdqa	%xmm0, -64(%rcx)
	vpaddd	160(%rsp), %xmm8, %xmm0 # 16-byte Folded Reload
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovdqa	%xmm0, -48(%rcx)
	vpaddd	192(%rsp), %xmm3, %xmm0 # 16-byte Folded Reload
	vmovdqa	%xmm0, -32(%rcx)
	vpaddd	224(%rsp), %xmm2, %xmm0 # 16-byte Folded Reload
	vmovdqa	%xmm0, -16(%rcx)
	vpaddd	256(%rsp), %xmm12, %xmm0 # 16-byte Folded Reload
	vmovdqa	%xmm0, (%rcx)
	addq	$8, %rax
	subq	$-128, %rcx
	cmpq	$32, %rax
	jb	.LBB1_3
# %bb.4:                                #   in Loop: Header=BB1_2 Depth=1
	addq	$1024, %rsi             # imm = 0x400
	addq	$1, %r8
	cmpq	%rdx, %r8
	jne	.LBB1_2
.LBB1_5:
	addq	$280, %rsp              # imm = 0x118
	retq
.Lfunc_end1:
	.size	sha256x16_avx_update, .Lfunc_end1-sha256x16_avx_update
	.cfi_endproc
                                        # -- End function
	.hidden	sha256x16_avx_final     # -- Begin function sha256x16_avx_final
	.globl	sha256x16_avx_final
	.p2align	4, 0x90
	.type	sha256x16_avx_final,@function
sha256x16_avx_final:                    # @sha256x16_avx_final
	.cfi_startproc
# %bb.0:
	xorl	%eax, %eax
	xorl	%ecx, %ecx
	.p2align	4, 0x90
.LBB2_1:                                # =>This Inner Loop Header: Depth=1
	movl	(%rdi,%rax), %edx
	bswapl	%edx
	movl	%edx, (%rsi,%rax)
	movl	16(%rdi,%rax), %edx
	bswapl	%edx
	movl	%edx, 4(%rsi,%rax)
	movl	32(%rdi,%rax), %edx
	bswapl	%edx
	movl	%edx, 8(%rsi,%rax)
	movl	48(%rdi,%rax), %edx
	bswapl	%edx
	movl	%edx, 12(%rsi,%rax)
	movl	64(%rdi,%rax), %edx
	bswapl	%edx
	movl	%edx, 16(%rsi,%rax)
	movl	80(%rdi,%rax), %edx
	bswapl	%edx
	movl	%edx, 20(%rsi,%rax)
	movl	96(%rdi,%rax), %edx
	bswapl	%edx
	movl	%edx, 24(%rsi,%rax)
	movl	112(%rdi,%rax), %edx
	bswapl	%edx
	movl	%edx, 28(%rsi,%rax)
	movl	4(%rdi,%rax), %edx
	bswapl	%edx
	movl	%edx, 32(%rsi,%rax)
	movl	20(%rdi,%rax), %edx
	bswapl	%edx
	movl	%edx, 36(%rsi,%rax)
	movl	36(%rdi,%rax), %edx
	bswapl	%edx
	movl	%edx, 40(%rsi,%rax)
	movl	52(%rdi,%rax), %edx
	bswapl	%edx
	movl	%edx, 44(%rsi,%rax)
	movl	68(%rdi,%rax), %edx
	bswapl	%edx
	movl	%edx, 48(%rsi,%rax)
	movl	84(%rdi,%rax), %edx
	bswapl	%edx
	movl	%edx, 52(%rsi,%rax)
	movl	100(%rdi,%rax), %edx
	bswapl	%edx
	movl	%edx, 56(%rsi,%rax)
	movl	116(%rdi,%rax), %edx
	bswapl	%edx
	movl	%edx, 60(%rsi,%rax)
	movl	8(%rdi,%rax), %edx
	bswapl	%edx
	movl	%edx, 64(%rsi,%rax)
	movl	24(%rdi,%rax), %edx
	bswapl	%edx
	movl	%edx, 68(%rsi,%rax)
	movl	40(%rdi,%rax), %edx
	bswapl	%edx
	movl	%edx, 72(%rsi,%rax)
	movl	56(%rdi,%rax), %edx
	bswapl	%edx
	movl	%edx, 76(%rsi,%rax)
	movl	72(%rdi,%rax), %edx
	bswapl	%edx
	movl	%edx, 80(%rsi,%rax)
	movl	88(%rdi,%rax), %edx
	bswapl	%edx
	movl	%edx, 84(%rsi,%rax)
	movl	104(%rdi,%rax), %edx
	bswapl	%edx
	movl	%edx, 88(%rsi,%rax)
	movl	120(%rdi,%rax), %edx
	bswapl	%edx
	movl	%edx, 92(%rsi,%rax)
	movl	12(%rdi,%rax), %edx
	bswapl	%edx
	movl	%edx, 96(%rsi,%rax)
	movl	28(%rdi,%rax), %edx
	bswapl	%edx
	movl	%edx, 100(%rsi,%rax)
	movl	44(%rdi,%rax), %edx
	bswapl	%edx
	movl	%edx, 104(%rsi,%rax)
	movl	60(%rdi,%rax), %edx
	bswapl	%edx
	movl	%edx, 108(%rsi,%rax)
	movl	76(%rdi,%rax), %edx
	bswapl	%edx
	movl	%edx, 112(%rsi,%rax)
	movl	92(%rdi,%rax), %edx
	bswapl	%edx
	movl	%edx, 116(%rsi,%rax)
	movl	108(%rdi,%rax), %edx
	bswapl	%edx
	movl	%edx, 120(%rsi,%rax)
	movl	124(%rdi,%rax), %edx
	bswapl	%edx
	movl	%edx, 124(%rsi,%rax)
	addq	$4, %rcx
	subq	$-128, %rax
	cmpq	$16, %rcx
	jb	.LBB2_1
# %bb.2:
	retq
.Lfunc_end2:
	.size	sha256x16_avx_final, .Lfunc_end2-sha256x16_avx_final
	.cfi_endproc
                                        # -- End function
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2               # -- Begin function sha256x16_avx2_init
.LCPI3_0:
	.long	1779033703              # 0x6a09e667
.LCPI3_1:
	.long	3144134277              # 0xbb67ae85
.LCPI3_2:
	.long	1013904242              # 0x3c6ef372
.LCPI3_3:
	.long	2773480762              # 0xa54ff53a
.LCPI3_4:
	.long	1359893119              # 0x510e527f
.LCPI3_5:
	.long	2600822924              # 0x9b05688c
.LCPI3_6:
	.long	528734635               # 0x1f83d9ab
.LCPI3_7:
	.long	1541459225              # 0x5be0cd19
	.text
	.hidden	sha256x16_avx2_init
	.globl	sha256x16_avx2_init
	.p2align	4, 0x90
	.type	sha256x16_avx2_init,@function
sha256x16_avx2_init:                    # @sha256x16_avx2_init
	.cfi_startproc
# %bb.0:
	vbroadcastss	.LCPI3_0(%rip), %ymm0 # ymm0 = [1779033703,1779033703,1779033703,1779033703,1779033703,1779033703,1779033703,1779033703]
	vmovaps	%ymm0, (%rdi)
	vmovaps	%ymm0, 256(%rdi)
	vbroadcastss	.LCPI3_1(%rip), %ymm0 # ymm0 = [3144134277,3144134277,3144134277,3144134277,3144134277,3144134277,3144134277,3144134277]
	vmovaps	%ymm0, 32(%rdi)
	vmovaps	%ymm0, 288(%rdi)
	vbroadcastss	.LCPI3_2(%rip), %ymm0 # ymm0 = [1013904242,1013904242,1013904242,1013904242,1013904242,1013904242,1013904242,1013904242]
	vmovaps	%ymm0, 64(%rdi)
	vmovaps	%ymm0, 320(%rdi)
	vbroadcastss	.LCPI3_3(%rip), %ymm0 # ymm0 = [2773480762,2773480762,2773480762,2773480762,2773480762,2773480762,2773480762,2773480762]
	vmovaps	%ymm0, 96(%rdi)
	vmovaps	%ymm0, 352(%rdi)
	vbroadcastss	.LCPI3_4(%rip), %ymm0 # ymm0 = [1359893119,1359893119,1359893119,1359893119,1359893119,1359893119,1359893119,1359893119]
	vmovaps	%ymm0, 128(%rdi)
	vmovaps	%ymm0, 384(%rdi)
	vbroadcastss	.LCPI3_5(%rip), %ymm0 # ymm0 = [2600822924,2600822924,2600822924,2600822924,2600822924,2600822924,2600822924,2600822924]
	vmovaps	%ymm0, 160(%rdi)
	vmovaps	%ymm0, 416(%rdi)
	vbroadcastss	.LCPI3_6(%rip), %ymm0 # ymm0 = [528734635,528734635,528734635,528734635,528734635,528734635,528734635,528734635]
	vmovaps	%ymm0, 192(%rdi)
	vmovaps	%ymm0, 448(%rdi)
	vbroadcastss	.LCPI3_7(%rip), %ymm0 # ymm0 = [1541459225,1541459225,1541459225,1541459225,1541459225,1541459225,1541459225,1541459225]
	vmovaps	%ymm0, 224(%rdi)
	vmovaps	%ymm0, 480(%rdi)
	vzeroupper
	retq
.Lfunc_end3:
	.size	sha256x16_avx2_init, .Lfunc_end3-sha256x16_avx2_init
	.cfi_endproc
                                        # -- End function
	.section	.rodata.cst32,"aM",@progbits,32
	.p2align	5               # -- Begin function sha256x16_avx2_update
.LCPI4_0:
	.byte	3                       # 0x3
	.byte	2                       # 0x2
	.byte	1                       # 0x1
	.byte	0                       # 0x0
	.byte	7                       # 0x7
	.byte	6                       # 0x6
	.byte	5                       # 0x5
	.byte	4                       # 0x4
	.byte	11                      # 0xb
	.byte	10                      # 0xa
	.byte	9                       # 0x9
	.byte	8                       # 0x8
	.byte	15                      # 0xf
	.byte	14                      # 0xe
	.byte	13                      # 0xd
	.byte	12                      # 0xc
	.byte	3                       # 0x3
	.byte	2                       # 0x2
	.byte	1                       # 0x1
	.byte	0                       # 0x0
	.byte	7                       # 0x7
	.byte	6                       # 0x6
	.byte	5                       # 0x5
	.byte	4                       # 0x4
	.byte	11                      # 0xb
	.byte	10                      # 0xa
	.byte	9                       # 0x9
	.byte	8                       # 0x8
	.byte	15                      # 0xf
	.byte	14                      # 0xe
	.byte	13                      # 0xd
	.byte	12                      # 0xc
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2
.LCPI4_1:
	.long	1116352408              # 0x428a2f98
.LCPI4_2:
	.long	1899447441              # 0x71374491
.LCPI4_3:
	.long	3049323471              # 0xb5c0fbcf
.LCPI4_4:
	.long	3921009573              # 0xe9b5dba5
.LCPI4_5:
	.long	961987163               # 0x3956c25b
.LCPI4_6:
	.long	1508970993              # 0x59f111f1
.LCPI4_7:
	.long	2453635748              # 0x923f82a4
.LCPI4_8:
	.long	2870763221              # 0xab1c5ed5
.LCPI4_9:
	.long	3624381080              # 0xd807aa98
.LCPI4_10:
	.long	310598401               # 0x12835b01
.LCPI4_11:
	.long	607225278               # 0x243185be
.LCPI4_12:
	.long	1426881987              # 0x550c7dc3
.LCPI4_13:
	.long	1925078388              # 0x72be5d74
.LCPI4_14:
	.long	2162078206              # 0x80deb1fe
.LCPI4_15:
	.long	2614888103              # 0x9bdc06a7
.LCPI4_16:
	.long	3248222580              # 0xc19bf174
.LCPI4_17:
	.long	3835390401              # 0xe49b69c1
.LCPI4_18:
	.long	4022224774              # 0xefbe4786
.LCPI4_19:
	.long	264347078               # 0xfc19dc6
.LCPI4_20:
	.long	604807628               # 0x240ca1cc
.LCPI4_21:
	.long	770255983               # 0x2de92c6f
.LCPI4_22:
	.long	1249150122              # 0x4a7484aa
.LCPI4_23:
	.long	1555081692              # 0x5cb0a9dc
.LCPI4_24:
	.long	1996064986              # 0x76f988da
.LCPI4_25:
	.long	2554220882              # 0x983e5152
.LCPI4_26:
	.long	2821834349              # 0xa831c66d
.LCPI4_27:
	.long	2952996808              # 0xb00327c8
.LCPI4_28:
	.long	3210313671              # 0xbf597fc7
.LCPI4_29:
	.long	3336571891              # 0xc6e00bf3
.LCPI4_30:
	.long	3584528711              # 0xd5a79147
.LCPI4_31:
	.long	113926993               # 0x6ca6351
.LCPI4_32:
	.long	338241895               # 0x14292967
.LCPI4_33:
	.long	666307205               # 0x27b70a85
.LCPI4_34:
	.long	773529912               # 0x2e1b2138
.LCPI4_35:
	.long	1294757372              # 0x4d2c6dfc
.LCPI4_36:
	.long	1396182291              # 0x53380d13
.LCPI4_37:
	.long	1695183700              # 0x650a7354
.LCPI4_38:
	.long	1986661051              # 0x766a0abb
.LCPI4_39:
	.long	2177026350              # 0x81c2c92e
.LCPI4_40:
	.long	2456956037              # 0x92722c85
.LCPI4_41:
	.long	2730485921              # 0xa2bfe8a1
.LCPI4_42:
	.long	2820302411              # 0xa81a664b
.LCPI4_43:
	.long	3259730800              # 0xc24b8b70
.LCPI4_44:
	.long	3345764771              # 0xc76c51a3
.LCPI4_45:
	.long	3516065817              # 0xd192e819
.LCPI4_46:
	.long	3600352804              # 0xd6990624
.LCPI4_47:
	.long	4094571909              # 0xf40e3585
.LCPI4_48:
	.long	275423344               # 0x106aa070
.LCPI4_49:
	.long	430227734               # 0x19a4c116
.LCPI4_50:
	.long	506948616               # 0x1e376c08
.LCPI4_51:
	.long	659060556               # 0x2748774c
.LCPI4_52:
	.long	883997877               # 0x34b0bcb5
.LCPI4_53:
	.long	958139571               # 0x391c0cb3
.LCPI4_54:
	.long	1322822218              # 0x4ed8aa4a
.LCPI4_55:
	.long	1537002063              # 0x5b9cca4f
.LCPI4_56:
	.long	1747873779              # 0x682e6ff3
.LCPI4_57:
	.long	1955562222              # 0x748f82ee
.LCPI4_58:
	.long	2024104815              # 0x78a5636f
.LCPI4_59:
	.long	2227730452              # 0x84c87814
.LCPI4_60:
	.long	2361852424              # 0x8cc70208
.LCPI4_61:
	.long	2428436474              # 0x90befffa
.LCPI4_62:
	.long	2756734187              # 0xa4506ceb
.LCPI4_63:
	.long	3204031479              # 0xbef9a3f7
.LCPI4_64:
	.long	3329325298              # 0xc67178f2
	.text
	.hidden	sha256x16_avx2_update
	.globl	sha256x16_avx2_update
	.p2align	4, 0x90
	.type	sha256x16_avx2_update,@function
sha256x16_avx2_update:                  # @sha256x16_avx2_update
	.cfi_startproc
# %bb.0:
	subq	$2744, %rsp             # imm = 0xAB8
	.cfi_def_cfa_offset 2752
	testq	%rdx, %rdx
	je	.LBB4_5
# %bb.1:
	addq	$224, %rdi
	xorl	%r8d, %r8d
	vpbroadcastd	.LCPI4_1(%rip), %ymm5 # ymm5 = [1116352408,1116352408,1116352408,1116352408,1116352408,1116352408,1116352408,1116352408]
	vpbroadcastd	.LCPI4_2(%rip), %ymm9 # ymm9 = [1899447441,1899447441,1899447441,1899447441,1899447441,1899447441,1899447441,1899447441]
	vpbroadcastd	.LCPI4_3(%rip), %ymm7 # ymm7 = [3049323471,3049323471,3049323471,3049323471,3049323471,3049323471,3049323471,3049323471]
	vpbroadcastd	.LCPI4_4(%rip), %ymm10 # ymm10 = [3921009573,3921009573,3921009573,3921009573,3921009573,3921009573,3921009573,3921009573]
	vpbroadcastd	.LCPI4_5(%rip), %ymm12 # ymm12 = [961987163,961987163,961987163,961987163,961987163,961987163,961987163,961987163]
	vpbroadcastd	.LCPI4_6(%rip), %ymm13 # ymm13 = [1508970993,1508970993,1508970993,1508970993,1508970993,1508970993,1508970993,1508970993]
	vbroadcastss	.LCPI4_7(%rip), %ymm0 # ymm0 = [2453635748,2453635748,2453635748,2453635748,2453635748,2453635748,2453635748,2453635748]
	vmovups	%ymm0, 2240(%rsp)       # 32-byte Spill
	vbroadcastss	.LCPI4_8(%rip), %ymm0 # ymm0 = [2870763221,2870763221,2870763221,2870763221,2870763221,2870763221,2870763221,2870763221]
	vmovups	%ymm0, 2208(%rsp)       # 32-byte Spill
	vbroadcastss	.LCPI4_9(%rip), %ymm0 # ymm0 = [3624381080,3624381080,3624381080,3624381080,3624381080,3624381080,3624381080,3624381080]
	vmovups	%ymm0, 2176(%rsp)       # 32-byte Spill
	vbroadcastss	.LCPI4_10(%rip), %ymm0 # ymm0 = [310598401,310598401,310598401,310598401,310598401,310598401,310598401,310598401]
	vmovups	%ymm0, 2144(%rsp)       # 32-byte Spill
	vbroadcastss	.LCPI4_11(%rip), %ymm0 # ymm0 = [607225278,607225278,607225278,607225278,607225278,607225278,607225278,607225278]
	vmovups	%ymm0, 2112(%rsp)       # 32-byte Spill
	vbroadcastss	.LCPI4_12(%rip), %ymm0 # ymm0 = [1426881987,1426881987,1426881987,1426881987,1426881987,1426881987,1426881987,1426881987]
	vmovups	%ymm0, 2080(%rsp)       # 32-byte Spill
	vbroadcastss	.LCPI4_13(%rip), %ymm0 # ymm0 = [1925078388,1925078388,1925078388,1925078388,1925078388,1925078388,1925078388,1925078388]
	vmovups	%ymm0, 2048(%rsp)       # 32-byte Spill
	vbroadcastss	.LCPI4_14(%rip), %ymm0 # ymm0 = [2162078206,2162078206,2162078206,2162078206,2162078206,2162078206,2162078206,2162078206]
	vmovups	%ymm0, 2016(%rsp)       # 32-byte Spill
	vbroadcastss	.LCPI4_15(%rip), %ymm0 # ymm0 = [2614888103,2614888103,2614888103,2614888103,2614888103,2614888103,2614888103,2614888103]
	vmovups	%ymm0, 1984(%rsp)       # 32-byte Spill
	vbroadcastss	.LCPI4_16(%rip), %ymm0 # ymm0 = [3248222580,3248222580,3248222580,3248222580,3248222580,3248222580,3248222580,3248222580]
	vmovups	%ymm0, 1952(%rsp)       # 32-byte Spill
	vbroadcastss	.LCPI4_17(%rip), %ymm0 # ymm0 = [3835390401,3835390401,3835390401,3835390401,3835390401,3835390401,3835390401,3835390401]
	vmovups	%ymm0, 1920(%rsp)       # 32-byte Spill
	vbroadcastss	.LCPI4_18(%rip), %ymm0 # ymm0 = [4022224774,4022224774,4022224774,4022224774,4022224774,4022224774,4022224774,4022224774]
	vmovups	%ymm0, 1888(%rsp)       # 32-byte Spill
	vbroadcastss	.LCPI4_19(%rip), %ymm0 # ymm0 = [264347078,264347078,264347078,264347078,264347078,264347078,264347078,264347078]
	vmovups	%ymm0, 1856(%rsp)       # 32-byte Spill
	vbroadcastss	.LCPI4_20(%rip), %ymm0 # ymm0 = [604807628,604807628,604807628,604807628,604807628,604807628,604807628,604807628]
	vmovups	%ymm0, 1824(%rsp)       # 32-byte Spill
	vbroadcastss	.LCPI4_21(%rip), %ymm0 # ymm0 = [770255983,770255983,770255983,770255983,770255983,770255983,770255983,770255983]
	vmovups	%ymm0, 1792(%rsp)       # 32-byte Spill
	vbroadcastss	.LCPI4_22(%rip), %ymm0 # ymm0 = [1249150122,1249150122,1249150122,1249150122,1249150122,1249150122,1249150122,1249150122]
	vmovups	%ymm0, 1760(%rsp)       # 32-byte Spill
	vbroadcastss	.LCPI4_23(%rip), %ymm0 # ymm0 = [1555081692,1555081692,1555081692,1555081692,1555081692,1555081692,1555081692,1555081692]
	vmovups	%ymm0, 1728(%rsp)       # 32-byte Spill
	vbroadcastss	.LCPI4_24(%rip), %ymm0 # ymm0 = [1996064986,1996064986,1996064986,1996064986,1996064986,1996064986,1996064986,1996064986]
	vmovups	%ymm0, 1696(%rsp)       # 32-byte Spill
	vbroadcastss	.LCPI4_25(%rip), %ymm0 # ymm0 = [2554220882,2554220882,2554220882,2554220882,2554220882,2554220882,2554220882,2554220882]
	vmovups	%ymm0, 1664(%rsp)       # 32-byte Spill
	vbroadcastss	.LCPI4_26(%rip), %ymm0 # ymm0 = [2821834349,2821834349,2821834349,2821834349,2821834349,2821834349,2821834349,2821834349]
	vmovups	%ymm0, 1632(%rsp)       # 32-byte Spill
	vbroadcastss	.LCPI4_27(%rip), %ymm0 # ymm0 = [2952996808,2952996808,2952996808,2952996808,2952996808,2952996808,2952996808,2952996808]
	vmovups	%ymm0, 1600(%rsp)       # 32-byte Spill
	vbroadcastss	.LCPI4_28(%rip), %ymm0 # ymm0 = [3210313671,3210313671,3210313671,3210313671,3210313671,3210313671,3210313671,3210313671]
	vmovups	%ymm0, 1568(%rsp)       # 32-byte Spill
	vbroadcastss	.LCPI4_29(%rip), %ymm0 # ymm0 = [3336571891,3336571891,3336571891,3336571891,3336571891,3336571891,3336571891,3336571891]
	vmovups	%ymm0, 1536(%rsp)       # 32-byte Spill
	vbroadcastss	.LCPI4_30(%rip), %ymm0 # ymm0 = [3584528711,3584528711,3584528711,3584528711,3584528711,3584528711,3584528711,3584528711]
	vmovups	%ymm0, 1504(%rsp)       # 32-byte Spill
	vbroadcastss	.LCPI4_31(%rip), %ymm0 # ymm0 = [113926993,113926993,113926993,113926993,113926993,113926993,113926993,113926993]
	vmovups	%ymm0, 1472(%rsp)       # 32-byte Spill
	vbroadcastss	.LCPI4_32(%rip), %ymm0 # ymm0 = [338241895,338241895,338241895,338241895,338241895,338241895,338241895,338241895]
	vmovups	%ymm0, 1440(%rsp)       # 32-byte Spill
	vbroadcastss	.LCPI4_33(%rip), %ymm0 # ymm0 = [666307205,666307205,666307205,666307205,666307205,666307205,666307205,666307205]
	vmovups	%ymm0, 1408(%rsp)       # 32-byte Spill
	vbroadcastss	.LCPI4_34(%rip), %ymm0 # ymm0 = [773529912,773529912,773529912,773529912,773529912,773529912,773529912,773529912]
	vmovups	%ymm0, 1376(%rsp)       # 32-byte Spill
	vbroadcastss	.LCPI4_35(%rip), %ymm0 # ymm0 = [1294757372,1294757372,1294757372,1294757372,1294757372,1294757372,1294757372,1294757372]
	vmovups	%ymm0, 1344(%rsp)       # 32-byte Spill
	vbroadcastss	.LCPI4_36(%rip), %ymm0 # ymm0 = [1396182291,1396182291,1396182291,1396182291,1396182291,1396182291,1396182291,1396182291]
	vmovups	%ymm0, 1312(%rsp)       # 32-byte Spill
	vbroadcastss	.LCPI4_37(%rip), %ymm0 # ymm0 = [1695183700,1695183700,1695183700,1695183700,1695183700,1695183700,1695183700,1695183700]
	vmovups	%ymm0, 1280(%rsp)       # 32-byte Spill
	vbroadcastss	.LCPI4_38(%rip), %ymm0 # ymm0 = [1986661051,1986661051,1986661051,1986661051,1986661051,1986661051,1986661051,1986661051]
	vmovups	%ymm0, 1248(%rsp)       # 32-byte Spill
	vbroadcastss	.LCPI4_39(%rip), %ymm0 # ymm0 = [2177026350,2177026350,2177026350,2177026350,2177026350,2177026350,2177026350,2177026350]
	vmovups	%ymm0, 1216(%rsp)       # 32-byte Spill
	vbroadcastss	.LCPI4_40(%rip), %ymm0 # ymm0 = [2456956037,2456956037,2456956037,2456956037,2456956037,2456956037,2456956037,2456956037]
	vmovups	%ymm0, 1184(%rsp)       # 32-byte Spill
	vbroadcastss	.LCPI4_41(%rip), %ymm0 # ymm0 = [2730485921,2730485921,2730485921,2730485921,2730485921,2730485921,2730485921,2730485921]
	vmovups	%ymm0, 1152(%rsp)       # 32-byte Spill
	vbroadcastss	.LCPI4_42(%rip), %ymm0 # ymm0 = [2820302411,2820302411,2820302411,2820302411,2820302411,2820302411,2820302411,2820302411]
	vmovups	%ymm0, 1120(%rsp)       # 32-byte Spill
	vbroadcastss	.LCPI4_43(%rip), %ymm0 # ymm0 = [3259730800,3259730800,3259730800,3259730800,3259730800,3259730800,3259730800,3259730800]
	vmovups	%ymm0, 1088(%rsp)       # 32-byte Spill
	vbroadcastss	.LCPI4_44(%rip), %ymm0 # ymm0 = [3345764771,3345764771,3345764771,3345764771,3345764771,3345764771,3345764771,3345764771]
	vmovups	%ymm0, 1056(%rsp)       # 32-byte Spill
	vbroadcastss	.LCPI4_45(%rip), %ymm0 # ymm0 = [3516065817,3516065817,3516065817,3516065817,3516065817,3516065817,3516065817,3516065817]
	vmovups	%ymm0, 1024(%rsp)       # 32-byte Spill
	vbroadcastss	.LCPI4_46(%rip), %ymm0 # ymm0 = [3600352804,3600352804,3600352804,3600352804,3600352804,3600352804,3600352804,3600352804]
	vmovups	%ymm0, 992(%rsp)        # 32-byte Spill
	vbroadcastss	.LCPI4_47(%rip), %ymm0 # ymm0 = [4094571909,4094571909,4094571909,4094571909,4094571909,4094571909,4094571909,4094571909]
	vmovups	%ymm0, 960(%rsp)        # 32-byte Spill
	vbroadcastss	.LCPI4_48(%rip), %ymm0 # ymm0 = [275423344,275423344,275423344,275423344,275423344,275423344,275423344,275423344]
	vmovups	%ymm0, 928(%rsp)        # 32-byte Spill
	vbroadcastss	.LCPI4_49(%rip), %ymm0 # ymm0 = [430227734,430227734,430227734,430227734,430227734,430227734,430227734,430227734]
	vmovups	%ymm0, 896(%rsp)        # 32-byte Spill
	vbroadcastss	.LCPI4_50(%rip), %ymm0 # ymm0 = [506948616,506948616,506948616,506948616,506948616,506948616,506948616,506948616]
	vmovups	%ymm0, 864(%rsp)        # 32-byte Spill
	vbroadcastss	.LCPI4_51(%rip), %ymm0 # ymm0 = [659060556,659060556,659060556,659060556,659060556,659060556,659060556,659060556]
	vmovups	%ymm0, 832(%rsp)        # 32-byte Spill
	vbroadcastss	.LCPI4_52(%rip), %ymm0 # ymm0 = [883997877,883997877,883997877,883997877,883997877,883997877,883997877,883997877]
	vmovups	%ymm0, 800(%rsp)        # 32-byte Spill
	vbroadcastss	.LCPI4_53(%rip), %ymm0 # ymm0 = [958139571,958139571,958139571,958139571,958139571,958139571,958139571,958139571]
	vmovups	%ymm0, 768(%rsp)        # 32-byte Spill
	vbroadcastss	.LCPI4_54(%rip), %ymm0 # ymm0 = [1322822218,1322822218,1322822218,1322822218,1322822218,1322822218,1322822218,1322822218]
	vmovups	%ymm0, 736(%rsp)        # 32-byte Spill
	vbroadcastss	.LCPI4_55(%rip), %ymm0 # ymm0 = [1537002063,1537002063,1537002063,1537002063,1537002063,1537002063,1537002063,1537002063]
	vmovups	%ymm0, 704(%rsp)        # 32-byte Spill
	vbroadcastss	.LCPI4_56(%rip), %ymm0 # ymm0 = [1747873779,1747873779,1747873779,1747873779,1747873779,1747873779,1747873779,1747873779]
	vmovups	%ymm0, 672(%rsp)        # 32-byte Spill
	vbroadcastss	.LCPI4_57(%rip), %ymm0 # ymm0 = [1955562222,1955562222,1955562222,1955562222,1955562222,1955562222,1955562222,1955562222]
	vmovups	%ymm0, 640(%rsp)        # 32-byte Spill
	vbroadcastss	.LCPI4_58(%rip), %ymm0 # ymm0 = [2024104815,2024104815,2024104815,2024104815,2024104815,2024104815,2024104815,2024104815]
	vmovups	%ymm0, 608(%rsp)        # 32-byte Spill
	vbroadcastss	.LCPI4_59(%rip), %ymm0 # ymm0 = [2227730452,2227730452,2227730452,2227730452,2227730452,2227730452,2227730452,2227730452]
	vmovups	%ymm0, 576(%rsp)        # 32-byte Spill
	vbroadcastss	.LCPI4_60(%rip), %ymm0 # ymm0 = [2361852424,2361852424,2361852424,2361852424,2361852424,2361852424,2361852424,2361852424]
	vmovups	%ymm0, 544(%rsp)        # 32-byte Spill
	vbroadcastss	.LCPI4_61(%rip), %ymm0 # ymm0 = [2428436474,2428436474,2428436474,2428436474,2428436474,2428436474,2428436474,2428436474]
	vmovups	%ymm0, 512(%rsp)        # 32-byte Spill
	vpbroadcastd	.LCPI4_62(%rip), %ymm14 # ymm14 = [2756734187,2756734187,2756734187,2756734187,2756734187,2756734187,2756734187,2756734187]
	vpbroadcastd	.LCPI4_63(%rip), %ymm0 # ymm0 = [3204031479,3204031479,3204031479,3204031479,3204031479,3204031479,3204031479,3204031479]
	vmovdqu	%ymm0, 448(%rsp)        # 32-byte Spill
	vpbroadcastd	.LCPI4_64(%rip), %ymm11 # ymm11 = [3329325298,3329325298,3329325298,3329325298,3329325298,3329325298,3329325298,3329325298]
	vmovdqu	%ymm5, 2432(%rsp)       # 32-byte Spill
	vmovdqu	%ymm9, 2400(%rsp)       # 32-byte Spill
	vmovdqu	%ymm7, 2368(%rsp)       # 32-byte Spill
	vmovdqu	%ymm10, 2336(%rsp)      # 32-byte Spill
	vmovdqu	%ymm12, 2304(%rsp)      # 32-byte Spill
	vmovdqu	%ymm13, 2272(%rsp)      # 32-byte Spill
	vmovdqu	%ymm14, 480(%rsp)       # 32-byte Spill
	vmovdqu	%ymm11, 416(%rsp)       # 32-byte Spill
	.p2align	4, 0x90
.LBB4_2:                                # =>This Loop Header: Depth=1
                                        #     Child Loop BB4_3 Depth 2
	movq	%rdi, %rcx
	xorl	%eax, %eax
	.p2align	4, 0x90
.LBB4_3:                                #   Parent Loop BB4_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovdqa	-224(%rcx), %ymm10
	vmovdqa	-192(%rcx), %ymm12
	vmovdqa	-160(%rcx), %ymm13
	vmovdqa	-128(%rcx), %ymm15
	vmovdqu	%ymm15, 2656(%rsp)      # 32-byte Spill
	vmovdqa	-96(%rcx), %ymm14
	vmovdqa	-64(%rcx), %ymm11
	vmovdqa	-32(%rcx), %ymm7
	vmovdqa	(%rcx), %ymm8
	vmovdqu	%ymm8, 2688(%rsp)       # 32-byte Spill
	vmovdqu	(%rsi,%rax,4), %ymm1
	vmovdqu	64(%rsi,%rax,4), %ymm0
	vmovdqa	.LCPI4_0(%rip), %ymm3   # ymm3 = [3,2,1,0,7,6,5,4,11,10,9,8,15,14,13,12,3,2,1,0,7,6,5,4,11,10,9,8,15,14,13,12]
	vpshufb	%ymm3, %ymm1, %ymm6
	vmovdqu	%ymm6, -128(%rsp)       # 32-byte Spill
	vpxor	%ymm7, %ymm11, %ymm1
	vpand	%ymm14, %ymm1, %ymm1
	vpxor	%ymm7, %ymm1, %ymm1
	vmovdqu	%ymm7, 2624(%rsp)       # 32-byte Spill
	vpslld	$27, %ymm14, %ymm2
	vpsrld	$5, %ymm14, %ymm3
	vpor	%ymm2, %ymm3, %ymm2
	vpxor	%ymm14, %ymm2, %ymm2
	vpslld	$26, %ymm2, %ymm3
	vpsrld	$6, %ymm2, %ymm2
	vpor	%ymm3, %ymm2, %ymm2
	vpslld	$7, %ymm14, %ymm3
	vpsrld	$25, %ymm14, %ymm4
	vpor	%ymm3, %ymm4, %ymm3
	vpxor	%ymm3, %ymm2, %ymm2
	vpaddd	%ymm1, %ymm8, %ymm1
	vpaddd	%ymm2, %ymm1, %ymm1
	vpaddd	%ymm5, %ymm6, %ymm2
	vpaddd	%ymm2, %ymm1, %ymm1
	vpaddd	%ymm15, %ymm1, %ymm4
	vpslld	$30, %ymm10, %ymm2
	vpsrld	$2, %ymm10, %ymm3
	vpor	%ymm2, %ymm3, %ymm2
	vpslld	$19, %ymm10, %ymm3
	vpsrld	$13, %ymm10, %ymm5
	vpor	%ymm3, %ymm5, %ymm3
	vpxor	%ymm3, %ymm2, %ymm2
	vpslld	$10, %ymm10, %ymm3
	vpsrld	$22, %ymm10, %ymm5
	vpor	%ymm3, %ymm5, %ymm3
	vpxor	%ymm3, %ymm2, %ymm2
	vpxor	%ymm13, %ymm12, %ymm3
	vpand	%ymm3, %ymm10, %ymm3
	vpand	%ymm13, %ymm12, %ymm5
	vmovdqu	%ymm13, 2592(%rsp)      # 32-byte Spill
	vpxor	%ymm5, %ymm3, %ymm3
	vpaddd	%ymm3, %ymm2, %ymm2
	vpaddd	%ymm1, %ymm2, %ymm1
	vmovdqa	.LCPI4_0(%rip), %ymm15  # ymm15 = [3,2,1,0,7,6,5,4,11,10,9,8,15,14,13,12,3,2,1,0,7,6,5,4,11,10,9,8,15,14,13,12]
	vpshufb	%ymm15, %ymm0, %ymm6
	vmovdqu	%ymm6, -96(%rsp)        # 32-byte Spill
	vpxor	%ymm11, %ymm14, %ymm0
	vpand	%ymm4, %ymm0, %ymm0
	vpxor	%ymm11, %ymm0, %ymm0
	vmovdqu	%ymm11, 2560(%rsp)      # 32-byte Spill
	vpslld	$27, %ymm4, %ymm2
	vpsrld	$5, %ymm4, %ymm3
	vpor	%ymm2, %ymm3, %ymm2
	vpxor	%ymm4, %ymm2, %ymm2
	vpslld	$26, %ymm2, %ymm3
	vpsrld	$6, %ymm2, %ymm2
	vpor	%ymm3, %ymm2, %ymm2
	vpslld	$7, %ymm4, %ymm3
	vpsrld	$25, %ymm4, %ymm5
	vpor	%ymm3, %ymm5, %ymm3
	vpxor	%ymm3, %ymm2, %ymm2
	vpaddd	%ymm9, %ymm2, %ymm2
	vpaddd	%ymm7, %ymm0, %ymm0
	vpaddd	%ymm6, %ymm0, %ymm0
	vpaddd	%ymm2, %ymm0, %ymm0
	vpslld	$30, %ymm1, %ymm2
	vpsrld	$2, %ymm1, %ymm3
	vpor	%ymm2, %ymm3, %ymm2
	vpslld	$19, %ymm1, %ymm3
	vpsrld	$13, %ymm1, %ymm5
	vpor	%ymm3, %ymm5, %ymm3
	vpxor	%ymm3, %ymm2, %ymm2
	vpslld	$10, %ymm1, %ymm3
	vpsrld	$22, %ymm1, %ymm5
	vpor	%ymm3, %ymm5, %ymm3
	vpxor	%ymm3, %ymm2, %ymm2
	vpxor	%ymm12, %ymm10, %ymm3
	vpand	%ymm1, %ymm3, %ymm3
	vpand	%ymm12, %ymm10, %ymm5
	vmovdqu	%ymm12, 2528(%rsp)      # 32-byte Spill
	vpxor	%ymm5, %ymm3, %ymm3
	vpaddd	%ymm13, %ymm0, %ymm5
	vpaddd	%ymm3, %ymm2, %ymm2
	vpaddd	%ymm0, %ymm2, %ymm2
	vpslld	$27, %ymm5, %ymm0
	vpsrld	$5, %ymm5, %ymm3
	vpor	%ymm0, %ymm3, %ymm0
	vpxor	%ymm5, %ymm0, %ymm0
	vpslld	$26, %ymm0, %ymm3
	vpsrld	$6, %ymm0, %ymm0
	vpor	%ymm3, %ymm0, %ymm0
	vpslld	$7, %ymm5, %ymm3
	vpsrld	$25, %ymm5, %ymm6
	vpor	%ymm3, %ymm6, %ymm3
	vpxor	%ymm3, %ymm0, %ymm0
	vmovdqu	128(%rsi,%rax,4), %ymm3
	vpshufb	%ymm15, %ymm3, %ymm6
	vmovdqu	%ymm6, 352(%rsp)        # 32-byte Spill
	vpxor	%ymm4, %ymm14, %ymm3
	vpand	%ymm5, %ymm3, %ymm3
	vpxor	%ymm14, %ymm3, %ymm3
	vmovdqu	%ymm14, 2496(%rsp)      # 32-byte Spill
	vpaddd	2368(%rsp), %ymm0, %ymm0 # 32-byte Folded Reload
	vpaddd	%ymm11, %ymm3, %ymm3
	vpaddd	%ymm6, %ymm3, %ymm3
	vpaddd	%ymm0, %ymm3, %ymm0
	vpslld	$30, %ymm2, %ymm3
	vpsrld	$2, %ymm2, %ymm6
	vpor	%ymm3, %ymm6, %ymm3
	vpslld	$19, %ymm2, %ymm6
	vpsrld	$13, %ymm2, %ymm7
	vpor	%ymm6, %ymm7, %ymm6
	vpxor	%ymm6, %ymm3, %ymm3
	vpslld	$10, %ymm2, %ymm6
	vpsrld	$22, %ymm2, %ymm7
	vpor	%ymm6, %ymm7, %ymm6
	vpxor	%ymm6, %ymm3, %ymm3
	vpxor	%ymm1, %ymm10, %ymm6
	vpand	%ymm2, %ymm6, %ymm6
	vpand	%ymm1, %ymm10, %ymm7
	vmovdqa	%ymm10, %ymm9
	vmovdqu	%ymm9, 2464(%rsp)       # 32-byte Spill
	vpxor	%ymm7, %ymm6, %ymm6
	vpaddd	%ymm6, %ymm3, %ymm3
	vpaddd	%ymm0, %ymm3, %ymm3
	vpaddd	%ymm12, %ymm0, %ymm6
	vpslld	$27, %ymm6, %ymm0
	vpsrld	$5, %ymm6, %ymm7
	vpor	%ymm0, %ymm7, %ymm0
	vpxor	%ymm6, %ymm0, %ymm0
	vpslld	$26, %ymm0, %ymm7
	vpsrld	$6, %ymm0, %ymm0
	vpor	%ymm7, %ymm0, %ymm0
	vpslld	$7, %ymm6, %ymm7
	vpsrld	$25, %ymm6, %ymm8
	vpor	%ymm7, %ymm8, %ymm7
	vpxor	%ymm7, %ymm0, %ymm0
	vmovdqu	192(%rsi,%rax,4), %ymm7
	vpshufb	%ymm15, %ymm7, %ymm8
	vmovdqu	%ymm8, 64(%rsp)         # 32-byte Spill
	vpxor	%ymm4, %ymm5, %ymm7
	vpand	%ymm6, %ymm7, %ymm7
	vpxor	%ymm4, %ymm7, %ymm7
	vpaddd	2336(%rsp), %ymm0, %ymm0 # 32-byte Folded Reload
	vpaddd	%ymm14, %ymm7, %ymm7
	vpaddd	%ymm8, %ymm7, %ymm7
	vpaddd	%ymm0, %ymm7, %ymm7
	vpslld	$30, %ymm3, %ymm0
	vpsrld	$2, %ymm3, %ymm8
	vpor	%ymm0, %ymm8, %ymm0
	vpslld	$19, %ymm3, %ymm8
	vpsrld	$13, %ymm3, %ymm10
	vpor	%ymm8, %ymm10, %ymm8
	vpxor	%ymm8, %ymm0, %ymm0
	vpslld	$10, %ymm3, %ymm8
	vpsrld	$22, %ymm3, %ymm10
	vpor	%ymm8, %ymm10, %ymm8
	vpxor	%ymm8, %ymm0, %ymm0
	vpxor	%ymm1, %ymm2, %ymm8
	vpand	%ymm8, %ymm3, %ymm8
	vpand	%ymm1, %ymm2, %ymm10
	vpxor	%ymm10, %ymm8, %ymm8
	vpaddd	%ymm8, %ymm0, %ymm0
	vpaddd	%ymm7, %ymm0, %ymm0
	vpaddd	%ymm9, %ymm7, %ymm7
	vpslld	$27, %ymm7, %ymm8
	vpsrld	$5, %ymm7, %ymm10
	vpor	%ymm8, %ymm10, %ymm8
	vpxor	%ymm7, %ymm8, %ymm8
	vpslld	$26, %ymm8, %ymm10
	vpsrld	$6, %ymm8, %ymm8
	vpor	%ymm10, %ymm8, %ymm8
	vpslld	$7, %ymm7, %ymm10
	vpsrld	$25, %ymm7, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm10, %ymm8, %ymm8
	vpxor	%ymm5, %ymm6, %ymm10
	vpand	%ymm7, %ymm10, %ymm10
	vpxor	%ymm5, %ymm10, %ymm10
	vpaddd	%ymm4, %ymm10, %ymm4
	vmovdqu	256(%rsi,%rax,4), %ymm10
	vpshufb	%ymm15, %ymm10, %ymm9
	vmovdqu	%ymm9, 32(%rsp)         # 32-byte Spill
	vpaddd	2304(%rsp), %ymm8, %ymm8 # 32-byte Folded Reload
	vpaddd	%ymm9, %ymm4, %ymm4
	vpaddd	%ymm8, %ymm4, %ymm8
	vpslld	$30, %ymm0, %ymm4
	vpsrld	$2, %ymm0, %ymm10
	vpor	%ymm4, %ymm10, %ymm4
	vpslld	$19, %ymm0, %ymm10
	vpsrld	$13, %ymm0, %ymm12
	vpor	%ymm10, %ymm12, %ymm10
	vpxor	%ymm10, %ymm4, %ymm4
	vpslld	$10, %ymm0, %ymm10
	vpsrld	$22, %ymm0, %ymm12
	vpor	%ymm10, %ymm12, %ymm10
	vpxor	%ymm10, %ymm4, %ymm4
	vpxor	%ymm2, %ymm3, %ymm10
	vpand	%ymm10, %ymm0, %ymm10
	vpand	%ymm2, %ymm3, %ymm12
	vpxor	%ymm12, %ymm10, %ymm10
	vpaddd	%ymm10, %ymm4, %ymm10
	vpaddd	%ymm1, %ymm8, %ymm4
	vpaddd	%ymm8, %ymm10, %ymm1
	vpslld	$27, %ymm4, %ymm8
	vpsrld	$5, %ymm4, %ymm10
	vpor	%ymm8, %ymm10, %ymm8
	vpxor	%ymm4, %ymm8, %ymm8
	vpslld	$26, %ymm8, %ymm10
	vpsrld	$6, %ymm8, %ymm8
	vpor	%ymm10, %ymm8, %ymm8
	vpslld	$7, %ymm4, %ymm10
	vpsrld	$25, %ymm4, %ymm12
	vpor	%ymm10, %ymm12, %ymm10
	vpxor	%ymm10, %ymm8, %ymm8
	vpxor	%ymm6, %ymm7, %ymm10
	vpand	%ymm4, %ymm10, %ymm10
	vpxor	%ymm6, %ymm10, %ymm10
	vpaddd	%ymm5, %ymm10, %ymm5
	vmovdqu	320(%rsi,%rax,4), %ymm10
	vpshufb	%ymm15, %ymm10, %ymm9
	vmovdqu	%ymm9, (%rsp)           # 32-byte Spill
	vpaddd	2272(%rsp), %ymm8, %ymm8 # 32-byte Folded Reload
	vpaddd	%ymm9, %ymm5, %ymm5
	vpaddd	%ymm8, %ymm5, %ymm8
	vpslld	$30, %ymm1, %ymm5
	vpsrld	$2, %ymm1, %ymm10
	vpor	%ymm5, %ymm10, %ymm5
	vpslld	$19, %ymm1, %ymm10
	vpsrld	$13, %ymm1, %ymm12
	vpor	%ymm10, %ymm12, %ymm10
	vpxor	%ymm10, %ymm5, %ymm5
	vpslld	$10, %ymm1, %ymm10
	vpsrld	$22, %ymm1, %ymm12
	vpor	%ymm10, %ymm12, %ymm10
	vpxor	%ymm10, %ymm5, %ymm5
	vpxor	%ymm3, %ymm0, %ymm10
	vpand	%ymm10, %ymm1, %ymm10
	vpand	%ymm3, %ymm0, %ymm12
	vpxor	%ymm12, %ymm10, %ymm10
	vpaddd	%ymm10, %ymm5, %ymm10
	vpaddd	%ymm2, %ymm8, %ymm5
	vpaddd	%ymm8, %ymm10, %ymm2
	vpslld	$27, %ymm5, %ymm8
	vpsrld	$5, %ymm5, %ymm10
	vpor	%ymm8, %ymm10, %ymm8
	vpxor	%ymm5, %ymm8, %ymm8
	vpslld	$26, %ymm8, %ymm10
	vpsrld	$6, %ymm8, %ymm8
	vpor	%ymm10, %ymm8, %ymm8
	vpslld	$7, %ymm5, %ymm10
	vpsrld	$25, %ymm5, %ymm12
	vpor	%ymm10, %ymm12, %ymm10
	vpxor	%ymm10, %ymm8, %ymm8
	vpxor	%ymm7, %ymm4, %ymm10
	vpand	%ymm5, %ymm10, %ymm10
	vpxor	%ymm7, %ymm10, %ymm10
	vpaddd	%ymm6, %ymm10, %ymm6
	vmovdqu	384(%rsi,%rax,4), %ymm10
	vpshufb	%ymm15, %ymm10, %ymm9
	vmovdqu	%ymm9, -32(%rsp)        # 32-byte Spill
	vpaddd	2240(%rsp), %ymm8, %ymm8 # 32-byte Folded Reload
	vpaddd	%ymm9, %ymm6, %ymm6
	vpaddd	%ymm8, %ymm6, %ymm8
	vpslld	$30, %ymm2, %ymm6
	vpsrld	$2, %ymm2, %ymm10
	vpor	%ymm6, %ymm10, %ymm6
	vpslld	$19, %ymm2, %ymm10
	vpsrld	$13, %ymm2, %ymm12
	vpor	%ymm10, %ymm12, %ymm10
	vpxor	%ymm10, %ymm6, %ymm6
	vpslld	$10, %ymm2, %ymm10
	vpsrld	$22, %ymm2, %ymm12
	vpor	%ymm10, %ymm12, %ymm10
	vpxor	%ymm10, %ymm6, %ymm6
	vpxor	%ymm0, %ymm1, %ymm10
	vpand	%ymm10, %ymm2, %ymm10
	vpand	%ymm0, %ymm1, %ymm12
	vpxor	%ymm12, %ymm10, %ymm10
	vpaddd	%ymm10, %ymm6, %ymm10
	vpaddd	%ymm3, %ymm8, %ymm6
	vpaddd	%ymm8, %ymm10, %ymm3
	vpslld	$27, %ymm6, %ymm8
	vpsrld	$5, %ymm6, %ymm10
	vpor	%ymm8, %ymm10, %ymm8
	vpxor	%ymm6, %ymm8, %ymm8
	vpslld	$26, %ymm8, %ymm10
	vpsrld	$6, %ymm8, %ymm8
	vpor	%ymm10, %ymm8, %ymm8
	vpslld	$7, %ymm6, %ymm10
	vpsrld	$25, %ymm6, %ymm12
	vpor	%ymm10, %ymm12, %ymm10
	vpxor	%ymm10, %ymm8, %ymm8
	vpxor	%ymm4, %ymm5, %ymm10
	vpand	%ymm6, %ymm10, %ymm10
	vpxor	%ymm4, %ymm10, %ymm10
	vpaddd	%ymm7, %ymm10, %ymm7
	vmovdqu	448(%rsi,%rax,4), %ymm10
	vpshufb	%ymm15, %ymm10, %ymm9
	vmovdqu	%ymm9, -64(%rsp)        # 32-byte Spill
	vpaddd	2208(%rsp), %ymm8, %ymm8 # 32-byte Folded Reload
	vpaddd	%ymm9, %ymm7, %ymm7
	vpaddd	%ymm8, %ymm7, %ymm8
	vpslld	$30, %ymm3, %ymm7
	vpsrld	$2, %ymm3, %ymm10
	vpor	%ymm7, %ymm10, %ymm7
	vpslld	$19, %ymm3, %ymm10
	vpsrld	$13, %ymm3, %ymm12
	vpor	%ymm10, %ymm12, %ymm10
	vpxor	%ymm10, %ymm7, %ymm7
	vpslld	$10, %ymm3, %ymm10
	vpsrld	$22, %ymm3, %ymm12
	vpor	%ymm10, %ymm12, %ymm10
	vpxor	%ymm10, %ymm7, %ymm7
	vpxor	%ymm1, %ymm2, %ymm10
	vpand	%ymm10, %ymm3, %ymm10
	vpand	%ymm1, %ymm2, %ymm12
	vpxor	%ymm12, %ymm10, %ymm10
	vpaddd	%ymm10, %ymm7, %ymm10
	vpaddd	%ymm0, %ymm8, %ymm7
	vpaddd	%ymm8, %ymm10, %ymm0
	vpslld	$27, %ymm7, %ymm8
	vpsrld	$5, %ymm7, %ymm10
	vpor	%ymm8, %ymm10, %ymm8
	vpxor	%ymm7, %ymm8, %ymm8
	vpslld	$26, %ymm8, %ymm10
	vpsrld	$6, %ymm8, %ymm8
	vpor	%ymm10, %ymm8, %ymm8
	vpslld	$7, %ymm7, %ymm10
	vpsrld	$25, %ymm7, %ymm12
	vpor	%ymm10, %ymm12, %ymm10
	vpxor	%ymm10, %ymm8, %ymm8
	vpxor	%ymm5, %ymm6, %ymm10
	vpand	%ymm7, %ymm10, %ymm10
	vpxor	%ymm5, %ymm10, %ymm10
	vpaddd	%ymm4, %ymm10, %ymm4
	vmovdqu	512(%rsi,%rax,4), %ymm10
	vpshufb	%ymm15, %ymm10, %ymm9
	vmovdqu	%ymm9, 320(%rsp)        # 32-byte Spill
	vpaddd	2176(%rsp), %ymm8, %ymm8 # 32-byte Folded Reload
	vpaddd	%ymm9, %ymm4, %ymm4
	vpaddd	%ymm8, %ymm4, %ymm8
	vpslld	$30, %ymm0, %ymm4
	vpsrld	$2, %ymm0, %ymm10
	vpor	%ymm4, %ymm10, %ymm4
	vpslld	$19, %ymm0, %ymm10
	vpsrld	$13, %ymm0, %ymm12
	vpor	%ymm10, %ymm12, %ymm10
	vpxor	%ymm10, %ymm4, %ymm4
	vpslld	$10, %ymm0, %ymm10
	vpsrld	$22, %ymm0, %ymm12
	vpor	%ymm10, %ymm12, %ymm10
	vpxor	%ymm10, %ymm4, %ymm4
	vpxor	%ymm2, %ymm3, %ymm10
	vpand	%ymm10, %ymm0, %ymm10
	vpand	%ymm2, %ymm3, %ymm12
	vpxor	%ymm12, %ymm10, %ymm10
	vpaddd	%ymm10, %ymm4, %ymm10
	vpaddd	%ymm1, %ymm8, %ymm4
	vpaddd	%ymm8, %ymm10, %ymm1
	vpslld	$27, %ymm4, %ymm8
	vpsrld	$5, %ymm4, %ymm10
	vpor	%ymm8, %ymm10, %ymm8
	vpxor	%ymm4, %ymm8, %ymm8
	vpslld	$26, %ymm8, %ymm10
	vpsrld	$6, %ymm8, %ymm8
	vpor	%ymm10, %ymm8, %ymm8
	vpslld	$7, %ymm4, %ymm10
	vpsrld	$25, %ymm4, %ymm12
	vpor	%ymm10, %ymm12, %ymm10
	vpxor	%ymm10, %ymm8, %ymm8
	vpxor	%ymm6, %ymm7, %ymm10
	vpand	%ymm4, %ymm10, %ymm10
	vpxor	%ymm6, %ymm10, %ymm10
	vpaddd	%ymm5, %ymm10, %ymm5
	vmovdqu	576(%rsi,%rax,4), %ymm10
	vpshufb	%ymm15, %ymm10, %ymm9
	vmovdqu	%ymm9, 192(%rsp)        # 32-byte Spill
	vpaddd	2144(%rsp), %ymm8, %ymm8 # 32-byte Folded Reload
	vpaddd	%ymm9, %ymm5, %ymm5
	vpaddd	%ymm8, %ymm5, %ymm5
	vpslld	$30, %ymm1, %ymm8
	vpsrld	$2, %ymm1, %ymm10
	vpor	%ymm8, %ymm10, %ymm8
	vpslld	$19, %ymm1, %ymm10
	vpsrld	$13, %ymm1, %ymm12
	vpor	%ymm10, %ymm12, %ymm10
	vpxor	%ymm10, %ymm8, %ymm8
	vpslld	$10, %ymm1, %ymm10
	vpsrld	$22, %ymm1, %ymm12
	vpor	%ymm10, %ymm12, %ymm10
	vpxor	%ymm10, %ymm8, %ymm8
	vpxor	%ymm3, %ymm0, %ymm10
	vpand	%ymm10, %ymm1, %ymm10
	vpand	%ymm3, %ymm0, %ymm12
	vpxor	%ymm12, %ymm10, %ymm10
	vpaddd	%ymm10, %ymm8, %ymm10
	vpaddd	%ymm2, %ymm5, %ymm8
	vpaddd	%ymm5, %ymm10, %ymm2
	vpslld	$27, %ymm8, %ymm5
	vpsrld	$5, %ymm8, %ymm10
	vpor	%ymm5, %ymm10, %ymm5
	vpxor	%ymm8, %ymm5, %ymm5
	vpslld	$26, %ymm5, %ymm10
	vpsrld	$6, %ymm5, %ymm5
	vpor	%ymm10, %ymm5, %ymm5
	vpslld	$7, %ymm8, %ymm10
	vpsrld	$25, %ymm8, %ymm12
	vpor	%ymm10, %ymm12, %ymm10
	vpxor	%ymm10, %ymm5, %ymm5
	vpxor	%ymm7, %ymm4, %ymm10
	vpand	%ymm8, %ymm10, %ymm10
	vpxor	%ymm7, %ymm10, %ymm10
	vpaddd	%ymm6, %ymm10, %ymm6
	vmovdqu	640(%rsi,%rax,4), %ymm10
	vpshufb	%ymm15, %ymm10, %ymm9
	vmovdqu	%ymm9, 160(%rsp)        # 32-byte Spill
	vpaddd	2112(%rsp), %ymm5, %ymm5 # 32-byte Folded Reload
	vpaddd	%ymm9, %ymm6, %ymm6
	vpaddd	%ymm5, %ymm6, %ymm5
	vpslld	$30, %ymm2, %ymm6
	vpsrld	$2, %ymm2, %ymm10
	vpor	%ymm6, %ymm10, %ymm6
	vpslld	$19, %ymm2, %ymm10
	vpsrld	$13, %ymm2, %ymm12
	vpor	%ymm10, %ymm12, %ymm10
	vpxor	%ymm10, %ymm6, %ymm6
	vpslld	$10, %ymm2, %ymm10
	vpsrld	$22, %ymm2, %ymm12
	vpor	%ymm10, %ymm12, %ymm10
	vpxor	%ymm10, %ymm6, %ymm6
	vpxor	%ymm0, %ymm1, %ymm10
	vpand	%ymm10, %ymm2, %ymm10
	vpand	%ymm0, %ymm1, %ymm12
	vpxor	%ymm12, %ymm10, %ymm10
	vpaddd	%ymm10, %ymm6, %ymm6
	vpaddd	%ymm3, %ymm5, %ymm12
	vpaddd	%ymm5, %ymm6, %ymm3
	vpslld	$27, %ymm12, %ymm5
	vpsrld	$5, %ymm12, %ymm6
	vpor	%ymm5, %ymm6, %ymm5
	vpxor	%ymm12, %ymm5, %ymm5
	vpslld	$26, %ymm5, %ymm6
	vpsrld	$6, %ymm5, %ymm5
	vpor	%ymm6, %ymm5, %ymm5
	vpslld	$7, %ymm12, %ymm6
	vpsrld	$25, %ymm12, %ymm10
	vpor	%ymm6, %ymm10, %ymm6
	vpxor	%ymm6, %ymm5, %ymm5
	vpxor	%ymm4, %ymm8, %ymm6
	vpand	%ymm12, %ymm6, %ymm6
	vpxor	%ymm4, %ymm6, %ymm6
	vpaddd	%ymm7, %ymm6, %ymm6
	vmovdqu	704(%rsi,%rax,4), %ymm7
	vpshufb	%ymm15, %ymm7, %ymm7
	vmovdqu	%ymm7, 128(%rsp)        # 32-byte Spill
	vpaddd	2080(%rsp), %ymm5, %ymm5 # 32-byte Folded Reload
	vpaddd	%ymm7, %ymm6, %ymm6
	vpaddd	%ymm5, %ymm6, %ymm6
	vpslld	$30, %ymm3, %ymm5
	vpsrld	$2, %ymm3, %ymm7
	vpor	%ymm5, %ymm7, %ymm5
	vpslld	$19, %ymm3, %ymm7
	vpsrld	$13, %ymm3, %ymm10
	vpor	%ymm7, %ymm10, %ymm7
	vpxor	%ymm7, %ymm5, %ymm5
	vpslld	$10, %ymm3, %ymm7
	vpsrld	$22, %ymm3, %ymm10
	vpor	%ymm7, %ymm10, %ymm7
	vpxor	%ymm7, %ymm5, %ymm5
	vpxor	%ymm1, %ymm2, %ymm7
	vpand	%ymm7, %ymm3, %ymm7
	vpand	%ymm1, %ymm2, %ymm10
	vpxor	%ymm10, %ymm7, %ymm7
	vpaddd	%ymm7, %ymm5, %ymm7
	vpaddd	%ymm0, %ymm6, %ymm5
	vpaddd	%ymm6, %ymm7, %ymm0
	vpslld	$27, %ymm5, %ymm6
	vpsrld	$5, %ymm5, %ymm7
	vpor	%ymm6, %ymm7, %ymm6
	vpxor	%ymm5, %ymm6, %ymm6
	vpslld	$26, %ymm6, %ymm7
	vpsrld	$6, %ymm6, %ymm6
	vpor	%ymm7, %ymm6, %ymm6
	vpslld	$7, %ymm5, %ymm7
	vpsrld	$25, %ymm5, %ymm10
	vpor	%ymm7, %ymm10, %ymm7
	vpxor	%ymm7, %ymm6, %ymm6
	vpxor	%ymm8, %ymm12, %ymm7
	vpand	%ymm5, %ymm7, %ymm7
	vpxor	%ymm8, %ymm7, %ymm7
	vpaddd	%ymm4, %ymm7, %ymm4
	vmovdqu	768(%rsi,%rax,4), %ymm7
	vpshufb	%ymm15, %ymm7, %ymm7
	vmovdqu	%ymm7, 224(%rsp)        # 32-byte Spill
	vpaddd	2048(%rsp), %ymm6, %ymm6 # 32-byte Folded Reload
	vpaddd	%ymm7, %ymm4, %ymm4
	vpaddd	%ymm6, %ymm4, %ymm6
	vpslld	$30, %ymm0, %ymm4
	vpsrld	$2, %ymm0, %ymm7
	vpor	%ymm4, %ymm7, %ymm4
	vpslld	$19, %ymm0, %ymm7
	vpsrld	$13, %ymm0, %ymm10
	vpor	%ymm7, %ymm10, %ymm7
	vpxor	%ymm7, %ymm4, %ymm4
	vpslld	$10, %ymm0, %ymm7
	vpsrld	$22, %ymm0, %ymm10
	vpor	%ymm7, %ymm10, %ymm7
	vpxor	%ymm7, %ymm4, %ymm4
	vpxor	%ymm2, %ymm3, %ymm7
	vpand	%ymm7, %ymm0, %ymm7
	vpand	%ymm2, %ymm3, %ymm10
	vpxor	%ymm10, %ymm7, %ymm7
	vpaddd	%ymm7, %ymm4, %ymm7
	vpaddd	%ymm1, %ymm6, %ymm4
	vpaddd	%ymm6, %ymm7, %ymm1
	vpslld	$27, %ymm4, %ymm6
	vpsrld	$5, %ymm4, %ymm7
	vpor	%ymm6, %ymm7, %ymm6
	vpxor	%ymm4, %ymm6, %ymm6
	vpslld	$26, %ymm6, %ymm7
	vpsrld	$6, %ymm6, %ymm6
	vpor	%ymm7, %ymm6, %ymm6
	vpslld	$7, %ymm4, %ymm7
	vpsrld	$25, %ymm4, %ymm10
	vpor	%ymm7, %ymm10, %ymm7
	vpxor	%ymm7, %ymm6, %ymm6
	vpxor	%ymm12, %ymm5, %ymm7
	vpand	%ymm4, %ymm7, %ymm7
	vpxor	%ymm12, %ymm7, %ymm7
	vpaddd	%ymm8, %ymm7, %ymm7
	vmovdqu	832(%rsi,%rax,4), %ymm8
	vpshufb	%ymm15, %ymm8, %ymm8
	vmovdqu	%ymm8, 256(%rsp)        # 32-byte Spill
	vpaddd	2016(%rsp), %ymm6, %ymm6 # 32-byte Folded Reload
	vpaddd	%ymm8, %ymm7, %ymm7
	vpaddd	%ymm6, %ymm7, %ymm7
	vpslld	$30, %ymm1, %ymm6
	vpsrld	$2, %ymm1, %ymm8
	vpor	%ymm6, %ymm8, %ymm6
	vpslld	$19, %ymm1, %ymm8
	vpsrld	$13, %ymm1, %ymm10
	vpor	%ymm8, %ymm10, %ymm8
	vpxor	%ymm8, %ymm6, %ymm6
	vpslld	$10, %ymm1, %ymm8
	vpsrld	$22, %ymm1, %ymm10
	vpor	%ymm8, %ymm10, %ymm8
	vpxor	%ymm8, %ymm6, %ymm6
	vpxor	%ymm3, %ymm0, %ymm8
	vpand	%ymm8, %ymm1, %ymm8
	vpand	%ymm3, %ymm0, %ymm10
	vpxor	%ymm10, %ymm8, %ymm8
	vpaddd	%ymm8, %ymm6, %ymm8
	vpaddd	%ymm2, %ymm7, %ymm6
	vpaddd	%ymm7, %ymm8, %ymm2
	vpslld	$27, %ymm6, %ymm7
	vpsrld	$5, %ymm6, %ymm8
	vpor	%ymm7, %ymm8, %ymm7
	vpxor	%ymm6, %ymm7, %ymm7
	vpslld	$26, %ymm7, %ymm8
	vpsrld	$6, %ymm7, %ymm7
	vpor	%ymm8, %ymm7, %ymm7
	vpslld	$7, %ymm6, %ymm8
	vpsrld	$25, %ymm6, %ymm10
	vpor	%ymm8, %ymm10, %ymm8
	vpxor	%ymm8, %ymm7, %ymm7
	vpxor	%ymm5, %ymm4, %ymm8
	vpand	%ymm6, %ymm8, %ymm8
	vpxor	%ymm5, %ymm8, %ymm8
	vpaddd	%ymm12, %ymm8, %ymm8
	vmovdqu	896(%rsi,%rax,4), %ymm10
	vpshufb	%ymm15, %ymm10, %ymm10
	vpaddd	1984(%rsp), %ymm7, %ymm7 # 32-byte Folded Reload
	vpaddd	%ymm10, %ymm8, %ymm8
	vpaddd	%ymm7, %ymm8, %ymm8
	vpslld	$30, %ymm2, %ymm7
	vpsrld	$2, %ymm2, %ymm12
	vpor	%ymm7, %ymm12, %ymm7
	vpslld	$19, %ymm2, %ymm12
	vpsrld	$13, %ymm2, %ymm13
	vpor	%ymm12, %ymm13, %ymm12
	vpxor	%ymm12, %ymm7, %ymm7
	vpslld	$10, %ymm2, %ymm12
	vpsrld	$22, %ymm2, %ymm13
	vpor	%ymm12, %ymm13, %ymm12
	vpxor	%ymm12, %ymm7, %ymm7
	vpxor	%ymm0, %ymm1, %ymm12
	vpand	%ymm12, %ymm2, %ymm12
	vpand	%ymm0, %ymm1, %ymm13
	vpxor	%ymm13, %ymm12, %ymm12
	vpaddd	%ymm12, %ymm7, %ymm12
	vpaddd	%ymm3, %ymm8, %ymm7
	vpaddd	%ymm8, %ymm12, %ymm3
	vpslld	$27, %ymm7, %ymm8
	vpsrld	$5, %ymm7, %ymm12
	vpor	%ymm8, %ymm12, %ymm8
	vpxor	%ymm7, %ymm8, %ymm8
	vpslld	$26, %ymm8, %ymm12
	vpsrld	$6, %ymm8, %ymm8
	vpor	%ymm12, %ymm8, %ymm8
	vpslld	$7, %ymm7, %ymm12
	vpsrld	$25, %ymm7, %ymm13
	vpor	%ymm12, %ymm13, %ymm12
	vpxor	%ymm12, %ymm8, %ymm8
	vpxor	%ymm4, %ymm6, %ymm12
	vpand	%ymm7, %ymm12, %ymm12
	vpxor	%ymm4, %ymm12, %ymm12
	vpaddd	%ymm5, %ymm12, %ymm5
	vmovdqu	960(%rsi,%rax,4), %ymm12
	vpshufb	%ymm15, %ymm12, %ymm12
	vpaddd	1952(%rsp), %ymm8, %ymm8 # 32-byte Folded Reload
	vpaddd	%ymm12, %ymm5, %ymm5
	vpaddd	%ymm8, %ymm5, %ymm5
	vpslld	$30, %ymm3, %ymm8
	vpsrld	$2, %ymm3, %ymm13
	vpor	%ymm8, %ymm13, %ymm8
	vpslld	$19, %ymm3, %ymm13
	vpsrld	$13, %ymm3, %ymm11
	vpor	%ymm13, %ymm11, %ymm11
	vpxor	%ymm11, %ymm8, %ymm8
	vpslld	$10, %ymm3, %ymm11
	vpsrld	$22, %ymm3, %ymm13
	vpor	%ymm11, %ymm13, %ymm11
	vpxor	%ymm11, %ymm8, %ymm8
	vpxor	%ymm1, %ymm2, %ymm11
	vpand	%ymm11, %ymm3, %ymm11
	vpand	%ymm1, %ymm2, %ymm13
	vpxor	%ymm13, %ymm11, %ymm11
	vpaddd	%ymm11, %ymm8, %ymm11
	vpaddd	%ymm0, %ymm5, %ymm8
	vpaddd	%ymm5, %ymm11, %ymm0
	vmovdqu	-96(%rsp), %ymm14       # 32-byte Reload
	vpslld	$21, %ymm14, %ymm5
	vpsrld	$11, %ymm14, %ymm11
	vpor	%ymm5, %ymm11, %ymm5
	vpxor	%ymm14, %ymm5, %ymm5
	vpslld	$25, %ymm5, %ymm11
	vpsrld	$7, %ymm5, %ymm5
	vpor	%ymm11, %ymm5, %ymm5
	vpsrld	$3, %ymm14, %ymm11
	vpxor	%ymm11, %ymm5, %ymm5
	vmovdqu	%ymm10, 384(%rsp)       # 32-byte Spill
	vpslld	$30, %ymm10, %ymm11
	vpsrld	$2, %ymm10, %ymm13
	vpor	%ymm11, %ymm13, %ymm11
	vpxor	%ymm10, %ymm11, %ymm11
	vpslld	$15, %ymm11, %ymm13
	vpsrld	$17, %ymm11, %ymm11
	vpor	%ymm13, %ymm11, %ymm11
	vpsrld	$10, %ymm10, %ymm13
	vpxor	%ymm13, %ymm11, %ymm11
	vmovdqu	-128(%rsp), %ymm9       # 32-byte Reload
	vpaddd	192(%rsp), %ymm9, %ymm13 # 32-byte Folded Reload
	vpaddd	%ymm5, %ymm13, %ymm5
	vpaddd	%ymm11, %ymm5, %ymm13
	vpslld	$27, %ymm8, %ymm5
	vpsrld	$5, %ymm8, %ymm11
	vpor	%ymm5, %ymm11, %ymm5
	vpxor	%ymm8, %ymm5, %ymm5
	vpslld	$26, %ymm5, %ymm11
	vpsrld	$6, %ymm5, %ymm5
	vpor	%ymm11, %ymm5, %ymm5
	vpslld	$7, %ymm8, %ymm11
	vpsrld	$25, %ymm8, %ymm9
	vpor	%ymm11, %ymm9, %ymm9
	vpxor	%ymm9, %ymm5, %ymm5
	vpxor	%ymm6, %ymm7, %ymm9
	vpand	%ymm8, %ymm9, %ymm9
	vpxor	%ymm6, %ymm9, %ymm9
	vpaddd	%ymm4, %ymm9, %ymm4
	vpaddd	1920(%rsp), %ymm5, %ymm5 # 32-byte Folded Reload
	vpaddd	%ymm13, %ymm4, %ymm4
	vpaddd	%ymm5, %ymm4, %ymm4
	vpslld	$30, %ymm0, %ymm5
	vpsrld	$2, %ymm0, %ymm9
	vpor	%ymm5, %ymm9, %ymm5
	vpslld	$19, %ymm0, %ymm9
	vpsrld	$13, %ymm0, %ymm11
	vpor	%ymm9, %ymm11, %ymm9
	vpxor	%ymm9, %ymm5, %ymm5
	vpslld	$10, %ymm0, %ymm9
	vpsrld	$22, %ymm0, %ymm11
	vpor	%ymm9, %ymm11, %ymm9
	vpxor	%ymm9, %ymm5, %ymm5
	vpxor	%ymm2, %ymm3, %ymm9
	vpand	%ymm9, %ymm0, %ymm9
	vpand	%ymm2, %ymm3, %ymm11
	vpxor	%ymm11, %ymm9, %ymm9
	vpaddd	%ymm9, %ymm5, %ymm9
	vpaddd	%ymm1, %ymm4, %ymm5
	vpaddd	%ymm4, %ymm9, %ymm1
	vmovdqu	352(%rsp), %ymm15       # 32-byte Reload
	vpslld	$21, %ymm15, %ymm4
	vpsrld	$11, %ymm15, %ymm9
	vpor	%ymm4, %ymm9, %ymm4
	vpxor	%ymm15, %ymm4, %ymm4
	vpslld	$25, %ymm4, %ymm9
	vpsrld	$7, %ymm4, %ymm4
	vpor	%ymm9, %ymm4, %ymm4
	vpsrld	$3, %ymm15, %ymm9
	vpxor	%ymm9, %ymm4, %ymm4
	vmovdqu	%ymm12, 96(%rsp)        # 32-byte Spill
	vpslld	$30, %ymm12, %ymm9
	vpsrld	$2, %ymm12, %ymm11
	vpor	%ymm9, %ymm11, %ymm9
	vpxor	%ymm12, %ymm9, %ymm9
	vpslld	$15, %ymm9, %ymm11
	vpsrld	$17, %ymm9, %ymm9
	vpor	%ymm11, %ymm9, %ymm9
	vpsrld	$10, %ymm12, %ymm11
	vpxor	%ymm11, %ymm9, %ymm9
	vpaddd	160(%rsp), %ymm14, %ymm11 # 32-byte Folded Reload
	vpaddd	%ymm4, %ymm11, %ymm4
	vpaddd	%ymm9, %ymm4, %ymm10
	vpslld	$27, %ymm5, %ymm4
	vpsrld	$5, %ymm5, %ymm9
	vpor	%ymm4, %ymm9, %ymm4
	vpxor	%ymm5, %ymm4, %ymm4
	vpslld	$26, %ymm4, %ymm9
	vpsrld	$6, %ymm4, %ymm4
	vpor	%ymm9, %ymm4, %ymm4
	vpslld	$7, %ymm5, %ymm9
	vpsrld	$25, %ymm5, %ymm11
	vpor	%ymm9, %ymm11, %ymm9
	vpxor	%ymm9, %ymm4, %ymm4
	vpxor	%ymm7, %ymm8, %ymm9
	vpand	%ymm5, %ymm9, %ymm9
	vpxor	%ymm7, %ymm9, %ymm9
	vpaddd	%ymm6, %ymm9, %ymm6
	vpaddd	1888(%rsp), %ymm4, %ymm4 # 32-byte Folded Reload
	vpaddd	%ymm10, %ymm6, %ymm6
	vpaddd	%ymm4, %ymm6, %ymm4
	vpslld	$30, %ymm1, %ymm6
	vpsrld	$2, %ymm1, %ymm9
	vpor	%ymm6, %ymm9, %ymm6
	vpslld	$19, %ymm1, %ymm9
	vpsrld	$13, %ymm1, %ymm11
	vpor	%ymm9, %ymm11, %ymm9
	vpxor	%ymm9, %ymm6, %ymm6
	vpslld	$10, %ymm1, %ymm9
	vpsrld	$22, %ymm1, %ymm11
	vpor	%ymm9, %ymm11, %ymm9
	vpxor	%ymm9, %ymm6, %ymm6
	vpxor	%ymm3, %ymm0, %ymm9
	vpand	%ymm9, %ymm1, %ymm9
	vpand	%ymm3, %ymm0, %ymm11
	vpxor	%ymm11, %ymm9, %ymm9
	vpaddd	%ymm9, %ymm6, %ymm9
	vpaddd	%ymm2, %ymm4, %ymm6
	vpaddd	%ymm4, %ymm9, %ymm2
	vmovdqu	64(%rsp), %ymm11        # 32-byte Reload
	vpslld	$21, %ymm11, %ymm4
	vpsrld	$11, %ymm11, %ymm9
	vpor	%ymm4, %ymm9, %ymm4
	vpxor	%ymm11, %ymm4, %ymm4
	vpslld	$25, %ymm4, %ymm9
	vpsrld	$7, %ymm4, %ymm4
	vpor	%ymm9, %ymm4, %ymm4
	vpsrld	$3, %ymm11, %ymm9
	vmovdqa	%ymm11, %ymm14
	vpxor	%ymm9, %ymm4, %ymm4
	vmovdqu	%ymm13, -128(%rsp)      # 32-byte Spill
	vpslld	$30, %ymm13, %ymm9
	vpsrld	$2, %ymm13, %ymm11
	vpor	%ymm9, %ymm11, %ymm9
	vpxor	%ymm13, %ymm9, %ymm9
	vpslld	$15, %ymm9, %ymm11
	vpsrld	$17, %ymm9, %ymm9
	vpor	%ymm11, %ymm9, %ymm9
	vpsrld	$10, %ymm13, %ymm11
	vpxor	%ymm11, %ymm9, %ymm9
	vpaddd	128(%rsp), %ymm15, %ymm11 # 32-byte Folded Reload
	vpaddd	%ymm4, %ymm11, %ymm4
	vpaddd	%ymm9, %ymm4, %ymm12
	vpslld	$27, %ymm6, %ymm4
	vpsrld	$5, %ymm6, %ymm9
	vpor	%ymm4, %ymm9, %ymm4
	vpxor	%ymm6, %ymm4, %ymm4
	vpslld	$26, %ymm4, %ymm9
	vpsrld	$6, %ymm4, %ymm4
	vpor	%ymm9, %ymm4, %ymm4
	vpslld	$7, %ymm6, %ymm9
	vpsrld	$25, %ymm6, %ymm11
	vpor	%ymm9, %ymm11, %ymm9
	vpxor	%ymm9, %ymm4, %ymm4
	vpxor	%ymm8, %ymm5, %ymm9
	vpand	%ymm6, %ymm9, %ymm9
	vpxor	%ymm8, %ymm9, %ymm9
	vpaddd	%ymm7, %ymm9, %ymm7
	vpaddd	1856(%rsp), %ymm4, %ymm4 # 32-byte Folded Reload
	vpaddd	%ymm12, %ymm7, %ymm7
	vpaddd	%ymm4, %ymm7, %ymm4
	vpslld	$30, %ymm2, %ymm7
	vpsrld	$2, %ymm2, %ymm9
	vpor	%ymm7, %ymm9, %ymm7
	vpslld	$19, %ymm2, %ymm9
	vpsrld	$13, %ymm2, %ymm11
	vpor	%ymm9, %ymm11, %ymm9
	vpxor	%ymm9, %ymm7, %ymm7
	vpslld	$10, %ymm2, %ymm9
	vpsrld	$22, %ymm2, %ymm11
	vpor	%ymm9, %ymm11, %ymm9
	vpxor	%ymm9, %ymm7, %ymm7
	vpxor	%ymm0, %ymm1, %ymm9
	vpand	%ymm9, %ymm2, %ymm9
	vpand	%ymm0, %ymm1, %ymm11
	vpxor	%ymm11, %ymm9, %ymm9
	vpaddd	%ymm9, %ymm7, %ymm9
	vpaddd	%ymm3, %ymm4, %ymm7
	vpaddd	%ymm4, %ymm9, %ymm3
	vmovdqu	32(%rsp), %ymm11        # 32-byte Reload
	vpslld	$21, %ymm11, %ymm4
	vpsrld	$11, %ymm11, %ymm9
	vpor	%ymm4, %ymm9, %ymm4
	vpxor	%ymm11, %ymm4, %ymm4
	vpslld	$25, %ymm4, %ymm9
	vpsrld	$7, %ymm4, %ymm4
	vpor	%ymm9, %ymm4, %ymm4
	vpsrld	$3, %ymm11, %ymm9
	vmovdqa	%ymm11, %ymm13
	vpxor	%ymm9, %ymm4, %ymm4
	vmovdqu	%ymm10, -96(%rsp)       # 32-byte Spill
	vpslld	$30, %ymm10, %ymm9
	vpsrld	$2, %ymm10, %ymm11
	vpor	%ymm9, %ymm11, %ymm9
	vpxor	%ymm10, %ymm9, %ymm9
	vpslld	$15, %ymm9, %ymm11
	vpsrld	$17, %ymm9, %ymm9
	vpor	%ymm11, %ymm9, %ymm9
	vpsrld	$10, %ymm10, %ymm11
	vpxor	%ymm11, %ymm9, %ymm9
	vpaddd	224(%rsp), %ymm14, %ymm11 # 32-byte Folded Reload
	vpaddd	%ymm4, %ymm11, %ymm4
	vpaddd	%ymm9, %ymm4, %ymm10
	vpslld	$27, %ymm7, %ymm4
	vpsrld	$5, %ymm7, %ymm9
	vpor	%ymm4, %ymm9, %ymm4
	vpxor	%ymm7, %ymm4, %ymm4
	vpslld	$26, %ymm4, %ymm9
	vpsrld	$6, %ymm4, %ymm4
	vpor	%ymm9, %ymm4, %ymm4
	vpslld	$7, %ymm7, %ymm9
	vpsrld	$25, %ymm7, %ymm11
	vpor	%ymm9, %ymm11, %ymm9
	vpxor	%ymm9, %ymm4, %ymm4
	vpxor	%ymm5, %ymm6, %ymm9
	vpand	%ymm7, %ymm9, %ymm9
	vpxor	%ymm5, %ymm9, %ymm9
	vpaddd	%ymm8, %ymm9, %ymm8
	vpaddd	1824(%rsp), %ymm4, %ymm4 # 32-byte Folded Reload
	vpaddd	%ymm10, %ymm8, %ymm8
	vpaddd	%ymm4, %ymm8, %ymm4
	vpslld	$30, %ymm3, %ymm8
	vpsrld	$2, %ymm3, %ymm9
	vpor	%ymm8, %ymm9, %ymm8
	vpslld	$19, %ymm3, %ymm9
	vpsrld	$13, %ymm3, %ymm11
	vpor	%ymm9, %ymm11, %ymm9
	vpxor	%ymm9, %ymm8, %ymm8
	vpslld	$10, %ymm3, %ymm9
	vpsrld	$22, %ymm3, %ymm11
	vpor	%ymm9, %ymm11, %ymm9
	vpxor	%ymm9, %ymm8, %ymm8
	vpxor	%ymm1, %ymm2, %ymm9
	vpand	%ymm9, %ymm3, %ymm9
	vpand	%ymm1, %ymm2, %ymm11
	vpxor	%ymm11, %ymm9, %ymm9
	vpaddd	%ymm9, %ymm8, %ymm8
	vpaddd	%ymm0, %ymm4, %ymm9
	vpaddd	%ymm4, %ymm8, %ymm0
	vmovdqu	(%rsp), %ymm11          # 32-byte Reload
	vpslld	$21, %ymm11, %ymm4
	vpsrld	$11, %ymm11, %ymm8
	vpor	%ymm4, %ymm8, %ymm4
	vpxor	%ymm11, %ymm4, %ymm4
	vpslld	$25, %ymm4, %ymm8
	vpsrld	$7, %ymm4, %ymm4
	vpor	%ymm8, %ymm4, %ymm4
	vpsrld	$3, %ymm11, %ymm8
	vmovdqa	%ymm11, %ymm14
	vpxor	%ymm8, %ymm4, %ymm4
	vmovdqu	%ymm12, 288(%rsp)       # 32-byte Spill
	vpslld	$30, %ymm12, %ymm8
	vpsrld	$2, %ymm12, %ymm11
	vpor	%ymm8, %ymm11, %ymm8
	vpxor	%ymm12, %ymm8, %ymm8
	vpslld	$15, %ymm8, %ymm11
	vpsrld	$17, %ymm8, %ymm8
	vpor	%ymm11, %ymm8, %ymm8
	vpsrld	$10, %ymm12, %ymm11
	vpxor	%ymm11, %ymm8, %ymm8
	vpaddd	256(%rsp), %ymm13, %ymm11 # 32-byte Folded Reload
	vpaddd	%ymm4, %ymm11, %ymm4
	vpaddd	%ymm8, %ymm4, %ymm12
	vpslld	$27, %ymm9, %ymm4
	vpsrld	$5, %ymm9, %ymm8
	vpor	%ymm4, %ymm8, %ymm4
	vpxor	%ymm9, %ymm4, %ymm4
	vpslld	$26, %ymm4, %ymm8
	vpsrld	$6, %ymm4, %ymm4
	vpor	%ymm8, %ymm4, %ymm4
	vpslld	$7, %ymm9, %ymm8
	vpsrld	$25, %ymm9, %ymm11
	vpor	%ymm8, %ymm11, %ymm8
	vpxor	%ymm8, %ymm4, %ymm4
	vpxor	%ymm6, %ymm7, %ymm8
	vpand	%ymm9, %ymm8, %ymm8
	vpxor	%ymm6, %ymm8, %ymm8
	vpaddd	%ymm5, %ymm8, %ymm5
	vpaddd	1792(%rsp), %ymm4, %ymm4 # 32-byte Folded Reload
	vpaddd	%ymm12, %ymm5, %ymm5
	vpaddd	%ymm4, %ymm5, %ymm4
	vpslld	$30, %ymm0, %ymm5
	vpsrld	$2, %ymm0, %ymm8
	vpor	%ymm5, %ymm8, %ymm5
	vpslld	$19, %ymm0, %ymm8
	vpsrld	$13, %ymm0, %ymm11
	vpor	%ymm8, %ymm11, %ymm8
	vpxor	%ymm8, %ymm5, %ymm5
	vpslld	$10, %ymm0, %ymm8
	vpsrld	$22, %ymm0, %ymm11
	vpor	%ymm8, %ymm11, %ymm8
	vpxor	%ymm8, %ymm5, %ymm5
	vpxor	%ymm2, %ymm3, %ymm8
	vpand	%ymm8, %ymm0, %ymm8
	vpand	%ymm2, %ymm3, %ymm11
	vpxor	%ymm11, %ymm8, %ymm8
	vpaddd	%ymm8, %ymm5, %ymm5
	vpaddd	%ymm1, %ymm4, %ymm11
	vpaddd	%ymm4, %ymm5, %ymm1
	vmovdqu	-32(%rsp), %ymm13       # 32-byte Reload
	vpslld	$21, %ymm13, %ymm4
	vpsrld	$11, %ymm13, %ymm5
	vpor	%ymm4, %ymm5, %ymm4
	vpxor	%ymm13, %ymm4, %ymm4
	vpslld	$25, %ymm4, %ymm5
	vpsrld	$7, %ymm4, %ymm4
	vpor	%ymm5, %ymm4, %ymm4
	vpsrld	$3, %ymm13, %ymm5
	vpxor	%ymm5, %ymm4, %ymm4
	vmovdqu	%ymm10, 64(%rsp)        # 32-byte Spill
	vpslld	$30, %ymm10, %ymm5
	vpsrld	$2, %ymm10, %ymm8
	vpor	%ymm5, %ymm8, %ymm5
	vpxor	%ymm10, %ymm5, %ymm5
	vpslld	$15, %ymm5, %ymm8
	vpsrld	$17, %ymm5, %ymm5
	vpor	%ymm8, %ymm5, %ymm5
	vpsrld	$10, %ymm10, %ymm8
	vpxor	%ymm8, %ymm5, %ymm5
	vmovdqu	384(%rsp), %ymm15       # 32-byte Reload
	vpaddd	%ymm14, %ymm15, %ymm8
	vpaddd	%ymm4, %ymm8, %ymm4
	vpaddd	%ymm5, %ymm4, %ymm10
	vpslld	$27, %ymm11, %ymm4
	vpsrld	$5, %ymm11, %ymm5
	vpor	%ymm4, %ymm5, %ymm4
	vpxor	%ymm11, %ymm4, %ymm4
	vpslld	$26, %ymm4, %ymm5
	vpsrld	$6, %ymm4, %ymm4
	vpor	%ymm5, %ymm4, %ymm4
	vpslld	$7, %ymm11, %ymm5
	vpsrld	$25, %ymm11, %ymm8
	vpor	%ymm5, %ymm8, %ymm5
	vpxor	%ymm5, %ymm4, %ymm4
	vpxor	%ymm7, %ymm9, %ymm5
	vpand	%ymm11, %ymm5, %ymm5
	vpxor	%ymm7, %ymm5, %ymm5
	vpaddd	%ymm6, %ymm5, %ymm5
	vpaddd	1760(%rsp), %ymm4, %ymm4 # 32-byte Folded Reload
	vpaddd	%ymm10, %ymm5, %ymm5
	vpaddd	%ymm4, %ymm5, %ymm5
	vpslld	$30, %ymm1, %ymm4
	vpsrld	$2, %ymm1, %ymm6
	vpor	%ymm4, %ymm6, %ymm4
	vpslld	$19, %ymm1, %ymm6
	vpsrld	$13, %ymm1, %ymm8
	vpor	%ymm6, %ymm8, %ymm6
	vpxor	%ymm6, %ymm4, %ymm4
	vpslld	$10, %ymm1, %ymm6
	vpsrld	$22, %ymm1, %ymm8
	vpor	%ymm6, %ymm8, %ymm6
	vpxor	%ymm6, %ymm4, %ymm4
	vpxor	%ymm3, %ymm0, %ymm6
	vpand	%ymm6, %ymm1, %ymm6
	vpand	%ymm3, %ymm0, %ymm8
	vpxor	%ymm8, %ymm6, %ymm6
	vpaddd	%ymm6, %ymm4, %ymm6
	vpaddd	%ymm2, %ymm5, %ymm4
	vpaddd	%ymm5, %ymm6, %ymm2
	vmovdqu	-64(%rsp), %ymm8        # 32-byte Reload
	vpslld	$21, %ymm8, %ymm5
	vpsrld	$11, %ymm8, %ymm6
	vpor	%ymm5, %ymm6, %ymm5
	vpxor	%ymm8, %ymm5, %ymm5
	vpslld	$25, %ymm5, %ymm6
	vpsrld	$7, %ymm5, %ymm5
	vpor	%ymm6, %ymm5, %ymm5
	vpsrld	$3, %ymm8, %ymm6
	vmovdqa	%ymm8, %ymm14
	vpxor	%ymm6, %ymm5, %ymm5
	vmovdqu	%ymm12, 32(%rsp)        # 32-byte Spill
	vpslld	$30, %ymm12, %ymm6
	vpsrld	$2, %ymm12, %ymm8
	vpor	%ymm6, %ymm8, %ymm6
	vpxor	%ymm12, %ymm6, %ymm6
	vpslld	$15, %ymm6, %ymm8
	vpsrld	$17, %ymm6, %ymm6
	vpor	%ymm8, %ymm6, %ymm6
	vpsrld	$10, %ymm12, %ymm8
	vpxor	%ymm8, %ymm6, %ymm6
	vpaddd	96(%rsp), %ymm13, %ymm8 # 32-byte Folded Reload
	vpaddd	%ymm5, %ymm8, %ymm5
	vpaddd	%ymm6, %ymm5, %ymm12
	vpslld	$27, %ymm4, %ymm5
	vpsrld	$5, %ymm4, %ymm6
	vpor	%ymm5, %ymm6, %ymm5
	vpxor	%ymm4, %ymm5, %ymm5
	vpslld	$26, %ymm5, %ymm6
	vpsrld	$6, %ymm5, %ymm5
	vpor	%ymm6, %ymm5, %ymm5
	vpslld	$7, %ymm4, %ymm6
	vpsrld	$25, %ymm4, %ymm8
	vpor	%ymm6, %ymm8, %ymm6
	vpxor	%ymm6, %ymm5, %ymm5
	vpxor	%ymm9, %ymm11, %ymm6
	vpand	%ymm4, %ymm6, %ymm6
	vpxor	%ymm9, %ymm6, %ymm6
	vpaddd	%ymm7, %ymm6, %ymm6
	vpaddd	1728(%rsp), %ymm5, %ymm5 # 32-byte Folded Reload
	vpaddd	%ymm12, %ymm6, %ymm6
	vpaddd	%ymm5, %ymm6, %ymm6
	vpslld	$30, %ymm2, %ymm5
	vpsrld	$2, %ymm2, %ymm7
	vpor	%ymm5, %ymm7, %ymm5
	vpslld	$19, %ymm2, %ymm7
	vpsrld	$13, %ymm2, %ymm8
	vpor	%ymm7, %ymm8, %ymm7
	vpxor	%ymm7, %ymm5, %ymm5
	vpslld	$10, %ymm2, %ymm7
	vpsrld	$22, %ymm2, %ymm8
	vpor	%ymm7, %ymm8, %ymm7
	vpxor	%ymm7, %ymm5, %ymm5
	vpxor	%ymm0, %ymm1, %ymm7
	vpand	%ymm7, %ymm2, %ymm7
	vpand	%ymm0, %ymm1, %ymm8
	vpxor	%ymm8, %ymm7, %ymm7
	vpaddd	%ymm7, %ymm5, %ymm7
	vpaddd	%ymm3, %ymm6, %ymm5
	vpaddd	%ymm6, %ymm7, %ymm3
	vmovdqu	320(%rsp), %ymm8        # 32-byte Reload
	vpslld	$21, %ymm8, %ymm6
	vpsrld	$11, %ymm8, %ymm7
	vpor	%ymm6, %ymm7, %ymm6
	vpxor	%ymm8, %ymm6, %ymm6
	vpslld	$25, %ymm6, %ymm7
	vpsrld	$7, %ymm6, %ymm6
	vpor	%ymm7, %ymm6, %ymm6
	vpsrld	$3, %ymm8, %ymm7
	vmovdqa	%ymm8, %ymm13
	vpxor	%ymm7, %ymm6, %ymm6
	vmovdqu	%ymm10, (%rsp)          # 32-byte Spill
	vpslld	$30, %ymm10, %ymm7
	vpsrld	$2, %ymm10, %ymm8
	vpor	%ymm7, %ymm8, %ymm7
	vpxor	%ymm10, %ymm7, %ymm7
	vpslld	$15, %ymm7, %ymm8
	vpsrld	$17, %ymm7, %ymm7
	vpor	%ymm8, %ymm7, %ymm7
	vpsrld	$10, %ymm10, %ymm8
	vpxor	%ymm8, %ymm7, %ymm7
	vpaddd	%ymm6, %ymm14, %ymm6
	vpaddd	-128(%rsp), %ymm6, %ymm6 # 32-byte Folded Reload
	vpaddd	%ymm7, %ymm6, %ymm10
	vpslld	$27, %ymm5, %ymm6
	vpsrld	$5, %ymm5, %ymm7
	vpor	%ymm6, %ymm7, %ymm6
	vpxor	%ymm5, %ymm6, %ymm6
	vpslld	$26, %ymm6, %ymm7
	vpsrld	$6, %ymm6, %ymm6
	vpor	%ymm7, %ymm6, %ymm6
	vpslld	$7, %ymm5, %ymm7
	vpsrld	$25, %ymm5, %ymm8
	vpor	%ymm7, %ymm8, %ymm7
	vpxor	%ymm7, %ymm6, %ymm6
	vpxor	%ymm11, %ymm4, %ymm7
	vpand	%ymm5, %ymm7, %ymm7
	vpxor	%ymm11, %ymm7, %ymm7
	vpaddd	%ymm9, %ymm7, %ymm7
	vpaddd	1696(%rsp), %ymm6, %ymm6 # 32-byte Folded Reload
	vpaddd	%ymm10, %ymm7, %ymm7
	vpaddd	%ymm6, %ymm7, %ymm7
	vpslld	$30, %ymm3, %ymm6
	vpsrld	$2, %ymm3, %ymm8
	vpor	%ymm6, %ymm8, %ymm6
	vpslld	$19, %ymm3, %ymm8
	vpsrld	$13, %ymm3, %ymm9
	vpor	%ymm8, %ymm9, %ymm8
	vpxor	%ymm8, %ymm6, %ymm6
	vpslld	$10, %ymm3, %ymm8
	vpsrld	$22, %ymm3, %ymm9
	vpor	%ymm8, %ymm9, %ymm8
	vpxor	%ymm8, %ymm6, %ymm6
	vpxor	%ymm1, %ymm2, %ymm8
	vpand	%ymm8, %ymm3, %ymm8
	vpand	%ymm1, %ymm2, %ymm9
	vpxor	%ymm9, %ymm8, %ymm8
	vpaddd	%ymm8, %ymm6, %ymm8
	vpaddd	%ymm0, %ymm7, %ymm6
	vpaddd	%ymm7, %ymm8, %ymm0
	vmovdqu	192(%rsp), %ymm14       # 32-byte Reload
	vpslld	$21, %ymm14, %ymm7
	vpsrld	$11, %ymm14, %ymm8
	vpor	%ymm7, %ymm8, %ymm7
	vpxor	%ymm14, %ymm7, %ymm7
	vpslld	$25, %ymm7, %ymm8
	vpsrld	$7, %ymm7, %ymm7
	vpor	%ymm8, %ymm7, %ymm7
	vpsrld	$3, %ymm14, %ymm8
	vpxor	%ymm8, %ymm7, %ymm7
	vmovdqu	%ymm12, -32(%rsp)       # 32-byte Spill
	vpslld	$30, %ymm12, %ymm8
	vpsrld	$2, %ymm12, %ymm9
	vpor	%ymm8, %ymm9, %ymm8
	vpxor	%ymm12, %ymm8, %ymm8
	vpslld	$15, %ymm8, %ymm9
	vpsrld	$17, %ymm8, %ymm8
	vpor	%ymm9, %ymm8, %ymm8
	vpsrld	$10, %ymm12, %ymm9
	vpxor	%ymm9, %ymm8, %ymm8
	vpaddd	%ymm7, %ymm13, %ymm7
	vpaddd	-96(%rsp), %ymm7, %ymm7 # 32-byte Folded Reload
	vpaddd	%ymm8, %ymm7, %ymm12
	vpslld	$27, %ymm6, %ymm7
	vpsrld	$5, %ymm6, %ymm8
	vpor	%ymm7, %ymm8, %ymm7
	vpxor	%ymm6, %ymm7, %ymm7
	vpslld	$26, %ymm7, %ymm8
	vpsrld	$6, %ymm7, %ymm7
	vpor	%ymm8, %ymm7, %ymm7
	vpslld	$7, %ymm6, %ymm8
	vpsrld	$25, %ymm6, %ymm9
	vpor	%ymm8, %ymm9, %ymm8
	vpxor	%ymm8, %ymm7, %ymm7
	vpxor	%ymm4, %ymm5, %ymm8
	vpand	%ymm6, %ymm8, %ymm8
	vpxor	%ymm4, %ymm8, %ymm8
	vpaddd	%ymm11, %ymm8, %ymm8
	vpaddd	1664(%rsp), %ymm7, %ymm7 # 32-byte Folded Reload
	vpaddd	%ymm12, %ymm8, %ymm8
	vpaddd	%ymm7, %ymm8, %ymm7
	vpslld	$30, %ymm0, %ymm8
	vpsrld	$2, %ymm0, %ymm9
	vpor	%ymm8, %ymm9, %ymm8
	vpslld	$19, %ymm0, %ymm9
	vpsrld	$13, %ymm0, %ymm11
	vpor	%ymm9, %ymm11, %ymm9
	vpxor	%ymm9, %ymm8, %ymm8
	vpslld	$10, %ymm0, %ymm9
	vpsrld	$22, %ymm0, %ymm11
	vpor	%ymm9, %ymm11, %ymm9
	vpxor	%ymm9, %ymm8, %ymm8
	vpxor	%ymm2, %ymm3, %ymm9
	vpand	%ymm9, %ymm0, %ymm9
	vpand	%ymm2, %ymm3, %ymm11
	vpxor	%ymm11, %ymm9, %ymm9
	vpaddd	%ymm9, %ymm8, %ymm8
	vpaddd	%ymm1, %ymm7, %ymm11
	vpaddd	%ymm7, %ymm8, %ymm1
	vmovdqu	160(%rsp), %ymm9        # 32-byte Reload
	vpslld	$21, %ymm9, %ymm7
	vpsrld	$11, %ymm9, %ymm8
	vpor	%ymm7, %ymm8, %ymm7
	vpxor	%ymm9, %ymm7, %ymm7
	vpslld	$25, %ymm7, %ymm8
	vpsrld	$7, %ymm7, %ymm7
	vpor	%ymm8, %ymm7, %ymm7
	vpsrld	$3, %ymm9, %ymm8
	vmovdqa	%ymm9, %ymm13
	vpxor	%ymm8, %ymm7, %ymm7
	vmovdqu	%ymm10, -64(%rsp)       # 32-byte Spill
	vpslld	$30, %ymm10, %ymm8
	vpsrld	$2, %ymm10, %ymm9
	vpor	%ymm8, %ymm9, %ymm8
	vpxor	%ymm10, %ymm8, %ymm8
	vpslld	$15, %ymm8, %ymm9
	vpsrld	$17, %ymm8, %ymm8
	vpor	%ymm9, %ymm8, %ymm8
	vpsrld	$10, %ymm10, %ymm9
	vpxor	%ymm9, %ymm8, %ymm8
	vpaddd	%ymm7, %ymm14, %ymm7
	vpaddd	288(%rsp), %ymm7, %ymm7 # 32-byte Folded Reload
	vpaddd	%ymm8, %ymm7, %ymm10
	vpslld	$27, %ymm11, %ymm7
	vpsrld	$5, %ymm11, %ymm8
	vpor	%ymm7, %ymm8, %ymm7
	vpxor	%ymm11, %ymm7, %ymm7
	vpslld	$26, %ymm7, %ymm8
	vpsrld	$6, %ymm7, %ymm7
	vpor	%ymm8, %ymm7, %ymm7
	vpslld	$7, %ymm11, %ymm8
	vpsrld	$25, %ymm11, %ymm9
	vpor	%ymm8, %ymm9, %ymm8
	vpxor	%ymm8, %ymm7, %ymm7
	vpxor	%ymm5, %ymm6, %ymm8
	vpand	%ymm11, %ymm8, %ymm8
	vpxor	%ymm5, %ymm8, %ymm8
	vpaddd	%ymm4, %ymm8, %ymm4
	vpaddd	1632(%rsp), %ymm7, %ymm7 # 32-byte Folded Reload
	vpaddd	%ymm10, %ymm4, %ymm4
	vpaddd	%ymm7, %ymm4, %ymm7
	vpslld	$30, %ymm1, %ymm4
	vpsrld	$2, %ymm1, %ymm8
	vpor	%ymm4, %ymm8, %ymm4
	vpslld	$19, %ymm1, %ymm8
	vpsrld	$13, %ymm1, %ymm9
	vpor	%ymm8, %ymm9, %ymm8
	vpxor	%ymm8, %ymm4, %ymm4
	vpslld	$10, %ymm1, %ymm8
	vpsrld	$22, %ymm1, %ymm9
	vpor	%ymm8, %ymm9, %ymm8
	vpxor	%ymm8, %ymm4, %ymm4
	vpxor	%ymm3, %ymm0, %ymm8
	vpand	%ymm8, %ymm1, %ymm8
	vpand	%ymm3, %ymm0, %ymm9
	vpxor	%ymm9, %ymm8, %ymm8
	vpaddd	%ymm8, %ymm4, %ymm8
	vpaddd	%ymm2, %ymm7, %ymm4
	vpaddd	%ymm7, %ymm8, %ymm2
	vmovdqu	128(%rsp), %ymm9        # 32-byte Reload
	vpslld	$21, %ymm9, %ymm7
	vpsrld	$11, %ymm9, %ymm8
	vpor	%ymm7, %ymm8, %ymm7
	vpxor	%ymm9, %ymm7, %ymm7
	vpslld	$25, %ymm7, %ymm8
	vpsrld	$7, %ymm7, %ymm7
	vpor	%ymm8, %ymm7, %ymm7
	vpsrld	$3, %ymm9, %ymm8
	vmovdqa	%ymm9, %ymm14
	vpxor	%ymm8, %ymm7, %ymm7
	vmovdqu	%ymm12, 320(%rsp)       # 32-byte Spill
	vpslld	$30, %ymm12, %ymm8
	vpsrld	$2, %ymm12, %ymm9
	vpor	%ymm8, %ymm9, %ymm8
	vpxor	%ymm12, %ymm8, %ymm8
	vpslld	$15, %ymm8, %ymm9
	vpsrld	$17, %ymm8, %ymm8
	vpor	%ymm9, %ymm8, %ymm8
	vpsrld	$10, %ymm12, %ymm9
	vpxor	%ymm9, %ymm8, %ymm8
	vpaddd	%ymm7, %ymm13, %ymm7
	vpaddd	64(%rsp), %ymm7, %ymm7  # 32-byte Folded Reload
	vpaddd	%ymm8, %ymm7, %ymm12
	vpslld	$27, %ymm4, %ymm7
	vpsrld	$5, %ymm4, %ymm8
	vpor	%ymm7, %ymm8, %ymm7
	vpxor	%ymm4, %ymm7, %ymm7
	vpslld	$26, %ymm7, %ymm8
	vpsrld	$6, %ymm7, %ymm7
	vpor	%ymm8, %ymm7, %ymm7
	vpslld	$7, %ymm4, %ymm8
	vpsrld	$25, %ymm4, %ymm9
	vpor	%ymm8, %ymm9, %ymm8
	vpxor	%ymm8, %ymm7, %ymm7
	vpxor	%ymm6, %ymm11, %ymm8
	vpand	%ymm4, %ymm8, %ymm8
	vpxor	%ymm6, %ymm8, %ymm8
	vpaddd	%ymm5, %ymm8, %ymm5
	vpaddd	1600(%rsp), %ymm7, %ymm7 # 32-byte Folded Reload
	vpaddd	%ymm12, %ymm5, %ymm5
	vpaddd	%ymm7, %ymm5, %ymm7
	vpslld	$30, %ymm2, %ymm5
	vpsrld	$2, %ymm2, %ymm8
	vpor	%ymm5, %ymm8, %ymm5
	vpslld	$19, %ymm2, %ymm8
	vpsrld	$13, %ymm2, %ymm9
	vpor	%ymm8, %ymm9, %ymm8
	vpxor	%ymm8, %ymm5, %ymm5
	vpslld	$10, %ymm2, %ymm8
	vpsrld	$22, %ymm2, %ymm9
	vpor	%ymm8, %ymm9, %ymm8
	vpxor	%ymm8, %ymm5, %ymm5
	vpxor	%ymm0, %ymm1, %ymm8
	vpand	%ymm8, %ymm2, %ymm8
	vpand	%ymm0, %ymm1, %ymm9
	vpxor	%ymm9, %ymm8, %ymm8
	vpaddd	%ymm8, %ymm5, %ymm8
	vpaddd	%ymm3, %ymm7, %ymm5
	vpaddd	%ymm7, %ymm8, %ymm3
	vmovdqu	224(%rsp), %ymm13       # 32-byte Reload
	vpslld	$21, %ymm13, %ymm7
	vpsrld	$11, %ymm13, %ymm8
	vpor	%ymm7, %ymm8, %ymm7
	vpxor	%ymm13, %ymm7, %ymm7
	vpslld	$25, %ymm7, %ymm8
	vpsrld	$7, %ymm7, %ymm7
	vpor	%ymm8, %ymm7, %ymm7
	vpsrld	$3, %ymm13, %ymm8
	vpxor	%ymm8, %ymm7, %ymm7
	vmovdqu	%ymm10, 192(%rsp)       # 32-byte Spill
	vpslld	$30, %ymm10, %ymm8
	vpsrld	$2, %ymm10, %ymm9
	vpor	%ymm8, %ymm9, %ymm8
	vpxor	%ymm10, %ymm8, %ymm8
	vpslld	$15, %ymm8, %ymm9
	vpsrld	$17, %ymm8, %ymm8
	vpor	%ymm9, %ymm8, %ymm8
	vpsrld	$10, %ymm10, %ymm9
	vpxor	%ymm9, %ymm8, %ymm8
	vpaddd	%ymm7, %ymm14, %ymm7
	vpaddd	32(%rsp), %ymm7, %ymm7  # 32-byte Folded Reload
	vpaddd	%ymm8, %ymm7, %ymm10
	vpslld	$27, %ymm5, %ymm7
	vpsrld	$5, %ymm5, %ymm8
	vpor	%ymm7, %ymm8, %ymm7
	vpxor	%ymm5, %ymm7, %ymm7
	vpslld	$26, %ymm7, %ymm8
	vpsrld	$6, %ymm7, %ymm7
	vpor	%ymm8, %ymm7, %ymm7
	vpslld	$7, %ymm5, %ymm8
	vpsrld	$25, %ymm5, %ymm9
	vpor	%ymm8, %ymm9, %ymm8
	vpxor	%ymm8, %ymm7, %ymm7
	vpxor	%ymm11, %ymm4, %ymm8
	vpand	%ymm5, %ymm8, %ymm8
	vpxor	%ymm11, %ymm8, %ymm8
	vpaddd	%ymm6, %ymm8, %ymm6
	vpaddd	1568(%rsp), %ymm7, %ymm7 # 32-byte Folded Reload
	vpaddd	%ymm10, %ymm6, %ymm6
	vpaddd	%ymm7, %ymm6, %ymm7
	vpslld	$30, %ymm3, %ymm6
	vpsrld	$2, %ymm3, %ymm8
	vpor	%ymm6, %ymm8, %ymm6
	vpslld	$19, %ymm3, %ymm8
	vpsrld	$13, %ymm3, %ymm9
	vpor	%ymm8, %ymm9, %ymm8
	vpxor	%ymm8, %ymm6, %ymm6
	vpslld	$10, %ymm3, %ymm8
	vpsrld	$22, %ymm3, %ymm9
	vpor	%ymm8, %ymm9, %ymm8
	vpxor	%ymm8, %ymm6, %ymm6
	vpxor	%ymm1, %ymm2, %ymm8
	vpand	%ymm8, %ymm3, %ymm8
	vpand	%ymm1, %ymm2, %ymm9
	vpxor	%ymm9, %ymm8, %ymm8
	vpaddd	%ymm8, %ymm6, %ymm8
	vpaddd	%ymm0, %ymm7, %ymm6
	vpaddd	%ymm7, %ymm8, %ymm0
	vmovdqu	256(%rsp), %ymm14       # 32-byte Reload
	vpslld	$21, %ymm14, %ymm7
	vpsrld	$11, %ymm14, %ymm8
	vpor	%ymm7, %ymm8, %ymm7
	vpxor	%ymm14, %ymm7, %ymm7
	vpslld	$25, %ymm7, %ymm8
	vpsrld	$7, %ymm7, %ymm7
	vpor	%ymm8, %ymm7, %ymm7
	vpsrld	$3, %ymm14, %ymm8
	vpxor	%ymm8, %ymm7, %ymm7
	vmovdqu	%ymm12, 160(%rsp)       # 32-byte Spill
	vpslld	$30, %ymm12, %ymm8
	vpsrld	$2, %ymm12, %ymm9
	vpor	%ymm8, %ymm9, %ymm8
	vpxor	%ymm12, %ymm8, %ymm8
	vpslld	$15, %ymm8, %ymm9
	vpsrld	$17, %ymm8, %ymm8
	vpor	%ymm9, %ymm8, %ymm8
	vpsrld	$10, %ymm12, %ymm9
	vpxor	%ymm9, %ymm8, %ymm8
	vpaddd	%ymm7, %ymm13, %ymm7
	vpaddd	(%rsp), %ymm7, %ymm7    # 32-byte Folded Reload
	vpaddd	%ymm8, %ymm7, %ymm12
	vpslld	$27, %ymm6, %ymm7
	vpsrld	$5, %ymm6, %ymm8
	vpor	%ymm7, %ymm8, %ymm7
	vpxor	%ymm6, %ymm7, %ymm7
	vpslld	$26, %ymm7, %ymm8
	vpsrld	$6, %ymm7, %ymm7
	vpor	%ymm8, %ymm7, %ymm7
	vpslld	$7, %ymm6, %ymm8
	vpsrld	$25, %ymm6, %ymm9
	vpor	%ymm8, %ymm9, %ymm8
	vpxor	%ymm8, %ymm7, %ymm7
	vpxor	%ymm4, %ymm5, %ymm8
	vpand	%ymm6, %ymm8, %ymm8
	vpxor	%ymm4, %ymm8, %ymm8
	vpaddd	%ymm11, %ymm8, %ymm8
	vpaddd	1536(%rsp), %ymm7, %ymm7 # 32-byte Folded Reload
	vpaddd	%ymm12, %ymm8, %ymm8
	vpaddd	%ymm7, %ymm8, %ymm8
	vpslld	$30, %ymm0, %ymm7
	vpsrld	$2, %ymm0, %ymm9
	vpor	%ymm7, %ymm9, %ymm7
	vpslld	$19, %ymm0, %ymm9
	vpsrld	$13, %ymm0, %ymm11
	vpor	%ymm9, %ymm11, %ymm9
	vpxor	%ymm9, %ymm7, %ymm7
	vpslld	$10, %ymm0, %ymm9
	vpsrld	$22, %ymm0, %ymm11
	vpor	%ymm9, %ymm11, %ymm9
	vpxor	%ymm9, %ymm7, %ymm7
	vpxor	%ymm2, %ymm3, %ymm9
	vpand	%ymm9, %ymm0, %ymm9
	vpand	%ymm2, %ymm3, %ymm11
	vpxor	%ymm11, %ymm9, %ymm9
	vpaddd	%ymm9, %ymm7, %ymm9
	vpaddd	%ymm1, %ymm8, %ymm7
	vpaddd	%ymm8, %ymm9, %ymm1
	vmovdqa	%ymm15, %ymm11
	vpslld	$21, %ymm11, %ymm8
	vpsrld	$11, %ymm11, %ymm9
	vpor	%ymm8, %ymm9, %ymm8
	vpxor	%ymm11, %ymm8, %ymm8
	vpslld	$25, %ymm8, %ymm9
	vpsrld	$7, %ymm8, %ymm8
	vpor	%ymm9, %ymm8, %ymm8
	vpsrld	$3, %ymm11, %ymm9
	vpxor	%ymm9, %ymm8, %ymm8
	vmovdqu	%ymm10, 128(%rsp)       # 32-byte Spill
	vpslld	$30, %ymm10, %ymm9
	vpsrld	$2, %ymm10, %ymm11
	vpor	%ymm9, %ymm11, %ymm9
	vpxor	%ymm10, %ymm9, %ymm9
	vpslld	$15, %ymm9, %ymm11
	vpsrld	$17, %ymm9, %ymm9
	vpor	%ymm11, %ymm9, %ymm9
	vpsrld	$10, %ymm10, %ymm11
	vpxor	%ymm11, %ymm9, %ymm9
	vpaddd	%ymm8, %ymm14, %ymm8
	vpaddd	-32(%rsp), %ymm8, %ymm8 # 32-byte Folded Reload
	vpaddd	%ymm9, %ymm8, %ymm13
	vpslld	$27, %ymm7, %ymm8
	vpsrld	$5, %ymm7, %ymm9
	vpor	%ymm8, %ymm9, %ymm8
	vpxor	%ymm7, %ymm8, %ymm8
	vpslld	$26, %ymm8, %ymm9
	vpsrld	$6, %ymm8, %ymm8
	vpor	%ymm9, %ymm8, %ymm8
	vpslld	$7, %ymm7, %ymm9
	vpsrld	$25, %ymm7, %ymm11
	vpor	%ymm9, %ymm11, %ymm9
	vpxor	%ymm9, %ymm8, %ymm8
	vpxor	%ymm5, %ymm6, %ymm9
	vpand	%ymm7, %ymm9, %ymm9
	vpxor	%ymm5, %ymm9, %ymm9
	vpaddd	%ymm4, %ymm9, %ymm4
	vpaddd	1504(%rsp), %ymm8, %ymm8 # 32-byte Folded Reload
	vpaddd	%ymm13, %ymm4, %ymm4
	vpaddd	%ymm8, %ymm4, %ymm8
	vpslld	$30, %ymm1, %ymm4
	vpsrld	$2, %ymm1, %ymm9
	vpor	%ymm4, %ymm9, %ymm4
	vpslld	$19, %ymm1, %ymm9
	vpsrld	$13, %ymm1, %ymm11
	vpor	%ymm9, %ymm11, %ymm9
	vpxor	%ymm9, %ymm4, %ymm4
	vpslld	$10, %ymm1, %ymm9
	vpsrld	$22, %ymm1, %ymm11
	vpor	%ymm9, %ymm11, %ymm9
	vpxor	%ymm9, %ymm4, %ymm4
	vpxor	%ymm3, %ymm0, %ymm9
	vpand	%ymm9, %ymm1, %ymm9
	vpand	%ymm3, %ymm0, %ymm11
	vpxor	%ymm11, %ymm9, %ymm9
	vpaddd	%ymm9, %ymm4, %ymm9
	vpaddd	%ymm2, %ymm8, %ymm4
	vpaddd	%ymm8, %ymm9, %ymm2
	vmovdqu	96(%rsp), %ymm10        # 32-byte Reload
	vpslld	$21, %ymm10, %ymm8
	vpsrld	$11, %ymm10, %ymm9
	vpor	%ymm8, %ymm9, %ymm8
	vpxor	%ymm10, %ymm8, %ymm8
	vpslld	$25, %ymm8, %ymm9
	vpsrld	$7, %ymm8, %ymm8
	vpor	%ymm9, %ymm8, %ymm8
	vpsrld	$3, %ymm10, %ymm9
	vmovdqa	%ymm10, %ymm14
	vpxor	%ymm9, %ymm8, %ymm8
	vmovdqu	%ymm12, 224(%rsp)       # 32-byte Spill
	vpslld	$30, %ymm12, %ymm9
	vpsrld	$2, %ymm12, %ymm11
	vpor	%ymm9, %ymm11, %ymm9
	vpxor	%ymm12, %ymm9, %ymm9
	vpslld	$15, %ymm9, %ymm11
	vpsrld	$17, %ymm9, %ymm9
	vpor	%ymm11, %ymm9, %ymm9
	vpsrld	$10, %ymm12, %ymm11
	vpxor	%ymm11, %ymm9, %ymm9
	vpaddd	%ymm8, %ymm15, %ymm8
	vpaddd	-64(%rsp), %ymm8, %ymm8 # 32-byte Folded Reload
	vpaddd	%ymm9, %ymm8, %ymm15
	vpslld	$27, %ymm4, %ymm9
	vpsrld	$5, %ymm4, %ymm10
	vpor	%ymm9, %ymm10, %ymm9
	vpxor	%ymm4, %ymm9, %ymm9
	vpslld	$26, %ymm9, %ymm10
	vpsrld	$6, %ymm9, %ymm9
	vpor	%ymm10, %ymm9, %ymm9
	vpslld	$7, %ymm4, %ymm10
	vpsrld	$25, %ymm4, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm10, %ymm9, %ymm9
	vpxor	%ymm6, %ymm7, %ymm10
	vpand	%ymm4, %ymm10, %ymm10
	vpxor	%ymm6, %ymm10, %ymm10
	vpaddd	%ymm5, %ymm10, %ymm5
	vpaddd	1472(%rsp), %ymm9, %ymm9 # 32-byte Folded Reload
	vpaddd	%ymm15, %ymm5, %ymm5
	vpaddd	%ymm9, %ymm5, %ymm9
	vpslld	$30, %ymm2, %ymm5
	vpsrld	$2, %ymm2, %ymm10
	vpor	%ymm5, %ymm10, %ymm5
	vpslld	$19, %ymm2, %ymm10
	vpsrld	$13, %ymm2, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm10, %ymm5, %ymm5
	vpslld	$10, %ymm2, %ymm10
	vpsrld	$22, %ymm2, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm10, %ymm5, %ymm5
	vpxor	%ymm0, %ymm1, %ymm10
	vpand	%ymm10, %ymm2, %ymm10
	vpand	%ymm0, %ymm1, %ymm11
	vpxor	%ymm11, %ymm10, %ymm10
	vpaddd	%ymm10, %ymm5, %ymm10
	vpaddd	%ymm3, %ymm9, %ymm5
	vpaddd	%ymm9, %ymm10, %ymm3
	vmovdqu	-128(%rsp), %ymm8       # 32-byte Reload
	vpslld	$21, %ymm8, %ymm9
	vpsrld	$11, %ymm8, %ymm10
	vpor	%ymm9, %ymm10, %ymm9
	vpxor	%ymm8, %ymm9, %ymm9
	vpslld	$25, %ymm9, %ymm10
	vpsrld	$7, %ymm9, %ymm9
	vpor	%ymm10, %ymm9, %ymm9
	vpsrld	$3, %ymm8, %ymm10
	vmovdqa	%ymm8, %ymm12
	vpxor	%ymm10, %ymm9, %ymm9
	vmovdqu	%ymm13, 352(%rsp)       # 32-byte Spill
	vpslld	$30, %ymm13, %ymm10
	vpsrld	$2, %ymm13, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm13, %ymm10, %ymm10
	vpslld	$15, %ymm10, %ymm11
	vpsrld	$17, %ymm10, %ymm10
	vpor	%ymm11, %ymm10, %ymm10
	vpsrld	$10, %ymm13, %ymm11
	vpxor	%ymm11, %ymm10, %ymm10
	vpaddd	%ymm9, %ymm14, %ymm9
	vpaddd	320(%rsp), %ymm9, %ymm9 # 32-byte Folded Reload
	vpaddd	%ymm10, %ymm9, %ymm14
	vpslld	$27, %ymm5, %ymm9
	vpsrld	$5, %ymm5, %ymm10
	vpor	%ymm9, %ymm10, %ymm9
	vpxor	%ymm5, %ymm9, %ymm9
	vpslld	$26, %ymm9, %ymm10
	vpsrld	$6, %ymm9, %ymm9
	vpor	%ymm10, %ymm9, %ymm9
	vpslld	$7, %ymm5, %ymm10
	vpsrld	$25, %ymm5, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm10, %ymm9, %ymm9
	vpxor	%ymm7, %ymm4, %ymm10
	vpand	%ymm5, %ymm10, %ymm10
	vpxor	%ymm7, %ymm10, %ymm10
	vpaddd	%ymm6, %ymm10, %ymm6
	vpaddd	1440(%rsp), %ymm9, %ymm9 # 32-byte Folded Reload
	vpaddd	%ymm14, %ymm6, %ymm6
	vpaddd	%ymm9, %ymm6, %ymm9
	vpslld	$30, %ymm3, %ymm6
	vpsrld	$2, %ymm3, %ymm10
	vpor	%ymm6, %ymm10, %ymm6
	vpslld	$19, %ymm3, %ymm10
	vpsrld	$13, %ymm3, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm10, %ymm6, %ymm6
	vpslld	$10, %ymm3, %ymm10
	vpsrld	$22, %ymm3, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm10, %ymm6, %ymm6
	vpxor	%ymm1, %ymm2, %ymm10
	vpand	%ymm10, %ymm3, %ymm10
	vpand	%ymm1, %ymm2, %ymm11
	vpxor	%ymm11, %ymm10, %ymm10
	vpaddd	%ymm10, %ymm6, %ymm10
	vpaddd	%ymm0, %ymm9, %ymm6
	vpaddd	%ymm9, %ymm10, %ymm0
	vmovdqu	-96(%rsp), %ymm8        # 32-byte Reload
	vpslld	$21, %ymm8, %ymm9
	vpsrld	$11, %ymm8, %ymm10
	vpor	%ymm9, %ymm10, %ymm9
	vpxor	%ymm8, %ymm9, %ymm9
	vpslld	$25, %ymm9, %ymm10
	vpsrld	$7, %ymm9, %ymm9
	vpor	%ymm10, %ymm9, %ymm9
	vpsrld	$3, %ymm8, %ymm10
	vpxor	%ymm10, %ymm9, %ymm9
	vmovdqu	%ymm15, 256(%rsp)       # 32-byte Spill
	vpslld	$30, %ymm15, %ymm10
	vpsrld	$2, %ymm15, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm15, %ymm10, %ymm10
	vpslld	$15, %ymm10, %ymm11
	vpsrld	$17, %ymm10, %ymm10
	vpor	%ymm11, %ymm10, %ymm10
	vpsrld	$10, %ymm15, %ymm11
	vpxor	%ymm11, %ymm10, %ymm10
	vpaddd	%ymm9, %ymm12, %ymm9
	vpaddd	192(%rsp), %ymm9, %ymm9 # 32-byte Folded Reload
	vpaddd	%ymm10, %ymm9, %ymm15
	vpslld	$27, %ymm6, %ymm9
	vpsrld	$5, %ymm6, %ymm10
	vpor	%ymm9, %ymm10, %ymm9
	vpxor	%ymm6, %ymm9, %ymm9
	vpslld	$26, %ymm9, %ymm10
	vpsrld	$6, %ymm9, %ymm9
	vpor	%ymm10, %ymm9, %ymm9
	vpslld	$7, %ymm6, %ymm10
	vpsrld	$25, %ymm6, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm10, %ymm9, %ymm9
	vpxor	%ymm4, %ymm5, %ymm10
	vpand	%ymm6, %ymm10, %ymm10
	vpxor	%ymm4, %ymm10, %ymm10
	vpaddd	%ymm7, %ymm10, %ymm7
	vpaddd	1408(%rsp), %ymm9, %ymm9 # 32-byte Folded Reload
	vpaddd	%ymm15, %ymm7, %ymm7
	vpaddd	%ymm9, %ymm7, %ymm9
	vpslld	$30, %ymm0, %ymm7
	vpsrld	$2, %ymm0, %ymm10
	vpor	%ymm7, %ymm10, %ymm7
	vpslld	$19, %ymm0, %ymm10
	vpsrld	$13, %ymm0, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm10, %ymm7, %ymm7
	vpslld	$10, %ymm0, %ymm10
	vpsrld	$22, %ymm0, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm10, %ymm7, %ymm7
	vpxor	%ymm2, %ymm3, %ymm10
	vpand	%ymm10, %ymm0, %ymm10
	vpand	%ymm2, %ymm3, %ymm11
	vpxor	%ymm11, %ymm10, %ymm10
	vpaddd	%ymm10, %ymm7, %ymm10
	vpaddd	%ymm1, %ymm9, %ymm7
	vpaddd	%ymm9, %ymm10, %ymm1
	vmovdqu	288(%rsp), %ymm11       # 32-byte Reload
	vpslld	$21, %ymm11, %ymm9
	vpsrld	$11, %ymm11, %ymm10
	vpor	%ymm9, %ymm10, %ymm9
	vpxor	%ymm11, %ymm9, %ymm9
	vpslld	$25, %ymm9, %ymm10
	vpsrld	$7, %ymm9, %ymm9
	vpor	%ymm10, %ymm9, %ymm9
	vpsrld	$3, %ymm11, %ymm10
	vmovdqa	%ymm11, %ymm12
	vpxor	%ymm10, %ymm9, %ymm9
	vmovdqu	%ymm14, 96(%rsp)        # 32-byte Spill
	vpslld	$30, %ymm14, %ymm10
	vpsrld	$2, %ymm14, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm14, %ymm10, %ymm10
	vpslld	$15, %ymm10, %ymm11
	vpsrld	$17, %ymm10, %ymm10
	vpor	%ymm11, %ymm10, %ymm10
	vpsrld	$10, %ymm14, %ymm11
	vpxor	%ymm11, %ymm10, %ymm10
	vpaddd	%ymm9, %ymm8, %ymm9
	vpaddd	160(%rsp), %ymm9, %ymm9 # 32-byte Folded Reload
	vpaddd	%ymm10, %ymm9, %ymm14
	vpslld	$27, %ymm7, %ymm9
	vpsrld	$5, %ymm7, %ymm10
	vpor	%ymm9, %ymm10, %ymm9
	vpxor	%ymm7, %ymm9, %ymm9
	vpslld	$26, %ymm9, %ymm10
	vpsrld	$6, %ymm9, %ymm9
	vpor	%ymm10, %ymm9, %ymm9
	vpslld	$7, %ymm7, %ymm10
	vpsrld	$25, %ymm7, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm10, %ymm9, %ymm9
	vpxor	%ymm5, %ymm6, %ymm10
	vpand	%ymm7, %ymm10, %ymm10
	vpxor	%ymm5, %ymm10, %ymm10
	vpaddd	%ymm4, %ymm10, %ymm4
	vpaddd	1376(%rsp), %ymm9, %ymm9 # 32-byte Folded Reload
	vpaddd	%ymm14, %ymm4, %ymm4
	vpaddd	%ymm9, %ymm4, %ymm9
	vpslld	$30, %ymm1, %ymm4
	vpsrld	$2, %ymm1, %ymm10
	vpor	%ymm4, %ymm10, %ymm4
	vpslld	$19, %ymm1, %ymm10
	vpsrld	$13, %ymm1, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm10, %ymm4, %ymm4
	vpslld	$10, %ymm1, %ymm10
	vpsrld	$22, %ymm1, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm10, %ymm4, %ymm4
	vpxor	%ymm3, %ymm0, %ymm10
	vpand	%ymm10, %ymm1, %ymm10
	vpand	%ymm3, %ymm0, %ymm11
	vpxor	%ymm11, %ymm10, %ymm10
	vpaddd	%ymm10, %ymm4, %ymm10
	vpaddd	%ymm2, %ymm9, %ymm4
	vpaddd	%ymm9, %ymm10, %ymm2
	vmovdqu	64(%rsp), %ymm8         # 32-byte Reload
	vpslld	$21, %ymm8, %ymm9
	vpsrld	$11, %ymm8, %ymm10
	vpor	%ymm9, %ymm10, %ymm9
	vpxor	%ymm8, %ymm9, %ymm9
	vpslld	$25, %ymm9, %ymm10
	vpsrld	$7, %ymm9, %ymm9
	vpor	%ymm10, %ymm9, %ymm9
	vpsrld	$3, %ymm8, %ymm10
	vpxor	%ymm10, %ymm9, %ymm9
	vmovdqu	%ymm15, -128(%rsp)      # 32-byte Spill
	vpslld	$30, %ymm15, %ymm10
	vpsrld	$2, %ymm15, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm15, %ymm10, %ymm10
	vpslld	$15, %ymm10, %ymm11
	vpsrld	$17, %ymm10, %ymm10
	vpor	%ymm11, %ymm10, %ymm10
	vpsrld	$10, %ymm15, %ymm11
	vpxor	%ymm11, %ymm10, %ymm10
	vpaddd	%ymm9, %ymm12, %ymm9
	vpaddd	128(%rsp), %ymm9, %ymm9 # 32-byte Folded Reload
	vpaddd	%ymm10, %ymm9, %ymm15
	vpslld	$27, %ymm4, %ymm9
	vpsrld	$5, %ymm4, %ymm10
	vpor	%ymm9, %ymm10, %ymm9
	vpxor	%ymm4, %ymm9, %ymm9
	vpslld	$26, %ymm9, %ymm10
	vpsrld	$6, %ymm9, %ymm9
	vpor	%ymm10, %ymm9, %ymm9
	vpslld	$7, %ymm4, %ymm10
	vpsrld	$25, %ymm4, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm10, %ymm9, %ymm9
	vpxor	%ymm6, %ymm7, %ymm10
	vpand	%ymm4, %ymm10, %ymm10
	vpxor	%ymm6, %ymm10, %ymm10
	vpaddd	%ymm5, %ymm10, %ymm5
	vpaddd	1344(%rsp), %ymm9, %ymm9 # 32-byte Folded Reload
	vpaddd	%ymm15, %ymm5, %ymm5
	vpaddd	%ymm9, %ymm5, %ymm9
	vpslld	$30, %ymm2, %ymm5
	vpsrld	$2, %ymm2, %ymm10
	vpor	%ymm5, %ymm10, %ymm5
	vpslld	$19, %ymm2, %ymm10
	vpsrld	$13, %ymm2, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm10, %ymm5, %ymm5
	vpslld	$10, %ymm2, %ymm10
	vpsrld	$22, %ymm2, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm10, %ymm5, %ymm5
	vpxor	%ymm0, %ymm1, %ymm10
	vpand	%ymm10, %ymm2, %ymm10
	vpand	%ymm0, %ymm1, %ymm11
	vpxor	%ymm11, %ymm10, %ymm10
	vpaddd	%ymm10, %ymm5, %ymm10
	vpaddd	%ymm3, %ymm9, %ymm5
	vpaddd	%ymm9, %ymm10, %ymm3
	vmovdqu	32(%rsp), %ymm13        # 32-byte Reload
	vpslld	$21, %ymm13, %ymm9
	vpsrld	$11, %ymm13, %ymm10
	vpor	%ymm9, %ymm10, %ymm9
	vpxor	%ymm13, %ymm9, %ymm9
	vpslld	$25, %ymm9, %ymm10
	vpsrld	$7, %ymm9, %ymm9
	vpor	%ymm10, %ymm9, %ymm9
	vpsrld	$3, %ymm13, %ymm10
	vpxor	%ymm10, %ymm9, %ymm9
	vmovdqu	%ymm14, -96(%rsp)       # 32-byte Spill
	vpslld	$30, %ymm14, %ymm10
	vpsrld	$2, %ymm14, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm14, %ymm10, %ymm10
	vpslld	$15, %ymm10, %ymm11
	vpsrld	$17, %ymm10, %ymm10
	vpor	%ymm11, %ymm10, %ymm10
	vpsrld	$10, %ymm14, %ymm11
	vpxor	%ymm11, %ymm10, %ymm10
	vpaddd	%ymm9, %ymm8, %ymm9
	vpaddd	224(%rsp), %ymm9, %ymm9 # 32-byte Folded Reload
	vpaddd	%ymm10, %ymm9, %ymm12
	vpslld	$27, %ymm5, %ymm9
	vpsrld	$5, %ymm5, %ymm10
	vpor	%ymm9, %ymm10, %ymm9
	vpxor	%ymm5, %ymm9, %ymm9
	vpslld	$26, %ymm9, %ymm10
	vpsrld	$6, %ymm9, %ymm9
	vpor	%ymm10, %ymm9, %ymm9
	vpslld	$7, %ymm5, %ymm10
	vpsrld	$25, %ymm5, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm10, %ymm9, %ymm9
	vpxor	%ymm7, %ymm4, %ymm10
	vpand	%ymm5, %ymm10, %ymm10
	vpxor	%ymm7, %ymm10, %ymm10
	vpaddd	%ymm6, %ymm10, %ymm6
	vpaddd	1312(%rsp), %ymm9, %ymm9 # 32-byte Folded Reload
	vpaddd	%ymm12, %ymm6, %ymm6
	vpaddd	%ymm9, %ymm6, %ymm9
	vpslld	$30, %ymm3, %ymm6
	vpsrld	$2, %ymm3, %ymm10
	vpor	%ymm6, %ymm10, %ymm6
	vpslld	$19, %ymm3, %ymm10
	vpsrld	$13, %ymm3, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm10, %ymm6, %ymm6
	vpslld	$10, %ymm3, %ymm10
	vpsrld	$22, %ymm3, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm10, %ymm6, %ymm6
	vpxor	%ymm1, %ymm2, %ymm10
	vpand	%ymm10, %ymm3, %ymm10
	vpand	%ymm1, %ymm2, %ymm11
	vpxor	%ymm11, %ymm10, %ymm10
	vpaddd	%ymm10, %ymm6, %ymm10
	vpaddd	%ymm0, %ymm9, %ymm6
	vpaddd	%ymm9, %ymm10, %ymm0
	vmovdqu	(%rsp), %ymm8           # 32-byte Reload
	vpslld	$21, %ymm8, %ymm9
	vpsrld	$11, %ymm8, %ymm10
	vpor	%ymm9, %ymm10, %ymm9
	vpxor	%ymm8, %ymm9, %ymm9
	vpslld	$25, %ymm9, %ymm10
	vpsrld	$7, %ymm9, %ymm9
	vpor	%ymm10, %ymm9, %ymm9
	vpsrld	$3, %ymm8, %ymm10
	vpxor	%ymm10, %ymm9, %ymm9
	vmovdqu	%ymm15, 288(%rsp)       # 32-byte Spill
	vpslld	$30, %ymm15, %ymm10
	vpsrld	$2, %ymm15, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm15, %ymm10, %ymm10
	vpslld	$15, %ymm10, %ymm11
	vpsrld	$17, %ymm10, %ymm10
	vpor	%ymm11, %ymm10, %ymm10
	vpsrld	$10, %ymm15, %ymm11
	vpxor	%ymm11, %ymm10, %ymm10
	vpaddd	%ymm9, %ymm13, %ymm9
	vpaddd	352(%rsp), %ymm9, %ymm9 # 32-byte Folded Reload
	vpaddd	%ymm10, %ymm9, %ymm13
	vpslld	$27, %ymm6, %ymm9
	vpsrld	$5, %ymm6, %ymm10
	vpor	%ymm9, %ymm10, %ymm9
	vpxor	%ymm6, %ymm9, %ymm9
	vpslld	$26, %ymm9, %ymm10
	vpsrld	$6, %ymm9, %ymm9
	vpor	%ymm10, %ymm9, %ymm9
	vpslld	$7, %ymm6, %ymm10
	vpsrld	$25, %ymm6, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm10, %ymm9, %ymm9
	vpxor	%ymm4, %ymm5, %ymm10
	vpand	%ymm6, %ymm10, %ymm10
	vpxor	%ymm4, %ymm10, %ymm10
	vpaddd	%ymm7, %ymm10, %ymm7
	vpaddd	1280(%rsp), %ymm9, %ymm9 # 32-byte Folded Reload
	vpaddd	%ymm13, %ymm7, %ymm7
	vpaddd	%ymm9, %ymm7, %ymm9
	vpslld	$30, %ymm0, %ymm7
	vpsrld	$2, %ymm0, %ymm10
	vpor	%ymm7, %ymm10, %ymm7
	vpslld	$19, %ymm0, %ymm10
	vpsrld	$13, %ymm0, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm10, %ymm7, %ymm7
	vpslld	$10, %ymm0, %ymm10
	vpsrld	$22, %ymm0, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm10, %ymm7, %ymm7
	vpxor	%ymm2, %ymm3, %ymm10
	vpand	%ymm10, %ymm0, %ymm10
	vpand	%ymm2, %ymm3, %ymm11
	vpxor	%ymm11, %ymm10, %ymm10
	vpaddd	%ymm10, %ymm7, %ymm10
	vpaddd	%ymm1, %ymm9, %ymm7
	vpaddd	%ymm9, %ymm10, %ymm1
	vmovdqu	-32(%rsp), %ymm14       # 32-byte Reload
	vpslld	$21, %ymm14, %ymm9
	vpsrld	$11, %ymm14, %ymm10
	vpor	%ymm9, %ymm10, %ymm9
	vpxor	%ymm14, %ymm9, %ymm9
	vpslld	$25, %ymm9, %ymm10
	vpsrld	$7, %ymm9, %ymm9
	vpor	%ymm10, %ymm9, %ymm9
	vpsrld	$3, %ymm14, %ymm10
	vpxor	%ymm10, %ymm9, %ymm9
	vmovdqa	%ymm12, %ymm15
	vpslld	$30, %ymm15, %ymm10
	vpsrld	$2, %ymm15, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm15, %ymm10, %ymm10
	vpslld	$15, %ymm10, %ymm11
	vpsrld	$17, %ymm10, %ymm10
	vpor	%ymm11, %ymm10, %ymm10
	vpsrld	$10, %ymm15, %ymm11
	vmovdqu	%ymm15, 384(%rsp)       # 32-byte Spill
	vpxor	%ymm11, %ymm10, %ymm10
	vpaddd	%ymm9, %ymm8, %ymm9
	vpaddd	256(%rsp), %ymm9, %ymm9 # 32-byte Folded Reload
	vpaddd	%ymm10, %ymm9, %ymm12
	vpslld	$27, %ymm7, %ymm9
	vpsrld	$5, %ymm7, %ymm10
	vpor	%ymm9, %ymm10, %ymm9
	vpxor	%ymm7, %ymm9, %ymm9
	vpslld	$26, %ymm9, %ymm10
	vpsrld	$6, %ymm9, %ymm9
	vpor	%ymm10, %ymm9, %ymm9
	vpslld	$7, %ymm7, %ymm10
	vpsrld	$25, %ymm7, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm10, %ymm9, %ymm9
	vpxor	%ymm5, %ymm6, %ymm10
	vpand	%ymm7, %ymm10, %ymm10
	vpxor	%ymm5, %ymm10, %ymm10
	vpaddd	%ymm4, %ymm10, %ymm4
	vpaddd	1248(%rsp), %ymm9, %ymm9 # 32-byte Folded Reload
	vpaddd	%ymm12, %ymm4, %ymm4
	vpaddd	%ymm9, %ymm4, %ymm9
	vpslld	$30, %ymm1, %ymm4
	vpsrld	$2, %ymm1, %ymm10
	vpor	%ymm4, %ymm10, %ymm4
	vpslld	$19, %ymm1, %ymm10
	vpsrld	$13, %ymm1, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm10, %ymm4, %ymm4
	vpslld	$10, %ymm1, %ymm10
	vpsrld	$22, %ymm1, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm10, %ymm4, %ymm4
	vpxor	%ymm3, %ymm0, %ymm10
	vpand	%ymm10, %ymm1, %ymm10
	vpand	%ymm3, %ymm0, %ymm11
	vpxor	%ymm11, %ymm10, %ymm10
	vpaddd	%ymm10, %ymm4, %ymm10
	vpaddd	%ymm2, %ymm9, %ymm4
	vpaddd	%ymm9, %ymm10, %ymm2
	vmovdqu	-64(%rsp), %ymm8        # 32-byte Reload
	vpslld	$21, %ymm8, %ymm9
	vpsrld	$11, %ymm8, %ymm10
	vpor	%ymm9, %ymm10, %ymm9
	vpxor	%ymm8, %ymm9, %ymm9
	vpslld	$25, %ymm9, %ymm10
	vpsrld	$7, %ymm9, %ymm9
	vpor	%ymm10, %ymm9, %ymm9
	vpsrld	$3, %ymm8, %ymm10
	vpxor	%ymm10, %ymm9, %ymm9
	vmovdqu	%ymm13, 64(%rsp)        # 32-byte Spill
	vpslld	$30, %ymm13, %ymm10
	vpsrld	$2, %ymm13, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm13, %ymm10, %ymm10
	vpslld	$15, %ymm10, %ymm11
	vpsrld	$17, %ymm10, %ymm10
	vpor	%ymm11, %ymm10, %ymm10
	vpsrld	$10, %ymm13, %ymm11
	vpxor	%ymm11, %ymm10, %ymm10
	vpaddd	%ymm9, %ymm14, %ymm9
	vpaddd	96(%rsp), %ymm9, %ymm9  # 32-byte Folded Reload
	vpaddd	%ymm10, %ymm9, %ymm14
	vpslld	$27, %ymm4, %ymm9
	vpsrld	$5, %ymm4, %ymm10
	vpor	%ymm9, %ymm10, %ymm9
	vpxor	%ymm4, %ymm9, %ymm9
	vpslld	$26, %ymm9, %ymm10
	vpsrld	$6, %ymm9, %ymm9
	vpor	%ymm10, %ymm9, %ymm9
	vpslld	$7, %ymm4, %ymm10
	vpsrld	$25, %ymm4, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm10, %ymm9, %ymm9
	vpxor	%ymm6, %ymm7, %ymm10
	vpand	%ymm4, %ymm10, %ymm10
	vpxor	%ymm6, %ymm10, %ymm10
	vpaddd	%ymm5, %ymm10, %ymm5
	vpaddd	1216(%rsp), %ymm9, %ymm9 # 32-byte Folded Reload
	vpaddd	%ymm14, %ymm5, %ymm5
	vpaddd	%ymm9, %ymm5, %ymm9
	vpslld	$30, %ymm2, %ymm5
	vpsrld	$2, %ymm2, %ymm10
	vpor	%ymm5, %ymm10, %ymm5
	vpslld	$19, %ymm2, %ymm10
	vpsrld	$13, %ymm2, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm10, %ymm5, %ymm5
	vpslld	$10, %ymm2, %ymm10
	vpsrld	$22, %ymm2, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm10, %ymm5, %ymm5
	vpxor	%ymm0, %ymm1, %ymm10
	vpand	%ymm10, %ymm2, %ymm10
	vpand	%ymm0, %ymm1, %ymm11
	vpxor	%ymm11, %ymm10, %ymm10
	vpaddd	%ymm10, %ymm5, %ymm10
	vpaddd	%ymm3, %ymm9, %ymm5
	vpaddd	%ymm9, %ymm10, %ymm3
	vmovdqu	320(%rsp), %ymm11       # 32-byte Reload
	vpslld	$21, %ymm11, %ymm9
	vpsrld	$11, %ymm11, %ymm10
	vpor	%ymm9, %ymm10, %ymm9
	vpxor	%ymm11, %ymm9, %ymm9
	vpslld	$25, %ymm9, %ymm10
	vpsrld	$7, %ymm9, %ymm9
	vpor	%ymm10, %ymm9, %ymm9
	vpsrld	$3, %ymm11, %ymm10
	vmovdqa	%ymm11, %ymm13
	vpxor	%ymm10, %ymm9, %ymm9
	vmovdqu	%ymm12, 32(%rsp)        # 32-byte Spill
	vpslld	$30, %ymm12, %ymm10
	vpsrld	$2, %ymm12, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm12, %ymm10, %ymm10
	vpslld	$15, %ymm10, %ymm11
	vpsrld	$17, %ymm10, %ymm10
	vpor	%ymm11, %ymm10, %ymm10
	vpsrld	$10, %ymm12, %ymm11
	vpxor	%ymm11, %ymm10, %ymm10
	vpaddd	%ymm9, %ymm8, %ymm9
	vpaddd	-128(%rsp), %ymm9, %ymm9 # 32-byte Folded Reload
	vpaddd	%ymm10, %ymm9, %ymm12
	vpslld	$27, %ymm5, %ymm9
	vpsrld	$5, %ymm5, %ymm10
	vpor	%ymm9, %ymm10, %ymm9
	vpxor	%ymm5, %ymm9, %ymm9
	vpslld	$26, %ymm9, %ymm10
	vpsrld	$6, %ymm9, %ymm9
	vpor	%ymm10, %ymm9, %ymm9
	vpslld	$7, %ymm5, %ymm10
	vpsrld	$25, %ymm5, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm10, %ymm9, %ymm9
	vpxor	%ymm7, %ymm4, %ymm10
	vpand	%ymm5, %ymm10, %ymm10
	vpxor	%ymm7, %ymm10, %ymm10
	vpaddd	%ymm6, %ymm10, %ymm6
	vpaddd	1184(%rsp), %ymm9, %ymm9 # 32-byte Folded Reload
	vpaddd	%ymm12, %ymm6, %ymm6
	vpaddd	%ymm9, %ymm6, %ymm9
	vpslld	$30, %ymm3, %ymm6
	vpsrld	$2, %ymm3, %ymm10
	vpor	%ymm6, %ymm10, %ymm6
	vpslld	$19, %ymm3, %ymm10
	vpsrld	$13, %ymm3, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm10, %ymm6, %ymm6
	vpslld	$10, %ymm3, %ymm10
	vpsrld	$22, %ymm3, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm10, %ymm6, %ymm6
	vpxor	%ymm1, %ymm2, %ymm10
	vpand	%ymm10, %ymm3, %ymm10
	vpand	%ymm1, %ymm2, %ymm11
	vpxor	%ymm11, %ymm10, %ymm10
	vpaddd	%ymm10, %ymm6, %ymm10
	vpaddd	%ymm0, %ymm9, %ymm6
	vpaddd	%ymm9, %ymm10, %ymm0
	vmovdqu	192(%rsp), %ymm8        # 32-byte Reload
	vpslld	$21, %ymm8, %ymm9
	vpsrld	$11, %ymm8, %ymm10
	vpor	%ymm9, %ymm10, %ymm9
	vpxor	%ymm8, %ymm9, %ymm9
	vpslld	$25, %ymm9, %ymm10
	vpsrld	$7, %ymm9, %ymm9
	vpor	%ymm10, %ymm9, %ymm9
	vpsrld	$3, %ymm8, %ymm10
	vpxor	%ymm10, %ymm9, %ymm9
	vmovdqu	%ymm14, (%rsp)          # 32-byte Spill
	vpslld	$30, %ymm14, %ymm10
	vpsrld	$2, %ymm14, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm14, %ymm10, %ymm10
	vpslld	$15, %ymm10, %ymm11
	vpsrld	$17, %ymm10, %ymm10
	vpor	%ymm11, %ymm10, %ymm10
	vpsrld	$10, %ymm14, %ymm11
	vpxor	%ymm11, %ymm10, %ymm10
	vpaddd	%ymm9, %ymm13, %ymm9
	vpaddd	-96(%rsp), %ymm9, %ymm9 # 32-byte Folded Reload
	vpaddd	%ymm10, %ymm9, %ymm14
	vpslld	$27, %ymm6, %ymm9
	vpsrld	$5, %ymm6, %ymm10
	vpor	%ymm9, %ymm10, %ymm9
	vpxor	%ymm6, %ymm9, %ymm9
	vpslld	$26, %ymm9, %ymm10
	vpsrld	$6, %ymm9, %ymm9
	vpor	%ymm10, %ymm9, %ymm9
	vpslld	$7, %ymm6, %ymm10
	vpsrld	$25, %ymm6, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm10, %ymm9, %ymm9
	vpxor	%ymm4, %ymm5, %ymm10
	vpand	%ymm6, %ymm10, %ymm10
	vpxor	%ymm4, %ymm10, %ymm10
	vpaddd	%ymm7, %ymm10, %ymm7
	vpaddd	1152(%rsp), %ymm9, %ymm9 # 32-byte Folded Reload
	vpaddd	%ymm14, %ymm7, %ymm7
	vpaddd	%ymm9, %ymm7, %ymm9
	vpslld	$30, %ymm0, %ymm7
	vpsrld	$2, %ymm0, %ymm10
	vpor	%ymm7, %ymm10, %ymm7
	vpslld	$19, %ymm0, %ymm10
	vpsrld	$13, %ymm0, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm10, %ymm7, %ymm7
	vpslld	$10, %ymm0, %ymm10
	vpsrld	$22, %ymm0, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm10, %ymm7, %ymm7
	vpxor	%ymm2, %ymm3, %ymm10
	vpand	%ymm10, %ymm0, %ymm10
	vpand	%ymm2, %ymm3, %ymm11
	vpxor	%ymm11, %ymm10, %ymm10
	vpaddd	%ymm10, %ymm7, %ymm10
	vpaddd	%ymm1, %ymm9, %ymm7
	vpaddd	%ymm9, %ymm10, %ymm1
	vmovdqu	160(%rsp), %ymm11       # 32-byte Reload
	vpslld	$21, %ymm11, %ymm9
	vpsrld	$11, %ymm11, %ymm10
	vpor	%ymm9, %ymm10, %ymm9
	vpxor	%ymm11, %ymm9, %ymm9
	vpslld	$25, %ymm9, %ymm10
	vpsrld	$7, %ymm9, %ymm9
	vpor	%ymm10, %ymm9, %ymm9
	vpsrld	$3, %ymm11, %ymm10
	vmovdqa	%ymm11, %ymm13
	vpxor	%ymm10, %ymm9, %ymm9
	vmovdqu	%ymm12, -32(%rsp)       # 32-byte Spill
	vpslld	$30, %ymm12, %ymm10
	vpsrld	$2, %ymm12, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm12, %ymm10, %ymm10
	vpslld	$15, %ymm10, %ymm11
	vpsrld	$17, %ymm10, %ymm10
	vpor	%ymm11, %ymm10, %ymm10
	vpsrld	$10, %ymm12, %ymm11
	vpxor	%ymm11, %ymm10, %ymm10
	vpaddd	%ymm9, %ymm8, %ymm9
	vpaddd	288(%rsp), %ymm9, %ymm9 # 32-byte Folded Reload
	vpaddd	%ymm10, %ymm9, %ymm12
	vpslld	$27, %ymm7, %ymm9
	vpsrld	$5, %ymm7, %ymm10
	vpor	%ymm9, %ymm10, %ymm9
	vpxor	%ymm7, %ymm9, %ymm9
	vpslld	$26, %ymm9, %ymm10
	vpsrld	$6, %ymm9, %ymm9
	vpor	%ymm10, %ymm9, %ymm9
	vpslld	$7, %ymm7, %ymm10
	vpsrld	$25, %ymm7, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm10, %ymm9, %ymm9
	vpxor	%ymm5, %ymm6, %ymm10
	vpand	%ymm7, %ymm10, %ymm10
	vpxor	%ymm5, %ymm10, %ymm10
	vpaddd	%ymm4, %ymm10, %ymm4
	vpaddd	1120(%rsp), %ymm9, %ymm9 # 32-byte Folded Reload
	vpaddd	%ymm12, %ymm4, %ymm4
	vpaddd	%ymm9, %ymm4, %ymm9
	vpslld	$30, %ymm1, %ymm4
	vpsrld	$2, %ymm1, %ymm10
	vpor	%ymm4, %ymm10, %ymm4
	vpslld	$19, %ymm1, %ymm10
	vpsrld	$13, %ymm1, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm10, %ymm4, %ymm4
	vpslld	$10, %ymm1, %ymm10
	vpsrld	$22, %ymm1, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm10, %ymm4, %ymm4
	vpxor	%ymm3, %ymm0, %ymm10
	vpand	%ymm10, %ymm1, %ymm10
	vpand	%ymm3, %ymm0, %ymm11
	vpxor	%ymm11, %ymm10, %ymm10
	vpaddd	%ymm10, %ymm4, %ymm10
	vpaddd	%ymm2, %ymm9, %ymm4
	vpaddd	%ymm9, %ymm10, %ymm2
	vmovdqu	128(%rsp), %ymm8        # 32-byte Reload
	vpslld	$21, %ymm8, %ymm9
	vpsrld	$11, %ymm8, %ymm10
	vpor	%ymm9, %ymm10, %ymm9
	vpxor	%ymm8, %ymm9, %ymm9
	vpslld	$25, %ymm9, %ymm10
	vpsrld	$7, %ymm9, %ymm9
	vpor	%ymm10, %ymm9, %ymm9
	vpsrld	$3, %ymm8, %ymm10
	vpxor	%ymm10, %ymm9, %ymm9
	vmovdqu	%ymm14, -64(%rsp)       # 32-byte Spill
	vpslld	$30, %ymm14, %ymm10
	vpsrld	$2, %ymm14, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm14, %ymm10, %ymm10
	vpslld	$15, %ymm10, %ymm11
	vpsrld	$17, %ymm10, %ymm10
	vpor	%ymm11, %ymm10, %ymm10
	vpsrld	$10, %ymm14, %ymm11
	vpxor	%ymm11, %ymm10, %ymm10
	vpaddd	%ymm9, %ymm13, %ymm9
	vpaddd	%ymm9, %ymm15, %ymm9
	vpaddd	%ymm10, %ymm9, %ymm14
	vpslld	$27, %ymm4, %ymm9
	vpsrld	$5, %ymm4, %ymm10
	vpor	%ymm9, %ymm10, %ymm9
	vpxor	%ymm4, %ymm9, %ymm9
	vpslld	$26, %ymm9, %ymm10
	vpsrld	$6, %ymm9, %ymm9
	vpor	%ymm10, %ymm9, %ymm9
	vpslld	$7, %ymm4, %ymm10
	vpsrld	$25, %ymm4, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm10, %ymm9, %ymm9
	vpxor	%ymm6, %ymm7, %ymm10
	vpand	%ymm4, %ymm10, %ymm10
	vpxor	%ymm6, %ymm10, %ymm10
	vpaddd	%ymm5, %ymm10, %ymm5
	vpaddd	1088(%rsp), %ymm9, %ymm9 # 32-byte Folded Reload
	vpaddd	%ymm14, %ymm5, %ymm5
	vpaddd	%ymm9, %ymm5, %ymm9
	vpslld	$30, %ymm2, %ymm5
	vpsrld	$2, %ymm2, %ymm10
	vpor	%ymm5, %ymm10, %ymm5
	vpslld	$19, %ymm2, %ymm10
	vpsrld	$13, %ymm2, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm10, %ymm5, %ymm5
	vpslld	$10, %ymm2, %ymm10
	vpsrld	$22, %ymm2, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm10, %ymm5, %ymm5
	vpxor	%ymm0, %ymm1, %ymm10
	vpand	%ymm10, %ymm2, %ymm10
	vpand	%ymm0, %ymm1, %ymm11
	vpxor	%ymm11, %ymm10, %ymm10
	vpaddd	%ymm10, %ymm5, %ymm10
	vpaddd	%ymm3, %ymm9, %ymm5
	vpaddd	%ymm9, %ymm10, %ymm3
	vmovdqu	224(%rsp), %ymm11       # 32-byte Reload
	vpslld	$21, %ymm11, %ymm9
	vpsrld	$11, %ymm11, %ymm10
	vpor	%ymm9, %ymm10, %ymm9
	vpxor	%ymm11, %ymm9, %ymm9
	vpslld	$25, %ymm9, %ymm10
	vpsrld	$7, %ymm9, %ymm9
	vpor	%ymm10, %ymm9, %ymm9
	vpsrld	$3, %ymm11, %ymm10
	vmovdqa	%ymm11, %ymm13
	vpxor	%ymm10, %ymm9, %ymm9
	vmovdqu	%ymm12, 320(%rsp)       # 32-byte Spill
	vpslld	$30, %ymm12, %ymm10
	vpsrld	$2, %ymm12, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm12, %ymm10, %ymm10
	vpslld	$15, %ymm10, %ymm11
	vpsrld	$17, %ymm10, %ymm10
	vpor	%ymm11, %ymm10, %ymm10
	vpsrld	$10, %ymm12, %ymm11
	vpxor	%ymm11, %ymm10, %ymm10
	vpaddd	%ymm9, %ymm8, %ymm9
	vpaddd	64(%rsp), %ymm9, %ymm9  # 32-byte Folded Reload
	vpaddd	%ymm10, %ymm9, %ymm12
	vpslld	$27, %ymm5, %ymm9
	vpsrld	$5, %ymm5, %ymm10
	vpor	%ymm9, %ymm10, %ymm9
	vpxor	%ymm5, %ymm9, %ymm9
	vpslld	$26, %ymm9, %ymm10
	vpsrld	$6, %ymm9, %ymm9
	vpor	%ymm10, %ymm9, %ymm9
	vpslld	$7, %ymm5, %ymm10
	vpsrld	$25, %ymm5, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm10, %ymm9, %ymm9
	vpxor	%ymm7, %ymm4, %ymm10
	vpand	%ymm5, %ymm10, %ymm10
	vpxor	%ymm7, %ymm10, %ymm10
	vpaddd	%ymm6, %ymm10, %ymm6
	vpaddd	1056(%rsp), %ymm9, %ymm9 # 32-byte Folded Reload
	vpaddd	%ymm12, %ymm6, %ymm6
	vpaddd	%ymm9, %ymm6, %ymm9
	vpslld	$30, %ymm3, %ymm6
	vpsrld	$2, %ymm3, %ymm10
	vpor	%ymm6, %ymm10, %ymm6
	vpslld	$19, %ymm3, %ymm10
	vpsrld	$13, %ymm3, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm10, %ymm6, %ymm6
	vpslld	$10, %ymm3, %ymm10
	vpsrld	$22, %ymm3, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm10, %ymm6, %ymm6
	vpxor	%ymm1, %ymm2, %ymm10
	vpand	%ymm10, %ymm3, %ymm10
	vpand	%ymm1, %ymm2, %ymm11
	vpxor	%ymm11, %ymm10, %ymm10
	vpaddd	%ymm10, %ymm6, %ymm10
	vpaddd	%ymm0, %ymm9, %ymm6
	vpaddd	%ymm9, %ymm10, %ymm0
	vmovdqu	352(%rsp), %ymm8        # 32-byte Reload
	vpslld	$21, %ymm8, %ymm9
	vpsrld	$11, %ymm8, %ymm10
	vpor	%ymm9, %ymm10, %ymm9
	vpxor	%ymm8, %ymm9, %ymm9
	vpslld	$25, %ymm9, %ymm10
	vpsrld	$7, %ymm9, %ymm9
	vpor	%ymm10, %ymm9, %ymm9
	vpsrld	$3, %ymm8, %ymm10
	vpxor	%ymm10, %ymm9, %ymm9
	vmovdqu	%ymm14, 192(%rsp)       # 32-byte Spill
	vpslld	$30, %ymm14, %ymm10
	vpsrld	$2, %ymm14, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm14, %ymm10, %ymm10
	vpslld	$15, %ymm10, %ymm11
	vpsrld	$17, %ymm10, %ymm10
	vpor	%ymm11, %ymm10, %ymm10
	vpsrld	$10, %ymm14, %ymm11
	vpxor	%ymm11, %ymm10, %ymm10
	vpaddd	%ymm9, %ymm13, %ymm9
	vpaddd	32(%rsp), %ymm9, %ymm9  # 32-byte Folded Reload
	vpaddd	%ymm10, %ymm9, %ymm13
	vpslld	$27, %ymm6, %ymm9
	vpsrld	$5, %ymm6, %ymm10
	vpor	%ymm9, %ymm10, %ymm9
	vpxor	%ymm6, %ymm9, %ymm9
	vpslld	$26, %ymm9, %ymm10
	vpsrld	$6, %ymm9, %ymm9
	vpor	%ymm10, %ymm9, %ymm9
	vpslld	$7, %ymm6, %ymm10
	vpsrld	$25, %ymm6, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm10, %ymm9, %ymm9
	vpxor	%ymm4, %ymm5, %ymm10
	vpand	%ymm6, %ymm10, %ymm10
	vpxor	%ymm4, %ymm10, %ymm10
	vpaddd	%ymm7, %ymm10, %ymm7
	vpaddd	1024(%rsp), %ymm9, %ymm9 # 32-byte Folded Reload
	vpaddd	%ymm13, %ymm7, %ymm7
	vpaddd	%ymm9, %ymm7, %ymm9
	vpslld	$30, %ymm0, %ymm7
	vpsrld	$2, %ymm0, %ymm10
	vpor	%ymm7, %ymm10, %ymm7
	vpslld	$19, %ymm0, %ymm10
	vpsrld	$13, %ymm0, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm10, %ymm7, %ymm7
	vpslld	$10, %ymm0, %ymm10
	vpsrld	$22, %ymm0, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm10, %ymm7, %ymm7
	vpxor	%ymm2, %ymm3, %ymm10
	vpand	%ymm10, %ymm0, %ymm10
	vpand	%ymm2, %ymm3, %ymm11
	vpxor	%ymm11, %ymm10, %ymm10
	vpaddd	%ymm10, %ymm7, %ymm10
	vpaddd	%ymm1, %ymm9, %ymm7
	vpaddd	%ymm9, %ymm10, %ymm1
	vmovdqu	256(%rsp), %ymm11       # 32-byte Reload
	vpslld	$21, %ymm11, %ymm9
	vpsrld	$11, %ymm11, %ymm10
	vpor	%ymm9, %ymm10, %ymm9
	vpxor	%ymm11, %ymm9, %ymm9
	vpslld	$25, %ymm9, %ymm10
	vpsrld	$7, %ymm9, %ymm9
	vpor	%ymm10, %ymm9, %ymm9
	vpsrld	$3, %ymm11, %ymm10
	vmovdqa	%ymm11, %ymm14
	vpxor	%ymm10, %ymm9, %ymm9
	vmovdqu	%ymm12, 160(%rsp)       # 32-byte Spill
	vpslld	$30, %ymm12, %ymm10
	vpsrld	$2, %ymm12, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm12, %ymm10, %ymm10
	vpslld	$15, %ymm10, %ymm11
	vpsrld	$17, %ymm10, %ymm10
	vpor	%ymm11, %ymm10, %ymm10
	vpsrld	$10, %ymm12, %ymm11
	vpxor	%ymm11, %ymm10, %ymm10
	vpaddd	%ymm9, %ymm8, %ymm9
	vpaddd	(%rsp), %ymm9, %ymm9    # 32-byte Folded Reload
	vpaddd	%ymm10, %ymm9, %ymm12
	vpslld	$27, %ymm7, %ymm9
	vpsrld	$5, %ymm7, %ymm10
	vpor	%ymm9, %ymm10, %ymm9
	vpxor	%ymm7, %ymm9, %ymm9
	vpslld	$26, %ymm9, %ymm10
	vpsrld	$6, %ymm9, %ymm9
	vpor	%ymm10, %ymm9, %ymm9
	vpslld	$7, %ymm7, %ymm10
	vpsrld	$25, %ymm7, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm10, %ymm9, %ymm9
	vpxor	%ymm5, %ymm6, %ymm10
	vpand	%ymm7, %ymm10, %ymm10
	vpxor	%ymm5, %ymm10, %ymm10
	vpaddd	%ymm4, %ymm10, %ymm4
	vpaddd	992(%rsp), %ymm9, %ymm9 # 32-byte Folded Reload
	vpaddd	%ymm12, %ymm4, %ymm4
	vpaddd	%ymm9, %ymm4, %ymm9
	vpslld	$30, %ymm1, %ymm4
	vpsrld	$2, %ymm1, %ymm10
	vpor	%ymm4, %ymm10, %ymm4
	vpslld	$19, %ymm1, %ymm10
	vpsrld	$13, %ymm1, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm10, %ymm4, %ymm4
	vpslld	$10, %ymm1, %ymm10
	vpsrld	$22, %ymm1, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm10, %ymm4, %ymm4
	vpxor	%ymm3, %ymm0, %ymm10
	vpand	%ymm10, %ymm1, %ymm10
	vpand	%ymm3, %ymm0, %ymm11
	vpxor	%ymm11, %ymm10, %ymm10
	vpaddd	%ymm10, %ymm4, %ymm10
	vpaddd	%ymm2, %ymm9, %ymm4
	vpaddd	%ymm9, %ymm10, %ymm2
	vmovdqu	96(%rsp), %ymm15        # 32-byte Reload
	vpslld	$21, %ymm15, %ymm9
	vpsrld	$11, %ymm15, %ymm10
	vpor	%ymm9, %ymm10, %ymm9
	vpxor	%ymm15, %ymm9, %ymm9
	vpslld	$25, %ymm9, %ymm10
	vpsrld	$7, %ymm9, %ymm9
	vpor	%ymm10, %ymm9, %ymm9
	vpsrld	$3, %ymm15, %ymm10
	vpxor	%ymm10, %ymm9, %ymm9
	vmovdqu	%ymm13, 224(%rsp)       # 32-byte Spill
	vpslld	$30, %ymm13, %ymm10
	vpsrld	$2, %ymm13, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vpxor	%ymm13, %ymm10, %ymm10
	vpslld	$15, %ymm10, %ymm11
	vpsrld	$17, %ymm10, %ymm10
	vpor	%ymm11, %ymm10, %ymm10
	vpsrld	$10, %ymm13, %ymm11
	vpxor	%ymm11, %ymm10, %ymm10
	vpaddd	%ymm9, %ymm14, %ymm8
	vpaddd	-32(%rsp), %ymm8, %ymm8 # 32-byte Folded Reload
	vpaddd	%ymm10, %ymm8, %ymm13
	vpslld	$27, %ymm4, %ymm8
	vpsrld	$5, %ymm4, %ymm9
	vpor	%ymm8, %ymm9, %ymm8
	vpxor	%ymm4, %ymm8, %ymm8
	vpslld	$26, %ymm8, %ymm9
	vpsrld	$6, %ymm8, %ymm8
	vpor	%ymm9, %ymm8, %ymm8
	vpslld	$7, %ymm4, %ymm9
	vpsrld	$25, %ymm4, %ymm10
	vpor	%ymm9, %ymm10, %ymm9
	vpxor	%ymm9, %ymm8, %ymm8
	vpxor	%ymm6, %ymm7, %ymm9
	vpand	%ymm4, %ymm9, %ymm9
	vpxor	%ymm6, %ymm9, %ymm9
	vpaddd	%ymm5, %ymm9, %ymm5
	vpaddd	960(%rsp), %ymm8, %ymm8 # 32-byte Folded Reload
	vpaddd	%ymm13, %ymm5, %ymm5
	vpaddd	%ymm8, %ymm5, %ymm8
	vpslld	$30, %ymm2, %ymm5
	vpsrld	$2, %ymm2, %ymm9
	vpor	%ymm5, %ymm9, %ymm5
	vpslld	$19, %ymm2, %ymm9
	vpsrld	$13, %ymm2, %ymm10
	vpor	%ymm9, %ymm10, %ymm9
	vpxor	%ymm9, %ymm5, %ymm5
	vpslld	$10, %ymm2, %ymm9
	vpsrld	$22, %ymm2, %ymm10
	vpor	%ymm9, %ymm10, %ymm9
	vpxor	%ymm9, %ymm5, %ymm5
	vpxor	%ymm0, %ymm1, %ymm9
	vpand	%ymm9, %ymm2, %ymm9
	vpand	%ymm0, %ymm1, %ymm10
	vpxor	%ymm10, %ymm9, %ymm9
	vpaddd	%ymm9, %ymm5, %ymm9
	vpaddd	%ymm3, %ymm8, %ymm5
	vpaddd	%ymm8, %ymm9, %ymm3
	vmovdqu	-128(%rsp), %ymm10      # 32-byte Reload
	vpslld	$21, %ymm10, %ymm8
	vpsrld	$11, %ymm10, %ymm9
	vpor	%ymm8, %ymm9, %ymm8
	vpxor	%ymm10, %ymm8, %ymm8
	vpslld	$25, %ymm8, %ymm9
	vpsrld	$7, %ymm8, %ymm8
	vpor	%ymm9, %ymm8, %ymm8
	vpsrld	$3, %ymm10, %ymm9
	vmovdqa	%ymm10, %ymm11
	vpxor	%ymm9, %ymm8, %ymm8
	vmovdqu	%ymm12, 128(%rsp)       # 32-byte Spill
	vpslld	$30, %ymm12, %ymm9
	vpsrld	$2, %ymm12, %ymm10
	vpor	%ymm9, %ymm10, %ymm9
	vpxor	%ymm12, %ymm9, %ymm9
	vpslld	$15, %ymm9, %ymm10
	vpsrld	$17, %ymm9, %ymm9
	vpor	%ymm10, %ymm9, %ymm9
	vpsrld	$10, %ymm12, %ymm10
	vpxor	%ymm10, %ymm9, %ymm9
	vpaddd	%ymm8, %ymm15, %ymm8
	vpaddd	-64(%rsp), %ymm8, %ymm8 # 32-byte Folded Reload
	vpaddd	%ymm9, %ymm8, %ymm14
	vpslld	$27, %ymm5, %ymm8
	vpsrld	$5, %ymm5, %ymm9
	vpor	%ymm8, %ymm9, %ymm8
	vpxor	%ymm5, %ymm8, %ymm8
	vpslld	$26, %ymm8, %ymm9
	vpsrld	$6, %ymm8, %ymm8
	vpor	%ymm9, %ymm8, %ymm8
	vpslld	$7, %ymm5, %ymm9
	vpsrld	$25, %ymm5, %ymm10
	vpor	%ymm9, %ymm10, %ymm9
	vpxor	%ymm9, %ymm8, %ymm8
	vpxor	%ymm7, %ymm4, %ymm9
	vpand	%ymm5, %ymm9, %ymm9
	vpxor	%ymm7, %ymm9, %ymm9
	vpaddd	%ymm6, %ymm9, %ymm6
	vpaddd	928(%rsp), %ymm8, %ymm8 # 32-byte Folded Reload
	vpaddd	%ymm14, %ymm6, %ymm6
	vpaddd	%ymm8, %ymm6, %ymm6
	vpslld	$30, %ymm3, %ymm8
	vpsrld	$2, %ymm3, %ymm9
	vpor	%ymm8, %ymm9, %ymm8
	vpslld	$19, %ymm3, %ymm9
	vpsrld	$13, %ymm3, %ymm10
	vpor	%ymm9, %ymm10, %ymm9
	vpxor	%ymm9, %ymm8, %ymm8
	vpslld	$10, %ymm3, %ymm9
	vpsrld	$22, %ymm3, %ymm10
	vpor	%ymm9, %ymm10, %ymm9
	vpxor	%ymm9, %ymm8, %ymm8
	vpxor	%ymm1, %ymm2, %ymm9
	vpand	%ymm9, %ymm3, %ymm9
	vpand	%ymm1, %ymm2, %ymm10
	vpxor	%ymm10, %ymm9, %ymm9
	vpaddd	%ymm9, %ymm8, %ymm9
	vpaddd	%ymm0, %ymm6, %ymm8
	vpaddd	%ymm6, %ymm9, %ymm0
	vmovdqu	-96(%rsp), %ymm10       # 32-byte Reload
	vpslld	$21, %ymm10, %ymm6
	vpsrld	$11, %ymm10, %ymm9
	vpor	%ymm6, %ymm9, %ymm6
	vpxor	%ymm10, %ymm6, %ymm6
	vpslld	$25, %ymm6, %ymm9
	vpsrld	$7, %ymm6, %ymm6
	vpor	%ymm9, %ymm6, %ymm6
	vpsrld	$3, %ymm10, %ymm9
	vmovdqa	%ymm10, %ymm12
	vpxor	%ymm9, %ymm6, %ymm6
	vmovdqu	%ymm13, 256(%rsp)       # 32-byte Spill
	vpslld	$30, %ymm13, %ymm9
	vpsrld	$2, %ymm13, %ymm10
	vpor	%ymm9, %ymm10, %ymm9
	vpxor	%ymm13, %ymm9, %ymm9
	vpslld	$15, %ymm9, %ymm10
	vpsrld	$17, %ymm9, %ymm9
	vpor	%ymm10, %ymm9, %ymm9
	vpsrld	$10, %ymm13, %ymm10
	vpxor	%ymm10, %ymm9, %ymm9
	vpaddd	%ymm6, %ymm11, %ymm6
	vpaddd	320(%rsp), %ymm6, %ymm6 # 32-byte Folded Reload
	vpaddd	%ymm9, %ymm6, %ymm15
	vpslld	$27, %ymm8, %ymm6
	vpsrld	$5, %ymm8, %ymm9
	vpor	%ymm6, %ymm9, %ymm6
	vpxor	%ymm8, %ymm6, %ymm6
	vpslld	$26, %ymm6, %ymm9
	vpsrld	$6, %ymm6, %ymm6
	vpor	%ymm9, %ymm6, %ymm6
	vpslld	$7, %ymm8, %ymm9
	vpsrld	$25, %ymm8, %ymm10
	vpor	%ymm9, %ymm10, %ymm9
	vpxor	%ymm9, %ymm6, %ymm6
	vpxor	%ymm4, %ymm5, %ymm9
	vpand	%ymm8, %ymm9, %ymm9
	vpxor	%ymm4, %ymm9, %ymm9
	vpaddd	%ymm7, %ymm9, %ymm7
	vpaddd	896(%rsp), %ymm6, %ymm6 # 32-byte Folded Reload
	vpaddd	%ymm15, %ymm7, %ymm7
	vpaddd	%ymm6, %ymm7, %ymm6
	vpslld	$30, %ymm0, %ymm7
	vpsrld	$2, %ymm0, %ymm9
	vpor	%ymm7, %ymm9, %ymm7
	vpslld	$19, %ymm0, %ymm9
	vpsrld	$13, %ymm0, %ymm10
	vpor	%ymm9, %ymm10, %ymm9
	vpxor	%ymm9, %ymm7, %ymm7
	vpslld	$10, %ymm0, %ymm9
	vpsrld	$22, %ymm0, %ymm10
	vpor	%ymm9, %ymm10, %ymm9
	vpxor	%ymm9, %ymm7, %ymm7
	vpxor	%ymm2, %ymm3, %ymm9
	vpand	%ymm9, %ymm0, %ymm9
	vpand	%ymm2, %ymm3, %ymm10
	vpxor	%ymm10, %ymm9, %ymm9
	vpaddd	%ymm9, %ymm7, %ymm7
	vpaddd	%ymm1, %ymm6, %ymm9
	vpaddd	%ymm6, %ymm7, %ymm1
	vmovdqu	288(%rsp), %ymm10       # 32-byte Reload
	vpslld	$21, %ymm10, %ymm6
	vpsrld	$11, %ymm10, %ymm7
	vpor	%ymm6, %ymm7, %ymm6
	vpxor	%ymm10, %ymm6, %ymm6
	vpslld	$25, %ymm6, %ymm7
	vpsrld	$7, %ymm6, %ymm6
	vpor	%ymm7, %ymm6, %ymm6
	vpsrld	$3, %ymm10, %ymm7
	vmovdqa	%ymm10, %ymm13
	vpxor	%ymm7, %ymm6, %ymm6
	vmovdqu	%ymm14, 96(%rsp)        # 32-byte Spill
	vpslld	$30, %ymm14, %ymm7
	vpsrld	$2, %ymm14, %ymm10
	vpor	%ymm7, %ymm10, %ymm7
	vpxor	%ymm14, %ymm7, %ymm7
	vpslld	$15, %ymm7, %ymm10
	vpsrld	$17, %ymm7, %ymm7
	vpor	%ymm10, %ymm7, %ymm7
	vpsrld	$10, %ymm14, %ymm10
	vpxor	%ymm10, %ymm7, %ymm7
	vpaddd	%ymm6, %ymm12, %ymm6
	vpaddd	192(%rsp), %ymm6, %ymm6 # 32-byte Folded Reload
	vpaddd	%ymm7, %ymm6, %ymm14
	vpslld	$27, %ymm9, %ymm6
	vpsrld	$5, %ymm9, %ymm7
	vpor	%ymm6, %ymm7, %ymm6
	vpxor	%ymm9, %ymm6, %ymm6
	vpslld	$26, %ymm6, %ymm7
	vpsrld	$6, %ymm6, %ymm6
	vpor	%ymm7, %ymm6, %ymm6
	vpslld	$7, %ymm9, %ymm7
	vpsrld	$25, %ymm9, %ymm10
	vpor	%ymm7, %ymm10, %ymm7
	vpxor	%ymm7, %ymm6, %ymm6
	vpxor	%ymm5, %ymm8, %ymm7
	vpand	%ymm9, %ymm7, %ymm7
	vpxor	%ymm5, %ymm7, %ymm7
	vpaddd	%ymm4, %ymm7, %ymm4
	vpaddd	864(%rsp), %ymm6, %ymm6 # 32-byte Folded Reload
	vpaddd	%ymm14, %ymm4, %ymm4
	vpaddd	%ymm6, %ymm4, %ymm6
	vpslld	$30, %ymm1, %ymm4
	vpsrld	$2, %ymm1, %ymm7
	vpor	%ymm4, %ymm7, %ymm4
	vpslld	$19, %ymm1, %ymm7
	vpsrld	$13, %ymm1, %ymm10
	vpor	%ymm7, %ymm10, %ymm7
	vpxor	%ymm7, %ymm4, %ymm4
	vpslld	$10, %ymm1, %ymm7
	vpsrld	$22, %ymm1, %ymm10
	vpor	%ymm7, %ymm10, %ymm7
	vpxor	%ymm7, %ymm4, %ymm4
	vpxor	%ymm3, %ymm0, %ymm7
	vpand	%ymm7, %ymm1, %ymm7
	vpand	%ymm3, %ymm0, %ymm10
	vpxor	%ymm10, %ymm7, %ymm7
	vpaddd	%ymm7, %ymm4, %ymm7
	vpaddd	%ymm2, %ymm6, %ymm4
	vpaddd	%ymm6, %ymm7, %ymm2
	vmovdqu	384(%rsp), %ymm10       # 32-byte Reload
	vpslld	$21, %ymm10, %ymm6
	vpsrld	$11, %ymm10, %ymm7
	vpor	%ymm6, %ymm7, %ymm6
	vpxor	%ymm10, %ymm6, %ymm6
	vpslld	$25, %ymm6, %ymm7
	vpsrld	$7, %ymm6, %ymm6
	vpor	%ymm7, %ymm6, %ymm6
	vpsrld	$3, %ymm10, %ymm7
	vmovdqa	%ymm10, %ymm11
	vpxor	%ymm7, %ymm6, %ymm6
	vmovdqu	%ymm15, -128(%rsp)      # 32-byte Spill
	vpslld	$30, %ymm15, %ymm7
	vpsrld	$2, %ymm15, %ymm10
	vpor	%ymm7, %ymm10, %ymm7
	vpxor	%ymm15, %ymm7, %ymm7
	vpslld	$15, %ymm7, %ymm10
	vpsrld	$17, %ymm7, %ymm7
	vpor	%ymm10, %ymm7, %ymm7
	vpsrld	$10, %ymm15, %ymm10
	vpxor	%ymm10, %ymm7, %ymm7
	vpaddd	%ymm6, %ymm13, %ymm6
	vpaddd	160(%rsp), %ymm6, %ymm6 # 32-byte Folded Reload
	vpaddd	%ymm7, %ymm6, %ymm15
	vpslld	$27, %ymm4, %ymm6
	vpsrld	$5, %ymm4, %ymm7
	vpor	%ymm6, %ymm7, %ymm6
	vpxor	%ymm4, %ymm6, %ymm6
	vpslld	$26, %ymm6, %ymm7
	vpsrld	$6, %ymm6, %ymm6
	vpor	%ymm7, %ymm6, %ymm6
	vpslld	$7, %ymm4, %ymm7
	vpsrld	$25, %ymm4, %ymm10
	vpor	%ymm7, %ymm10, %ymm7
	vpxor	%ymm7, %ymm6, %ymm6
	vpxor	%ymm8, %ymm9, %ymm7
	vpand	%ymm4, %ymm7, %ymm7
	vpxor	%ymm8, %ymm7, %ymm7
	vpaddd	%ymm5, %ymm7, %ymm5
	vpaddd	832(%rsp), %ymm6, %ymm6 # 32-byte Folded Reload
	vpaddd	%ymm15, %ymm5, %ymm5
	vpaddd	%ymm6, %ymm5, %ymm6
	vpslld	$30, %ymm2, %ymm5
	vpsrld	$2, %ymm2, %ymm7
	vpor	%ymm5, %ymm7, %ymm5
	vpslld	$19, %ymm2, %ymm7
	vpsrld	$13, %ymm2, %ymm10
	vpor	%ymm7, %ymm10, %ymm7
	vpxor	%ymm7, %ymm5, %ymm5
	vpslld	$10, %ymm2, %ymm7
	vpsrld	$22, %ymm2, %ymm10
	vpor	%ymm7, %ymm10, %ymm7
	vpxor	%ymm7, %ymm5, %ymm5
	vpxor	%ymm0, %ymm1, %ymm7
	vpand	%ymm7, %ymm2, %ymm7
	vpand	%ymm0, %ymm1, %ymm10
	vpxor	%ymm10, %ymm7, %ymm7
	vpaddd	%ymm7, %ymm5, %ymm7
	vpaddd	%ymm3, %ymm6, %ymm5
	vpaddd	%ymm6, %ymm7, %ymm3
	vmovdqu	64(%rsp), %ymm10        # 32-byte Reload
	vpslld	$21, %ymm10, %ymm6
	vpsrld	$11, %ymm10, %ymm7
	vpor	%ymm6, %ymm7, %ymm6
	vpxor	%ymm10, %ymm6, %ymm6
	vpslld	$25, %ymm6, %ymm7
	vpsrld	$7, %ymm6, %ymm6
	vpor	%ymm7, %ymm6, %ymm6
	vpsrld	$3, %ymm10, %ymm7
	vmovdqa	%ymm10, %ymm12
	vpxor	%ymm7, %ymm6, %ymm6
	vmovdqu	%ymm14, -96(%rsp)       # 32-byte Spill
	vpslld	$30, %ymm14, %ymm7
	vpsrld	$2, %ymm14, %ymm10
	vpor	%ymm7, %ymm10, %ymm7
	vpxor	%ymm14, %ymm7, %ymm7
	vpslld	$15, %ymm7, %ymm10
	vpsrld	$17, %ymm7, %ymm7
	vpor	%ymm10, %ymm7, %ymm7
	vpsrld	$10, %ymm14, %ymm10
	vpxor	%ymm10, %ymm7, %ymm7
	vpaddd	%ymm6, %ymm11, %ymm6
	vpaddd	224(%rsp), %ymm6, %ymm6 # 32-byte Folded Reload
	vpaddd	%ymm7, %ymm6, %ymm13
	vpslld	$27, %ymm5, %ymm6
	vpsrld	$5, %ymm5, %ymm7
	vpor	%ymm6, %ymm7, %ymm6
	vpxor	%ymm5, %ymm6, %ymm6
	vpslld	$26, %ymm6, %ymm7
	vpsrld	$6, %ymm6, %ymm6
	vpor	%ymm7, %ymm6, %ymm6
	vpslld	$7, %ymm5, %ymm7
	vpsrld	$25, %ymm5, %ymm11
	vpor	%ymm7, %ymm11, %ymm7
	vpxor	%ymm7, %ymm6, %ymm6
	vpxor	%ymm9, %ymm4, %ymm7
	vpand	%ymm5, %ymm7, %ymm7
	vpxor	%ymm9, %ymm7, %ymm7
	vpaddd	%ymm8, %ymm7, %ymm7
	vpaddd	800(%rsp), %ymm6, %ymm6 # 32-byte Folded Reload
	vpaddd	%ymm13, %ymm7, %ymm7
	vpaddd	%ymm6, %ymm7, %ymm7
	vpslld	$30, %ymm3, %ymm6
	vpsrld	$2, %ymm3, %ymm8
	vpor	%ymm6, %ymm8, %ymm6
	vpslld	$19, %ymm3, %ymm8
	vpsrld	$13, %ymm3, %ymm11
	vpor	%ymm8, %ymm11, %ymm8
	vpxor	%ymm8, %ymm6, %ymm6
	vpslld	$10, %ymm3, %ymm8
	vpsrld	$22, %ymm3, %ymm11
	vpor	%ymm8, %ymm11, %ymm8
	vpxor	%ymm8, %ymm6, %ymm6
	vpxor	%ymm1, %ymm2, %ymm8
	vpand	%ymm8, %ymm3, %ymm8
	vpand	%ymm1, %ymm2, %ymm11
	vpxor	%ymm11, %ymm8, %ymm8
	vpaddd	%ymm8, %ymm6, %ymm8
	vpaddd	%ymm0, %ymm7, %ymm6
	vpaddd	%ymm7, %ymm8, %ymm0
	vmovdqu	32(%rsp), %ymm14        # 32-byte Reload
	vpslld	$21, %ymm14, %ymm7
	vpsrld	$11, %ymm14, %ymm8
	vpor	%ymm7, %ymm8, %ymm7
	vpxor	%ymm14, %ymm7, %ymm7
	vpslld	$25, %ymm7, %ymm8
	vpsrld	$7, %ymm7, %ymm7
	vpor	%ymm8, %ymm7, %ymm7
	vpsrld	$3, %ymm14, %ymm8
	vpxor	%ymm8, %ymm7, %ymm7
	vmovdqu	%ymm15, 288(%rsp)       # 32-byte Spill
	vpslld	$30, %ymm15, %ymm8
	vpsrld	$2, %ymm15, %ymm11
	vpor	%ymm8, %ymm11, %ymm8
	vpxor	%ymm15, %ymm8, %ymm8
	vpslld	$15, %ymm8, %ymm11
	vpsrld	$17, %ymm8, %ymm8
	vpor	%ymm11, %ymm8, %ymm8
	vpsrld	$10, %ymm15, %ymm11
	vpxor	%ymm11, %ymm8, %ymm8
	vpaddd	%ymm7, %ymm12, %ymm7
	vpaddd	128(%rsp), %ymm7, %ymm7 # 32-byte Folded Reload
	vpaddd	%ymm8, %ymm7, %ymm11
	vpslld	$27, %ymm6, %ymm7
	vpsrld	$5, %ymm6, %ymm8
	vpor	%ymm7, %ymm8, %ymm7
	vpxor	%ymm6, %ymm7, %ymm7
	vpslld	$26, %ymm7, %ymm8
	vpsrld	$6, %ymm7, %ymm7
	vpor	%ymm8, %ymm7, %ymm7
	vpslld	$7, %ymm6, %ymm8
	vpsrld	$25, %ymm6, %ymm12
	vpor	%ymm8, %ymm12, %ymm8
	vpxor	%ymm8, %ymm7, %ymm7
	vpxor	%ymm4, %ymm5, %ymm8
	vpand	%ymm6, %ymm8, %ymm8
	vpxor	%ymm4, %ymm8, %ymm8
	vpaddd	%ymm9, %ymm8, %ymm8
	vpaddd	768(%rsp), %ymm7, %ymm7 # 32-byte Folded Reload
	vpaddd	%ymm11, %ymm8, %ymm8
	vpaddd	%ymm7, %ymm8, %ymm8
	vpslld	$30, %ymm0, %ymm7
	vpsrld	$2, %ymm0, %ymm9
	vpor	%ymm7, %ymm9, %ymm7
	vpslld	$19, %ymm0, %ymm9
	vpsrld	$13, %ymm0, %ymm12
	vpor	%ymm9, %ymm12, %ymm9
	vpxor	%ymm9, %ymm7, %ymm7
	vpslld	$10, %ymm0, %ymm9
	vpsrld	$22, %ymm0, %ymm12
	vpor	%ymm9, %ymm12, %ymm9
	vpxor	%ymm9, %ymm7, %ymm7
	vpxor	%ymm2, %ymm3, %ymm9
	vpand	%ymm9, %ymm0, %ymm9
	vpand	%ymm2, %ymm3, %ymm12
	vpxor	%ymm12, %ymm9, %ymm9
	vpaddd	%ymm9, %ymm7, %ymm9
	vpaddd	%ymm1, %ymm8, %ymm7
	vpaddd	%ymm8, %ymm9, %ymm1
	vmovdqu	(%rsp), %ymm10          # 32-byte Reload
	vpslld	$21, %ymm10, %ymm8
	vpsrld	$11, %ymm10, %ymm9
	vpor	%ymm8, %ymm9, %ymm8
	vpxor	%ymm10, %ymm8, %ymm8
	vpslld	$25, %ymm8, %ymm9
	vpsrld	$7, %ymm8, %ymm8
	vpor	%ymm9, %ymm8, %ymm8
	vpsrld	$3, %ymm10, %ymm9
	vpxor	%ymm9, %ymm8, %ymm8
	vmovdqu	%ymm13, 352(%rsp)       # 32-byte Spill
	vpslld	$30, %ymm13, %ymm9
	vpsrld	$2, %ymm13, %ymm12
	vpor	%ymm9, %ymm12, %ymm9
	vpxor	%ymm13, %ymm9, %ymm9
	vpslld	$15, %ymm9, %ymm12
	vpsrld	$17, %ymm9, %ymm9
	vpor	%ymm12, %ymm9, %ymm9
	vpsrld	$10, %ymm13, %ymm12
	vpxor	%ymm12, %ymm9, %ymm9
	vpaddd	%ymm8, %ymm14, %ymm8
	vpaddd	256(%rsp), %ymm8, %ymm8 # 32-byte Folded Reload
	vpaddd	%ymm9, %ymm8, %ymm14
	vpslld	$27, %ymm7, %ymm9
	vpsrld	$5, %ymm7, %ymm12
	vpor	%ymm9, %ymm12, %ymm9
	vpxor	%ymm7, %ymm9, %ymm9
	vpslld	$26, %ymm9, %ymm12
	vpsrld	$6, %ymm9, %ymm9
	vpor	%ymm12, %ymm9, %ymm9
	vpslld	$7, %ymm7, %ymm12
	vpsrld	$25, %ymm7, %ymm13
	vpor	%ymm12, %ymm13, %ymm12
	vpxor	%ymm12, %ymm9, %ymm9
	vpxor	%ymm5, %ymm6, %ymm12
	vpand	%ymm7, %ymm12, %ymm12
	vpxor	%ymm5, %ymm12, %ymm12
	vpaddd	%ymm4, %ymm12, %ymm4
	vpaddd	736(%rsp), %ymm9, %ymm9 # 32-byte Folded Reload
	vpaddd	%ymm14, %ymm4, %ymm4
	vpaddd	%ymm9, %ymm4, %ymm9
	vpslld	$30, %ymm1, %ymm4
	vpsrld	$2, %ymm1, %ymm12
	vpor	%ymm4, %ymm12, %ymm4
	vpslld	$19, %ymm1, %ymm12
	vpsrld	$13, %ymm1, %ymm13
	vpor	%ymm12, %ymm13, %ymm12
	vpxor	%ymm12, %ymm4, %ymm4
	vpslld	$10, %ymm1, %ymm12
	vpsrld	$22, %ymm1, %ymm13
	vpor	%ymm12, %ymm13, %ymm12
	vpxor	%ymm12, %ymm4, %ymm4
	vpxor	%ymm3, %ymm0, %ymm12
	vpand	%ymm12, %ymm1, %ymm12
	vpand	%ymm3, %ymm0, %ymm13
	vpxor	%ymm13, %ymm12, %ymm12
	vpaddd	%ymm12, %ymm4, %ymm12
	vpaddd	%ymm2, %ymm9, %ymm4
	vpaddd	%ymm9, %ymm12, %ymm2
	vmovdqu	-32(%rsp), %ymm15       # 32-byte Reload
	vpslld	$21, %ymm15, %ymm9
	vpsrld	$11, %ymm15, %ymm12
	vpor	%ymm9, %ymm12, %ymm9
	vpxor	%ymm15, %ymm9, %ymm9
	vpslld	$25, %ymm9, %ymm12
	vpsrld	$7, %ymm9, %ymm9
	vpor	%ymm12, %ymm9, %ymm9
	vpsrld	$3, %ymm15, %ymm12
	vpxor	%ymm12, %ymm9, %ymm9
	vmovdqu	%ymm11, 64(%rsp)        # 32-byte Spill
	vpslld	$30, %ymm11, %ymm12
	vpsrld	$2, %ymm11, %ymm13
	vpor	%ymm12, %ymm13, %ymm12
	vpxor	%ymm11, %ymm12, %ymm12
	vpslld	$15, %ymm12, %ymm13
	vpsrld	$17, %ymm12, %ymm12
	vpor	%ymm13, %ymm12, %ymm12
	vpsrld	$10, %ymm11, %ymm13
	vpxor	%ymm13, %ymm12, %ymm12
	vpaddd	%ymm9, %ymm10, %ymm9
	vpaddd	96(%rsp), %ymm9, %ymm9  # 32-byte Folded Reload
	vpaddd	%ymm12, %ymm9, %ymm12
	vpslld	$27, %ymm4, %ymm9
	vpsrld	$5, %ymm4, %ymm13
	vpor	%ymm9, %ymm13, %ymm9
	vpxor	%ymm4, %ymm9, %ymm9
	vpslld	$26, %ymm9, %ymm13
	vpsrld	$6, %ymm9, %ymm9
	vpor	%ymm13, %ymm9, %ymm9
	vpslld	$7, %ymm4, %ymm13
	vpsrld	$25, %ymm4, %ymm8
	vpor	%ymm13, %ymm8, %ymm8
	vpxor	%ymm8, %ymm9, %ymm8
	vpxor	%ymm6, %ymm7, %ymm9
	vpand	%ymm4, %ymm9, %ymm9
	vpxor	%ymm6, %ymm9, %ymm9
	vpaddd	%ymm5, %ymm9, %ymm5
	vpaddd	704(%rsp), %ymm8, %ymm8 # 32-byte Folded Reload
	vpaddd	%ymm12, %ymm5, %ymm5
	vpaddd	%ymm8, %ymm5, %ymm8
	vpslld	$30, %ymm2, %ymm5
	vpsrld	$2, %ymm2, %ymm9
	vpor	%ymm5, %ymm9, %ymm5
	vpslld	$19, %ymm2, %ymm9
	vpsrld	$13, %ymm2, %ymm13
	vpor	%ymm9, %ymm13, %ymm9
	vpxor	%ymm9, %ymm5, %ymm5
	vpslld	$10, %ymm2, %ymm9
	vpsrld	$22, %ymm2, %ymm13
	vpor	%ymm9, %ymm13, %ymm9
	vpxor	%ymm9, %ymm5, %ymm5
	vpxor	%ymm0, %ymm1, %ymm9
	vpand	%ymm9, %ymm2, %ymm9
	vpand	%ymm0, %ymm1, %ymm13
	vpxor	%ymm13, %ymm9, %ymm9
	vpaddd	%ymm9, %ymm5, %ymm9
	vpaddd	%ymm3, %ymm8, %ymm5
	vpaddd	%ymm8, %ymm9, %ymm3
	vmovdqu	-64(%rsp), %ymm10       # 32-byte Reload
	vpslld	$21, %ymm10, %ymm8
	vpsrld	$11, %ymm10, %ymm9
	vpor	%ymm8, %ymm9, %ymm8
	vpxor	%ymm10, %ymm8, %ymm8
	vpslld	$25, %ymm8, %ymm9
	vpsrld	$7, %ymm8, %ymm8
	vpor	%ymm9, %ymm8, %ymm8
	vpsrld	$3, %ymm10, %ymm9
	vpxor	%ymm9, %ymm8, %ymm8
	vmovdqu	%ymm14, 32(%rsp)        # 32-byte Spill
	vpslld	$30, %ymm14, %ymm9
	vpsrld	$2, %ymm14, %ymm13
	vpor	%ymm9, %ymm13, %ymm9
	vpxor	%ymm14, %ymm9, %ymm9
	vpslld	$15, %ymm9, %ymm13
	vpsrld	$17, %ymm9, %ymm9
	vpor	%ymm13, %ymm9, %ymm9
	vpsrld	$10, %ymm14, %ymm13
	vpxor	%ymm13, %ymm9, %ymm9
	vpaddd	%ymm8, %ymm15, %ymm8
	vpaddd	-128(%rsp), %ymm8, %ymm8 # 32-byte Folded Reload
	vpaddd	%ymm9, %ymm8, %ymm15
	vpslld	$27, %ymm5, %ymm8
	vpsrld	$5, %ymm5, %ymm13
	vpor	%ymm8, %ymm13, %ymm8
	vpxor	%ymm5, %ymm8, %ymm8
	vpslld	$26, %ymm8, %ymm13
	vpsrld	$6, %ymm8, %ymm8
	vpor	%ymm13, %ymm8, %ymm8
	vpslld	$7, %ymm5, %ymm13
	vpsrld	$25, %ymm5, %ymm9
	vpor	%ymm13, %ymm9, %ymm9
	vpxor	%ymm9, %ymm8, %ymm8
	vpxor	%ymm7, %ymm4, %ymm9
	vpand	%ymm5, %ymm9, %ymm9
	vpxor	%ymm7, %ymm9, %ymm9
	vpaddd	%ymm6, %ymm9, %ymm6
	vpaddd	672(%rsp), %ymm8, %ymm8 # 32-byte Folded Reload
	vpaddd	%ymm15, %ymm6, %ymm6
	vpaddd	%ymm8, %ymm6, %ymm8
	vpslld	$30, %ymm3, %ymm6
	vpsrld	$2, %ymm3, %ymm9
	vpor	%ymm6, %ymm9, %ymm6
	vpslld	$19, %ymm3, %ymm9
	vpsrld	$13, %ymm3, %ymm13
	vpor	%ymm9, %ymm13, %ymm9
	vpxor	%ymm9, %ymm6, %ymm6
	vpslld	$10, %ymm3, %ymm9
	vpsrld	$22, %ymm3, %ymm13
	vpor	%ymm9, %ymm13, %ymm9
	vpxor	%ymm9, %ymm6, %ymm6
	vpxor	%ymm1, %ymm2, %ymm9
	vpand	%ymm9, %ymm3, %ymm9
	vpand	%ymm1, %ymm2, %ymm13
	vpxor	%ymm13, %ymm9, %ymm9
	vpaddd	%ymm9, %ymm6, %ymm9
	vpaddd	%ymm0, %ymm8, %ymm6
	vpaddd	%ymm8, %ymm9, %ymm0
	vmovdqu	320(%rsp), %ymm11       # 32-byte Reload
	vpslld	$21, %ymm11, %ymm8
	vpsrld	$11, %ymm11, %ymm9
	vpor	%ymm8, %ymm9, %ymm8
	vpxor	%ymm11, %ymm8, %ymm8
	vpslld	$25, %ymm8, %ymm9
	vpsrld	$7, %ymm8, %ymm8
	vpor	%ymm9, %ymm8, %ymm8
	vpsrld	$3, %ymm11, %ymm9
	vpxor	%ymm9, %ymm8, %ymm8
	vmovdqu	%ymm12, (%rsp)          # 32-byte Spill
	vpslld	$30, %ymm12, %ymm9
	vpsrld	$2, %ymm12, %ymm13
	vpor	%ymm9, %ymm13, %ymm9
	vpxor	%ymm12, %ymm9, %ymm9
	vpslld	$15, %ymm9, %ymm13
	vpsrld	$17, %ymm9, %ymm9
	vpor	%ymm13, %ymm9, %ymm9
	vpsrld	$10, %ymm12, %ymm13
	vpxor	%ymm13, %ymm9, %ymm9
	vpaddd	%ymm8, %ymm10, %ymm8
	vpaddd	-96(%rsp), %ymm8, %ymm8 # 32-byte Folded Reload
	vpaddd	%ymm9, %ymm8, %ymm12
	vpslld	$27, %ymm6, %ymm8
	vpsrld	$5, %ymm6, %ymm9
	vpor	%ymm8, %ymm9, %ymm8
	vpxor	%ymm6, %ymm8, %ymm8
	vpslld	$26, %ymm8, %ymm9
	vpsrld	$6, %ymm8, %ymm8
	vpor	%ymm9, %ymm8, %ymm8
	vpslld	$7, %ymm6, %ymm9
	vpsrld	$25, %ymm6, %ymm14
	vpor	%ymm9, %ymm14, %ymm9
	vpxor	%ymm9, %ymm8, %ymm8
	vpxor	%ymm4, %ymm5, %ymm9
	vpand	%ymm6, %ymm9, %ymm9
	vpxor	%ymm4, %ymm9, %ymm9
	vpaddd	%ymm7, %ymm9, %ymm7
	vpaddd	640(%rsp), %ymm8, %ymm8 # 32-byte Folded Reload
	vpaddd	%ymm12, %ymm7, %ymm7
	vpaddd	%ymm8, %ymm7, %ymm8
	vpslld	$30, %ymm0, %ymm7
	vpsrld	$2, %ymm0, %ymm9
	vpor	%ymm7, %ymm9, %ymm7
	vpslld	$19, %ymm0, %ymm9
	vpsrld	$13, %ymm0, %ymm14
	vpor	%ymm9, %ymm14, %ymm9
	vpxor	%ymm9, %ymm7, %ymm7
	vpslld	$10, %ymm0, %ymm9
	vpsrld	$22, %ymm0, %ymm14
	vpor	%ymm9, %ymm14, %ymm9
	vpxor	%ymm9, %ymm7, %ymm7
	vpxor	%ymm2, %ymm3, %ymm9
	vpand	%ymm9, %ymm0, %ymm9
	vpand	%ymm2, %ymm3, %ymm14
	vpxor	%ymm14, %ymm9, %ymm9
	vpaddd	%ymm9, %ymm7, %ymm9
	vpaddd	%ymm1, %ymm8, %ymm7
	vpaddd	%ymm8, %ymm9, %ymm14
	vmovdqu	192(%rsp), %ymm9        # 32-byte Reload
	vpslld	$21, %ymm9, %ymm1
	vpsrld	$11, %ymm9, %ymm8
	vpor	%ymm1, %ymm8, %ymm1
	vpxor	%ymm9, %ymm1, %ymm1
	vpslld	$25, %ymm1, %ymm8
	vpsrld	$7, %ymm1, %ymm1
	vpor	%ymm8, %ymm1, %ymm1
	vpsrld	$3, %ymm9, %ymm8
	vmovdqa	%ymm9, %ymm10
	vpxor	%ymm8, %ymm1, %ymm1
	vmovdqu	%ymm15, -32(%rsp)       # 32-byte Spill
	vpslld	$30, %ymm15, %ymm8
	vpsrld	$2, %ymm15, %ymm9
	vpor	%ymm8, %ymm9, %ymm8
	vpxor	%ymm15, %ymm8, %ymm8
	vpslld	$15, %ymm8, %ymm9
	vpsrld	$17, %ymm8, %ymm8
	vpor	%ymm9, %ymm8, %ymm8
	vpsrld	$10, %ymm15, %ymm9
	vpxor	%ymm9, %ymm8, %ymm8
	vpaddd	%ymm1, %ymm11, %ymm1
	vpaddd	288(%rsp), %ymm1, %ymm1 # 32-byte Folded Reload
	vpaddd	%ymm8, %ymm1, %ymm1
	vpslld	$27, %ymm7, %ymm8
	vpsrld	$5, %ymm7, %ymm9
	vpor	%ymm8, %ymm9, %ymm8
	vpxor	%ymm7, %ymm8, %ymm8
	vpslld	$26, %ymm8, %ymm9
	vpsrld	$6, %ymm8, %ymm8
	vpor	%ymm9, %ymm8, %ymm8
	vpslld	$7, %ymm7, %ymm9
	vpsrld	$25, %ymm7, %ymm15
	vpor	%ymm9, %ymm15, %ymm9
	vpxor	%ymm9, %ymm8, %ymm8
	vpxor	%ymm5, %ymm6, %ymm9
	vpand	%ymm7, %ymm9, %ymm9
	vpxor	%ymm5, %ymm9, %ymm9
	vpaddd	%ymm4, %ymm9, %ymm4
	vpaddd	608(%rsp), %ymm8, %ymm8 # 32-byte Folded Reload
	vpaddd	%ymm1, %ymm4, %ymm4
	vpaddd	%ymm8, %ymm4, %ymm8
	vpslld	$30, %ymm14, %ymm4
	vpsrld	$2, %ymm14, %ymm9
	vpor	%ymm4, %ymm9, %ymm4
	vpslld	$19, %ymm14, %ymm9
	vpsrld	$13, %ymm14, %ymm15
	vpor	%ymm9, %ymm15, %ymm9
	vpxor	%ymm9, %ymm4, %ymm4
	vpslld	$10, %ymm14, %ymm9
	vpsrld	$22, %ymm14, %ymm15
	vpor	%ymm9, %ymm15, %ymm9
	vpxor	%ymm9, %ymm4, %ymm4
	vpxor	%ymm3, %ymm0, %ymm9
	vpand	%ymm9, %ymm14, %ymm9
	vpand	%ymm3, %ymm0, %ymm15
	vpxor	%ymm15, %ymm9, %ymm9
	vpaddd	%ymm9, %ymm4, %ymm9
	vpaddd	%ymm2, %ymm8, %ymm4
	vpaddd	%ymm8, %ymm9, %ymm2
	vmovdqu	160(%rsp), %ymm11       # 32-byte Reload
	vpslld	$21, %ymm11, %ymm8
	vpsrld	$11, %ymm11, %ymm9
	vpor	%ymm8, %ymm9, %ymm8
	vpxor	%ymm11, %ymm8, %ymm8
	vpslld	$25, %ymm8, %ymm9
	vpsrld	$7, %ymm8, %ymm8
	vpor	%ymm9, %ymm8, %ymm8
	vpsrld	$3, %ymm11, %ymm9
	vpxor	%ymm9, %ymm8, %ymm8
	vmovdqu	%ymm12, -64(%rsp)       # 32-byte Spill
	vpslld	$30, %ymm12, %ymm9
	vpsrld	$2, %ymm12, %ymm15
	vpor	%ymm9, %ymm15, %ymm9
	vpxor	%ymm12, %ymm9, %ymm9
	vpslld	$15, %ymm9, %ymm15
	vpsrld	$17, %ymm9, %ymm9
	vpor	%ymm15, %ymm9, %ymm9
	vpsrld	$10, %ymm12, %ymm15
	vpxor	%ymm15, %ymm9, %ymm9
	vpaddd	%ymm8, %ymm10, %ymm8
	vpaddd	352(%rsp), %ymm8, %ymm8 # 32-byte Folded Reload
	vpaddd	%ymm9, %ymm8, %ymm10
	vpslld	$27, %ymm4, %ymm8
	vpsrld	$5, %ymm4, %ymm9
	vpor	%ymm8, %ymm9, %ymm8
	vpxor	%ymm4, %ymm8, %ymm8
	vpslld	$26, %ymm8, %ymm9
	vpsrld	$6, %ymm8, %ymm8
	vpor	%ymm9, %ymm8, %ymm8
	vpslld	$7, %ymm4, %ymm9
	vpsrld	$25, %ymm4, %ymm15
	vpor	%ymm9, %ymm15, %ymm9
	vpxor	%ymm9, %ymm8, %ymm8
	vpxor	%ymm6, %ymm7, %ymm9
	vpand	%ymm4, %ymm9, %ymm9
	vpxor	%ymm6, %ymm9, %ymm9
	vpaddd	%ymm5, %ymm9, %ymm5
	vpaddd	576(%rsp), %ymm8, %ymm8 # 32-byte Folded Reload
	vpaddd	%ymm10, %ymm5, %ymm5
	vpaddd	%ymm8, %ymm5, %ymm8
	vpslld	$30, %ymm2, %ymm5
	vpsrld	$2, %ymm2, %ymm9
	vpor	%ymm5, %ymm9, %ymm5
	vpslld	$19, %ymm2, %ymm9
	vpsrld	$13, %ymm2, %ymm15
	vpor	%ymm9, %ymm15, %ymm9
	vpxor	%ymm9, %ymm5, %ymm5
	vpslld	$10, %ymm2, %ymm9
	vpsrld	$22, %ymm2, %ymm15
	vpor	%ymm9, %ymm15, %ymm9
	vpxor	%ymm9, %ymm5, %ymm5
	vpxor	%ymm0, %ymm14, %ymm9
	vpand	%ymm9, %ymm2, %ymm9
	vpand	%ymm0, %ymm14, %ymm15
	vpxor	%ymm15, %ymm9, %ymm9
	vpaddd	%ymm9, %ymm5, %ymm9
	vpaddd	%ymm3, %ymm8, %ymm5
	vpaddd	%ymm8, %ymm9, %ymm3
	vmovdqu	224(%rsp), %ymm12       # 32-byte Reload
	vpslld	$21, %ymm12, %ymm8
	vpsrld	$11, %ymm12, %ymm9
	vpor	%ymm8, %ymm9, %ymm8
	vpxor	%ymm12, %ymm8, %ymm8
	vpslld	$25, %ymm8, %ymm9
	vpsrld	$7, %ymm8, %ymm8
	vpor	%ymm9, %ymm8, %ymm8
	vpsrld	$3, %ymm12, %ymm9
	vpxor	%ymm9, %ymm8, %ymm8
	vpslld	$30, %ymm1, %ymm9
	vpsrld	$2, %ymm1, %ymm15
	vpor	%ymm9, %ymm15, %ymm9
	vpxor	%ymm1, %ymm9, %ymm9
	vpslld	$15, %ymm9, %ymm15
	vpsrld	$17, %ymm9, %ymm9
	vpor	%ymm15, %ymm9, %ymm9
	vpsrld	$10, %ymm1, %ymm1
	vpxor	%ymm1, %ymm9, %ymm1
	vpaddd	%ymm8, %ymm11, %ymm8
	vpaddd	64(%rsp), %ymm8, %ymm8  # 32-byte Folded Reload
	vpaddd	%ymm1, %ymm8, %ymm11
	vpslld	$27, %ymm5, %ymm1
	vpsrld	$5, %ymm5, %ymm8
	vpor	%ymm1, %ymm8, %ymm1
	vpxor	%ymm5, %ymm1, %ymm1
	vpslld	$26, %ymm1, %ymm8
	vpsrld	$6, %ymm1, %ymm1
	vpor	%ymm8, %ymm1, %ymm1
	vpslld	$7, %ymm5, %ymm8
	vpsrld	$25, %ymm5, %ymm9
	vpor	%ymm8, %ymm9, %ymm8
	vpxor	%ymm8, %ymm1, %ymm1
	vpxor	%ymm7, %ymm4, %ymm8
	vpand	%ymm5, %ymm8, %ymm8
	vpxor	%ymm7, %ymm8, %ymm8
	vpaddd	%ymm6, %ymm8, %ymm6
	vpaddd	544(%rsp), %ymm1, %ymm1 # 32-byte Folded Reload
	vpaddd	%ymm11, %ymm6, %ymm6
	vpaddd	%ymm1, %ymm6, %ymm1
	vpslld	$30, %ymm3, %ymm6
	vpsrld	$2, %ymm3, %ymm8
	vpor	%ymm6, %ymm8, %ymm6
	vpslld	$19, %ymm3, %ymm8
	vpsrld	$13, %ymm3, %ymm9
	vpor	%ymm8, %ymm9, %ymm8
	vpxor	%ymm8, %ymm6, %ymm6
	vpslld	$10, %ymm3, %ymm8
	vpsrld	$22, %ymm3, %ymm9
	vpor	%ymm8, %ymm9, %ymm8
	vpxor	%ymm8, %ymm6, %ymm6
	vpxor	%ymm14, %ymm2, %ymm8
	vpand	%ymm8, %ymm3, %ymm8
	vpand	%ymm14, %ymm2, %ymm9
	vpxor	%ymm9, %ymm8, %ymm8
	vpaddd	%ymm8, %ymm6, %ymm6
	vpaddd	%ymm0, %ymm1, %ymm0
	vpaddd	%ymm1, %ymm6, %ymm1
	vmovdqu	128(%rsp), %ymm15       # 32-byte Reload
	vpslld	$21, %ymm15, %ymm6
	vpsrld	$11, %ymm15, %ymm8
	vpor	%ymm6, %ymm8, %ymm6
	vpxor	%ymm15, %ymm6, %ymm6
	vpslld	$25, %ymm6, %ymm8
	vpsrld	$7, %ymm6, %ymm6
	vpor	%ymm8, %ymm6, %ymm6
	vpsrld	$3, %ymm15, %ymm8
	vpxor	%ymm8, %ymm6, %ymm6
	vpslld	$30, %ymm10, %ymm8
	vpsrld	$2, %ymm10, %ymm9
	vpor	%ymm8, %ymm9, %ymm8
	vpxor	%ymm10, %ymm8, %ymm8
	vpslld	$15, %ymm8, %ymm9
	vpsrld	$17, %ymm8, %ymm8
	vpor	%ymm9, %ymm8, %ymm8
	vpsrld	$10, %ymm10, %ymm9
	vpxor	%ymm9, %ymm8, %ymm8
	vpaddd	%ymm6, %ymm12, %ymm6
	vpaddd	32(%rsp), %ymm6, %ymm6  # 32-byte Folded Reload
	vpaddd	%ymm8, %ymm6, %ymm8
	vpslld	$27, %ymm0, %ymm6
	vpsrld	$5, %ymm0, %ymm9
	vpor	%ymm6, %ymm9, %ymm6
	vpxor	%ymm0, %ymm6, %ymm6
	vpslld	$26, %ymm6, %ymm9
	vpsrld	$6, %ymm6, %ymm6
	vpor	%ymm9, %ymm6, %ymm6
	vpslld	$7, %ymm0, %ymm9
	vpsrld	$25, %ymm0, %ymm10
	vpor	%ymm9, %ymm10, %ymm9
	vpxor	%ymm9, %ymm6, %ymm6
	vpxor	%ymm4, %ymm5, %ymm9
	vpand	%ymm0, %ymm9, %ymm9
	vpxor	%ymm4, %ymm9, %ymm9
	vpaddd	%ymm7, %ymm9, %ymm7
	vpaddd	512(%rsp), %ymm6, %ymm6 # 32-byte Folded Reload
	vpaddd	%ymm8, %ymm7, %ymm7
	vpaddd	%ymm6, %ymm7, %ymm7
	vpslld	$30, %ymm1, %ymm6
	vpsrld	$2, %ymm1, %ymm9
	vpor	%ymm6, %ymm9, %ymm6
	vpslld	$19, %ymm1, %ymm9
	vpsrld	$13, %ymm1, %ymm10
	vpor	%ymm9, %ymm10, %ymm9
	vpxor	%ymm9, %ymm6, %ymm6
	vpslld	$10, %ymm1, %ymm9
	vpsrld	$22, %ymm1, %ymm10
	vpor	%ymm9, %ymm10, %ymm9
	vpxor	%ymm9, %ymm6, %ymm6
	vpxor	%ymm2, %ymm3, %ymm9
	vpand	%ymm9, %ymm1, %ymm9
	vpand	%ymm2, %ymm3, %ymm10
	vpxor	%ymm10, %ymm9, %ymm9
	vpaddd	%ymm9, %ymm6, %ymm9
	vpaddd	%ymm14, %ymm7, %ymm6
	vpaddd	%ymm7, %ymm9, %ymm7
	vmovdqu	256(%rsp), %ymm13       # 32-byte Reload
	vpslld	$21, %ymm13, %ymm9
	vpsrld	$11, %ymm13, %ymm10
	vpor	%ymm9, %ymm10, %ymm9
	vpxor	%ymm13, %ymm9, %ymm9
	vpslld	$25, %ymm9, %ymm10
	vpsrld	$7, %ymm9, %ymm9
	vpor	%ymm10, %ymm9, %ymm9
	vpsrld	$3, %ymm13, %ymm10
	vpxor	%ymm10, %ymm9, %ymm9
	vpslld	$30, %ymm11, %ymm10
	vpsrld	$2, %ymm11, %ymm14
	vpor	%ymm10, %ymm14, %ymm10
	vpxor	%ymm11, %ymm10, %ymm10
	vpslld	$15, %ymm10, %ymm14
	vpsrld	$17, %ymm10, %ymm10
	vpor	%ymm14, %ymm10, %ymm10
	vmovdqu	480(%rsp), %ymm14       # 32-byte Reload
	vpsrld	$10, %ymm11, %ymm11
	vpxor	%ymm11, %ymm10, %ymm10
	vpaddd	%ymm9, %ymm15, %ymm9
	vpaddd	(%rsp), %ymm9, %ymm9    # 32-byte Folded Reload
	vpaddd	%ymm10, %ymm9, %ymm10
	vpslld	$27, %ymm6, %ymm9
	vpsrld	$5, %ymm6, %ymm11
	vpor	%ymm9, %ymm11, %ymm9
	vpxor	%ymm6, %ymm9, %ymm9
	vpslld	$26, %ymm9, %ymm11
	vpsrld	$6, %ymm9, %ymm9
	vpor	%ymm11, %ymm9, %ymm9
	vpslld	$7, %ymm6, %ymm11
	vpsrld	$25, %ymm6, %ymm12
	vpor	%ymm11, %ymm12, %ymm11
	vpxor	%ymm11, %ymm9, %ymm9
	vpxor	%ymm5, %ymm0, %ymm11
	vpand	%ymm6, %ymm11, %ymm11
	vpxor	%ymm5, %ymm11, %ymm11
	vpaddd	%ymm4, %ymm11, %ymm4
	vpaddd	%ymm14, %ymm9, %ymm9
	vpaddd	%ymm10, %ymm4, %ymm4
	vpaddd	%ymm9, %ymm4, %ymm4
	vpslld	$30, %ymm7, %ymm9
	vpsrld	$2, %ymm7, %ymm11
	vpor	%ymm9, %ymm11, %ymm9
	vpslld	$19, %ymm7, %ymm11
	vpsrld	$13, %ymm7, %ymm12
	vpor	%ymm11, %ymm12, %ymm11
	vpxor	%ymm11, %ymm9, %ymm9
	vpslld	$10, %ymm7, %ymm11
	vpsrld	$22, %ymm7, %ymm12
	vpor	%ymm11, %ymm12, %ymm11
	vpxor	%ymm11, %ymm9, %ymm9
	vpxor	%ymm3, %ymm1, %ymm11
	vpand	%ymm11, %ymm7, %ymm11
	vpand	%ymm3, %ymm1, %ymm12
	vpxor	%ymm12, %ymm11, %ymm11
	vpaddd	%ymm11, %ymm9, %ymm9
	vpaddd	%ymm2, %ymm4, %ymm2
	vpaddd	%ymm4, %ymm9, %ymm4
	vmovdqu	96(%rsp), %ymm12        # 32-byte Reload
	vpslld	$21, %ymm12, %ymm9
	vpsrld	$11, %ymm12, %ymm11
	vpor	%ymm9, %ymm11, %ymm9
	vpxor	%ymm12, %ymm9, %ymm9
	vpslld	$25, %ymm9, %ymm11
	vpsrld	$7, %ymm9, %ymm9
	vpor	%ymm11, %ymm9, %ymm9
	vpsrld	$3, %ymm12, %ymm11
	vmovdqa	%ymm12, %ymm15
	vpxor	%ymm11, %ymm9, %ymm9
	vpslld	$30, %ymm8, %ymm11
	vpsrld	$2, %ymm8, %ymm12
	vpor	%ymm11, %ymm12, %ymm11
	vpxor	%ymm8, %ymm11, %ymm11
	vpslld	$15, %ymm11, %ymm12
	vpsrld	$17, %ymm11, %ymm11
	vpor	%ymm12, %ymm11, %ymm11
	vpsrld	$10, %ymm8, %ymm8
	vpxor	%ymm8, %ymm11, %ymm8
	vpaddd	%ymm9, %ymm13, %ymm9
	vpaddd	-32(%rsp), %ymm9, %ymm9 # 32-byte Folded Reload
	vpaddd	%ymm8, %ymm9, %ymm8
	vpslld	$27, %ymm2, %ymm9
	vpsrld	$5, %ymm2, %ymm11
	vpor	%ymm9, %ymm11, %ymm9
	vpxor	%ymm2, %ymm9, %ymm9
	vpslld	$26, %ymm9, %ymm11
	vpsrld	$6, %ymm9, %ymm9
	vpor	%ymm11, %ymm9, %ymm9
	vpslld	$7, %ymm2, %ymm11
	vpsrld	$25, %ymm2, %ymm12
	vpor	%ymm11, %ymm12, %ymm11
	vpxor	%ymm11, %ymm9, %ymm9
	vpxor	%ymm0, %ymm6, %ymm11
	vpand	%ymm2, %ymm11, %ymm11
	vpxor	%ymm0, %ymm11, %ymm11
	vpaddd	%ymm5, %ymm11, %ymm5
	vpaddd	%ymm8, %ymm5, %ymm5
	vpaddd	448(%rsp), %ymm9, %ymm8 # 32-byte Folded Reload
	vpaddd	%ymm8, %ymm5, %ymm5
	vpslld	$30, %ymm4, %ymm8
	vpsrld	$2, %ymm4, %ymm9
	vpor	%ymm8, %ymm9, %ymm8
	vpslld	$19, %ymm4, %ymm9
	vpsrld	$13, %ymm4, %ymm11
	vpor	%ymm9, %ymm11, %ymm9
	vpxor	%ymm9, %ymm8, %ymm8
	vpslld	$10, %ymm4, %ymm9
	vpsrld	$22, %ymm4, %ymm11
	vpor	%ymm9, %ymm11, %ymm9
	vpxor	%ymm9, %ymm8, %ymm8
	vpxor	%ymm1, %ymm7, %ymm9
	vpand	%ymm9, %ymm4, %ymm9
	vpand	%ymm1, %ymm7, %ymm11
	vpxor	%ymm11, %ymm9, %ymm9
	vpaddd	%ymm9, %ymm8, %ymm8
	vpaddd	%ymm3, %ymm5, %ymm3
	vpaddd	%ymm5, %ymm8, %ymm5
	vmovdqu	-128(%rsp), %ymm11      # 32-byte Reload
	vpslld	$21, %ymm11, %ymm8
	vpsrld	$11, %ymm11, %ymm9
	vpor	%ymm8, %ymm9, %ymm8
	vpxor	%ymm11, %ymm8, %ymm8
	vpslld	$25, %ymm8, %ymm9
	vpsrld	$7, %ymm8, %ymm8
	vpor	%ymm9, %ymm8, %ymm8
	vpsrld	$3, %ymm11, %ymm9
	vpxor	%ymm9, %ymm8, %ymm8
	vpslld	$30, %ymm10, %ymm9
	vpsrld	$2, %ymm10, %ymm11
	vpor	%ymm9, %ymm11, %ymm9
	vpxor	%ymm10, %ymm9, %ymm9
	vpslld	$15, %ymm9, %ymm11
	vpsrld	$17, %ymm9, %ymm9
	vpor	%ymm11, %ymm9, %ymm9
	vpsrld	$10, %ymm10, %ymm10
	vpxor	%ymm10, %ymm9, %ymm9
	vpaddd	%ymm8, %ymm15, %ymm8
	vpaddd	-64(%rsp), %ymm8, %ymm8 # 32-byte Folded Reload
	vpaddd	%ymm9, %ymm8, %ymm8
	vpslld	$27, %ymm3, %ymm9
	vpsrld	$5, %ymm3, %ymm10
	vpor	%ymm9, %ymm10, %ymm9
	vpxor	%ymm3, %ymm9, %ymm9
	vpslld	$26, %ymm9, %ymm10
	vpsrld	$6, %ymm9, %ymm9
	vpor	%ymm10, %ymm9, %ymm9
	vpslld	$7, %ymm3, %ymm10
	vpsrld	$25, %ymm3, %ymm11
	vpor	%ymm10, %ymm11, %ymm10
	vmovdqu	416(%rsp), %ymm11       # 32-byte Reload
	vpxor	%ymm10, %ymm9, %ymm9
	vpxor	%ymm6, %ymm2, %ymm10
	vpand	%ymm3, %ymm10, %ymm10
	vpxor	%ymm6, %ymm10, %ymm10
	vpaddd	%ymm0, %ymm10, %ymm0
	vpaddd	%ymm8, %ymm0, %ymm0
	vpaddd	%ymm11, %ymm9, %ymm8
	vpaddd	%ymm8, %ymm0, %ymm0
	vpslld	$30, %ymm5, %ymm8
	vpsrld	$2, %ymm5, %ymm9
	vpor	%ymm8, %ymm9, %ymm8
	vpslld	$19, %ymm5, %ymm9
	vpsrld	$13, %ymm5, %ymm10
	vpor	%ymm9, %ymm10, %ymm9
	vpxor	%ymm9, %ymm8, %ymm8
	vpslld	$10, %ymm5, %ymm9
	vpsrld	$22, %ymm5, %ymm10
	vpor	%ymm9, %ymm10, %ymm9
	vpxor	%ymm9, %ymm8, %ymm8
	vpxor	%ymm7, %ymm4, %ymm9
	vpand	%ymm9, %ymm5, %ymm9
	vpand	%ymm7, %ymm4, %ymm10
	vpxor	%ymm10, %ymm9, %ymm9
	vpaddd	%ymm9, %ymm8, %ymm8
	vpaddd	2464(%rsp), %ymm8, %ymm8 # 32-byte Folded Reload
	vpaddd	%ymm8, %ymm0, %ymm8
	vmovdqa	%ymm8, -224(%rcx)
	vpaddd	2528(%rsp), %ymm5, %ymm5 # 32-byte Folded Reload
	vmovdqa	%ymm5, -192(%rcx)
	vmovdqu	2432(%rsp), %ymm5       # 32-byte Reload
	vpaddd	2592(%rsp), %ymm4, %ymm4 # 32-byte Folded Reload
	vmovdqa	%ymm4, -160(%rcx)
	vpaddd	2656(%rsp), %ymm7, %ymm4 # 32-byte Folded Reload
	vmovdqa	%ymm4, -128(%rcx)
	vpaddd	2496(%rsp), %ymm1, %ymm1 # 32-byte Folded Reload
	vpaddd	%ymm1, %ymm0, %ymm0
	vmovdqa	%ymm0, -96(%rcx)
	vpaddd	2560(%rsp), %ymm3, %ymm0 # 32-byte Folded Reload
	vmovdqa	%ymm0, -64(%rcx)
	vpaddd	2624(%rsp), %ymm2, %ymm0 # 32-byte Folded Reload
	vmovdqa	%ymm0, -32(%rcx)
	vpaddd	2688(%rsp), %ymm6, %ymm0 # 32-byte Folded Reload
	vmovdqu	2400(%rsp), %ymm9       # 32-byte Reload
	vmovdqa	%ymm0, (%rcx)
	addq	$8, %rax
	addq	$256, %rcx              # imm = 0x100
	cmpq	$16, %rax
	jb	.LBB4_3
# %bb.4:                                #   in Loop: Header=BB4_2 Depth=1
	addq	$1024, %rsi             # imm = 0x400
	addq	$1, %r8
	cmpq	%rdx, %r8
	jne	.LBB4_2
.LBB4_5:
	addq	$2744, %rsp             # imm = 0xAB8
	vzeroupper
	retq
.Lfunc_end4:
	.size	sha256x16_avx2_update, .Lfunc_end4-sha256x16_avx2_update
	.cfi_endproc
                                        # -- End function
	.section	.rodata.cst32,"aM",@progbits,32
	.p2align	5               # -- Begin function sha256x16_avx2_final
.LCPI5_0:
	.long	0                       # 0x0
	.long	8                       # 0x8
	.long	16                      # 0x10
	.long	24                      # 0x18
	.long	32                      # 0x20
	.long	40                      # 0x28
	.long	48                      # 0x30
	.long	56                      # 0x38
.LCPI5_1:
	.byte	3                       # 0x3
	.byte	2                       # 0x2
	.byte	1                       # 0x1
	.byte	0                       # 0x0
	.byte	7                       # 0x7
	.byte	6                       # 0x6
	.byte	5                       # 0x5
	.byte	4                       # 0x4
	.byte	11                      # 0xb
	.byte	10                      # 0xa
	.byte	9                       # 0x9
	.byte	8                       # 0x8
	.byte	15                      # 0xf
	.byte	14                      # 0xe
	.byte	13                      # 0xd
	.byte	12                      # 0xc
	.byte	3                       # 0x3
	.byte	2                       # 0x2
	.byte	1                       # 0x1
	.byte	0                       # 0x0
	.byte	7                       # 0x7
	.byte	6                       # 0x6
	.byte	5                       # 0x5
	.byte	4                       # 0x4
	.byte	11                      # 0xb
	.byte	10                      # 0xa
	.byte	9                       # 0x9
	.byte	8                       # 0x8
	.byte	15                      # 0xf
	.byte	14                      # 0xe
	.byte	13                      # 0xd
	.byte	12                      # 0xc
	.text
	.hidden	sha256x16_avx2_final
	.globl	sha256x16_avx2_final
	.p2align	4, 0x90
	.type	sha256x16_avx2_final,@function
sha256x16_avx2_final:                   # @sha256x16_avx2_final
	.cfi_startproc
# %bb.0:
	vmovdqa	.LCPI5_0(%rip), %ymm0   # ymm0 = [0,8,16,24,32,40,48,56]
	vpxor	%xmm2, %xmm2, %xmm2
	vpcmpeqd	%ymm1, %ymm1, %ymm1
	vpgatherdd	%ymm1, (%rdi,%ymm0,4), %ymm2
	vmovdqa	.LCPI5_1(%rip), %ymm1   # ymm1 = [3,2,1,0,7,6,5,4,11,10,9,8,15,14,13,12,3,2,1,0,7,6,5,4,11,10,9,8,15,14,13,12]
	vpshufb	%ymm1, %ymm2, %ymm2
	vmovdqu	%ymm2, (%rsi)
	leaq	4(%rdi), %rax
	vpxor	%xmm2, %xmm2, %xmm2
	vpcmpeqd	%ymm3, %ymm3, %ymm3
	vpgatherdd	%ymm3, (%rax,%ymm0,4), %ymm2
	vpshufb	%ymm1, %ymm2, %ymm2
	vmovdqu	%ymm2, 32(%rsi)
	leaq	8(%rdi), %rax
	vpxor	%xmm2, %xmm2, %xmm2
	vpcmpeqd	%ymm3, %ymm3, %ymm3
	vpgatherdd	%ymm3, (%rax,%ymm0,4), %ymm2
	vpshufb	%ymm1, %ymm2, %ymm2
	vmovdqu	%ymm2, 64(%rsi)
	leaq	12(%rdi), %rax
	vpxor	%xmm2, %xmm2, %xmm2
	vpcmpeqd	%ymm3, %ymm3, %ymm3
	vpgatherdd	%ymm3, (%rax,%ymm0,4), %ymm2
	vpshufb	%ymm1, %ymm2, %ymm2
	vmovdqu	%ymm2, 96(%rsi)
	leaq	16(%rdi), %rax
	vpxor	%xmm2, %xmm2, %xmm2
	vpcmpeqd	%ymm3, %ymm3, %ymm3
	vpgatherdd	%ymm3, (%rax,%ymm0,4), %ymm2
	vpshufb	%ymm1, %ymm2, %ymm2
	vmovdqu	%ymm2, 128(%rsi)
	leaq	20(%rdi), %rax
	vpxor	%xmm2, %xmm2, %xmm2
	vpcmpeqd	%ymm3, %ymm3, %ymm3
	vpgatherdd	%ymm3, (%rax,%ymm0,4), %ymm2
	vpshufb	%ymm1, %ymm2, %ymm2
	vmovdqu	%ymm2, 160(%rsi)
	leaq	24(%rdi), %rax
	vpxor	%xmm2, %xmm2, %xmm2
	vpcmpeqd	%ymm3, %ymm3, %ymm3
	vpgatherdd	%ymm3, (%rax,%ymm0,4), %ymm2
	vpshufb	%ymm1, %ymm2, %ymm2
	vmovdqu	%ymm2, 192(%rsi)
	leaq	28(%rdi), %rax
	vpxor	%xmm2, %xmm2, %xmm2
	vpcmpeqd	%ymm3, %ymm3, %ymm3
	vpgatherdd	%ymm3, (%rax,%ymm0,4), %ymm2
	vpshufb	%ymm1, %ymm2, %ymm2
	vmovdqu	%ymm2, 224(%rsi)
	leaq	256(%rdi), %rax
	vpxor	%xmm2, %xmm2, %xmm2
	vpcmpeqd	%ymm3, %ymm3, %ymm3
	vpgatherdd	%ymm3, (%rax,%ymm0,4), %ymm2
	vpshufb	%ymm1, %ymm2, %ymm2
	vmovdqu	%ymm2, 256(%rsi)
	leaq	260(%rdi), %rax
	vpxor	%xmm2, %xmm2, %xmm2
	vpcmpeqd	%ymm3, %ymm3, %ymm3
	vpgatherdd	%ymm3, (%rax,%ymm0,4), %ymm2
	vpshufb	%ymm1, %ymm2, %ymm2
	vmovdqu	%ymm2, 288(%rsi)
	leaq	264(%rdi), %rax
	vpxor	%xmm2, %xmm2, %xmm2
	vpcmpeqd	%ymm3, %ymm3, %ymm3
	vpgatherdd	%ymm3, (%rax,%ymm0,4), %ymm2
	vpshufb	%ymm1, %ymm2, %ymm2
	vmovdqu	%ymm2, 320(%rsi)
	leaq	268(%rdi), %rax
	vpxor	%xmm2, %xmm2, %xmm2
	vpcmpeqd	%ymm3, %ymm3, %ymm3
	vpgatherdd	%ymm3, (%rax,%ymm0,4), %ymm2
	vpshufb	%ymm1, %ymm2, %ymm2
	vmovdqu	%ymm2, 352(%rsi)
	vpcmpeqd	%ymm2, %ymm2, %ymm2
	leaq	272(%rdi), %rax
	vpxor	%xmm3, %xmm3, %xmm3
	vpcmpeqd	%ymm4, %ymm4, %ymm4
	vpgatherdd	%ymm4, (%rax,%ymm0,4), %ymm3
	vpshufb	%ymm1, %ymm3, %ymm3
	vmovdqu	%ymm3, 384(%rsi)
	vpxor	%xmm3, %xmm3, %xmm3
	leaq	276(%rdi), %rax
	vpxor	%xmm4, %xmm4, %xmm4
	vpcmpeqd	%ymm5, %ymm5, %ymm5
	vpgatherdd	%ymm5, (%rax,%ymm0,4), %ymm4
	vpshufb	%ymm1, %ymm4, %ymm4
	vmovdqu	%ymm4, 416(%rsi)
	leaq	280(%rdi), %rax
	vpxor	%xmm4, %xmm4, %xmm4
	vpcmpeqd	%ymm5, %ymm5, %ymm5
	vpgatherdd	%ymm5, (%rax,%ymm0,4), %ymm4
	vpshufb	%ymm1, %ymm4, %ymm4
	vmovdqu	%ymm4, 448(%rsi)
	addq	$284, %rdi              # imm = 0x11C
	vpgatherdd	%ymm2, (%rdi,%ymm0,4), %ymm3
	vpshufb	%ymm1, %ymm3, %ymm0
	vmovdqu	%ymm0, 480(%rsi)
	vzeroupper
	retq
.Lfunc_end5:
	.size	sha256x16_avx2_final, .Lfunc_end5-sha256x16_avx2_final
	.cfi_endproc
                                        # -- End function

	.ident	"clang version 6.0.1 (tags/RELEASE_601/final)"
	.section	".note.GNU-stack","",@progbits
