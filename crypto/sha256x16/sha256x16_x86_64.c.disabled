/* Copyright (c) 2018, Google Inc.
 *
 * Permission to use, copy, modify, and/or distribute this software for any
 * purpose with or without fee is hereby granted, provided that the above
 * copyright notice and this permission notice appear in all copies.
 *
 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY
 * SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN ACTION
 * OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF OR IN
 * CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE. */

// This file is not compiled in the normal way due its use of intrinsics and the
// problems that they cause when trying to support a various compilers. Instead,
// it is compiled to textual assembly (asm/sha256x16_x86_64.S) and that file is
// checked in. If you need to edit it, you'll need to recompile it by hand:
//
//   ln -s sha256x16_x86_64.c.disabled sha256x16_x86_64.c &&
//   clang -S -O3 sha256x16_x86_64.c -I../../include -Wall
//      -fvisibility=hidden -o asm/sha256x16_x86_64.S &&
//   rm sha256x16_x86_64.c

#include <openssl/sha256x16.h>

#include <openssl/cpu.h>
#include <openssl/digest.h>
#include <openssl/obj.h>
#include <openssl/sha.h>
#include <openssl/type_check.h>

#include <emmintrin.h>
#include <immintrin.h>
#include <tmmintrin.h>

#define AVX
#define AVX2

#include "../internal.h"
#include "internal.h"

static_assert(sizeof(((struct state *)0)->u.avx) <=
                  sizeof(((struct state *)0)->u.generic),
              "generic state too small for AVX");
static_assert(sizeof(((struct state *)0)->u.avx2) <=
                  sizeof(((struct state *)0)->u.generic),
              "generic state too small for AVX2");

// kRoundConstants are magic values for SHA-256.
static const uint32_t kRoundConstants[64] = {
    0x428a2f98, 0x71374491, 0xb5c0fbcf, 0xe9b5dba5, 0x3956c25b, 0x59f111f1,
    0x923f82a4, 0xab1c5ed5, 0xd807aa98, 0x12835b01, 0x243185be, 0x550c7dc3,
    0x72be5d74, 0x80deb1fe, 0x9bdc06a7, 0xc19bf174, 0xe49b69c1, 0xefbe4786,
    0x0fc19dc6, 0x240ca1cc, 0x2de92c6f, 0x4a7484aa, 0x5cb0a9dc, 0x76f988da,
    0x983e5152, 0xa831c66d, 0xb00327c8, 0xbf597fc7, 0xc6e00bf3, 0xd5a79147,
    0x06ca6351, 0x14292967, 0x27b70a85, 0x2e1b2138, 0x4d2c6dfc, 0x53380d13,
    0x650a7354, 0x766a0abb, 0x81c2c92e, 0x92722c85, 0xa2bfe8a1, 0xa81a664b,
    0xc24b8b70, 0xc76c51a3, 0xd192e819, 0xd6990624, 0xf40e3585, 0x106aa070,
    0x19a4c116, 0x1e376c08, 0x2748774c, 0x34b0bcb5, 0x391c0cb3, 0x4ed8aa4a,
    0x5b9cca4f, 0x682e6ff3, 0x748f82ee, 0x78a5636f, 0x84c87814, 0x8cc70208,
    0x90befffa, 0xa4506ceb, 0xbef9a3f7, 0xc67178f2,
};

__attribute((target("avx")))
static __m128i rotate_avx(__m128i v, uint8_t right_bits) {
  return _mm_slli_epi32(v, 32 - right_bits) ^
         _mm_srli_epi32(v, right_bits);
}

__attribute((target("avx")))
static __m128i from_u32_avx(const uint32_t *v) {
  return _mm_set1_epi32((int) *v);
}

__attribute__((target("avx")))
void sha256x16_avx_init(struct state *st) {
  for (size_t i = 0; i < 8; i++) {
    st->u.avx[i] = from_u32_avx(&kInitialValues[i]);
    st->u.avx[8 + i] = st->u.avx[i];
    st->u.avx[16 + i] = st->u.avx[i];
    st->u.avx[24 + i] = st->u.avx[i];
  }
}

__attribute__((target("avx"))) void sha256x16_avx_update(struct state *st,
                                                         const uint8_t *data,
                                                         size_t num_blocks) {
  const __m128i kByteSwapIndexes =
      _mm_setr_epi8(3, 2, 1, 0, 7, 6, 5, 4, 11, 10, 9, 8, 15, 14, 13, 12);

  const uint8_t *block_start = data;

  for (size_t i = 0; i < num_blocks; i++) {
    for (size_t starting_word = 0; starting_word < 32; starting_word += 8) {
      const uint8_t *half_data = block_start + 2 * starting_word;

      __m128i a = st->u.avx[0 + starting_word];
      __m128i b = st->u.avx[1 + starting_word];
      __m128i c = st->u.avx[2 + starting_word];
      __m128i d = st->u.avx[3 + starting_word];
      __m128i e = st->u.avx[4 + starting_word];
      __m128i f = st->u.avx[5 + starting_word];
      __m128i g = st->u.avx[6 + starting_word];
      __m128i h = st->u.avx[7 + starting_word];
      __m128i window[16];

#if defined(__clang__)
#pragma unroll
#endif
      for (size_t j = 0; j < 64; j++) {
        __m128i w;

        if (j < 16) {
          memcpy(&w, half_data, sizeof(w));
          w = _mm_shuffle_epi8(w, kByteSwapIndexes);
          half_data += 512 / 8;
          window[j] = w;
        } else {
          const __m128i w16 = window[j & 15];
          const __m128i w15 = window[(j - 15) & 15];
          const __m128i w7 = window[(j - 7) & 15];
          const __m128i w2 = window[(j - 2) & 15];

          const __m128i s0 = rotate_avx(w15 ^ rotate_avx(w15, 18 - 7), 7) ^
                             _mm_srli_epi32(w15, 3);
          const __m128i s1 = rotate_avx(w2 ^ rotate_avx(w2, 19 - 17), 17) ^
                             _mm_srli_epi32(w2, 10);

          w = _mm_add_epi32(_mm_add_epi32(w16, s0), _mm_add_epi32(w7, s1));
        }

        const __m128i ch = ((f ^ g) & e) ^ g;
        const __m128i S1 =
            rotate_avx(e ^ rotate_avx(e, 5), 6) ^ rotate_avx(e, 25);

        const __m128i temp1 = _mm_add_epi32(
            _mm_add_epi32(_mm_add_epi32(h, S1),
                          _mm_add_epi32(ch, from_u32_avx(&kRoundConstants[j]))),
            w);

        h = g;
        g = f;
        f = e;
        e = _mm_add_epi32(d, temp1);
        d = c;

        const __m128i S0 =
            rotate_avx(a, 2) ^ rotate_avx(a, 13) ^ rotate_avx(a, 22);
        const __m128i maj = (a & b) ^ (a & c) ^ (b & c);
        const __m128i temp2 = _mm_add_epi32(S0, maj);

        c = b;
        b = a;
        a = _mm_add_epi32(temp1, temp2);

        if (j >= 16) {
          window[j & 15] = w;
        }
      }

      st->u.avx[0 + starting_word] =
          _mm_add_epi32(st->u.avx[0 + starting_word], a);
      st->u.avx[1 + starting_word] =
          _mm_add_epi32(st->u.avx[1 + starting_word], b);
      st->u.avx[2 + starting_word] =
          _mm_add_epi32(st->u.avx[2 + starting_word], c);
      st->u.avx[3 + starting_word] =
          _mm_add_epi32(st->u.avx[3 + starting_word], d);
      st->u.avx[4 + starting_word] =
          _mm_add_epi32(st->u.avx[4 + starting_word], e);
      st->u.avx[5 + starting_word] =
          _mm_add_epi32(st->u.avx[5 + starting_word], f);
      st->u.avx[6 + starting_word] =
          _mm_add_epi32(st->u.avx[6 + starting_word], g);
      st->u.avx[7 + starting_word] =
          _mm_add_epi32(st->u.avx[7 + starting_word], h);
    }

    block_start += 1024;
  }
}

__attribute__((target("avx"))) void sha256x16_avx_final(struct state *st,
                                                        uint8_t out[32 * 16]) {
  const uint32_t *state_words = (uint32_t *)&st->u.avx[0];

  for (size_t starting_lane = 0; starting_lane < 16; starting_lane += 4) {
    for (size_t sublane = 0; sublane < 4; sublane++) {
      for (size_t i = 0; i < 8; i++) {
        uint32_t w;
        OPENSSL_memcpy(&w, &state_words[4*i + sublane], sizeof(w));
        w = CRYPTO_bswap4(w);
        OPENSSL_memcpy(out, &w, sizeof(w));
        out += sizeof(w);
      }
    }

    state_words += 4*8;
  }
}

__attribute((target("avx2")))
static __m256i rotate_avx2(__m256i v, uint8_t right_bits) {
  return _mm256_slli_epi32(v, 32 - right_bits) ^
         _mm256_srli_epi32(v, right_bits);
}

__attribute((target("avx2")))
static __m256i from_u32_avx2(uint32_t v) {
  return _mm256_broadcastd_epi32(_mm_setr_epi32(v, 0, 0, 0));
}

__attribute__((target("avx2"))) void sha256x16_avx2_init(struct state *st) {
  for (size_t i = 0; i < 8; i++) {
    st->u.avx2[i] = from_u32_avx2(kInitialValues[i]);
    st->u.avx2[8 + i] = st->u.avx2[i];
  }
}

__attribute__((target("avx2"))) void sha256x16_avx2_update(struct state *st,
                                                           const uint8_t *data,
                                                           size_t num_blocks) {
  const __m256i kByteSwapIndexes = _mm256_broadcastsi128_si256(
      _mm_setr_epi8(3, 2, 1, 0, 7, 6, 5, 4, 11, 10, 9, 8, 15, 14, 13, 12));

  const uint8_t *block_start = data;

  for (size_t i = 0; i < num_blocks; i++) {
    for (size_t left_right = 0; left_right < 16; left_right += 8) {
      const uint8_t *half_data = block_start + 4 * left_right;

      __m256i a = st->u.avx2[0 + left_right];
      __m256i b = st->u.avx2[1 + left_right];
      __m256i c = st->u.avx2[2 + left_right];
      __m256i d = st->u.avx2[3 + left_right];
      __m256i e = st->u.avx2[4 + left_right];
      __m256i f = st->u.avx2[5 + left_right];
      __m256i g = st->u.avx2[6 + left_right];
      __m256i h = st->u.avx2[7 + left_right];
      __m256i window[16];

#if defined(__clang__)
#pragma unroll
#endif
      for (size_t j = 0; j < 64; j++) {
        __m256i w;

        if (j < 16) {
          memcpy(&w, half_data, sizeof(w));
          w = _mm256_shuffle_epi8(w, kByteSwapIndexes);
          half_data += 512 / 8;
          window[j] = w;
        } else {
          const __m256i w16 = window[j & 15];
          const __m256i w15 = window[(j - 15) & 15];
          const __m256i w7 = window[(j - 7) & 15];
          const __m256i w2 = window[(j - 2) & 15];

          const __m256i s0 = rotate_avx2(w15 ^ rotate_avx2(w15, 18 - 7), 7) ^
                             _mm256_srli_epi32(w15, 3);
          const __m256i s1 = rotate_avx2(w2 ^ rotate_avx2(w2, 19 - 17), 17) ^
                             _mm256_srli_epi32(w2, 10);

          w = _mm256_add_epi32(_mm256_add_epi32(w16, s0),
                               _mm256_add_epi32(w7, s1));
        }

        const __m256i ch = ((f ^ g) & e) ^ g;
        const __m256i S1 =
            rotate_avx2(e ^ rotate_avx2(e, 5), 6) ^ rotate_avx2(e, 25);

        const __m256i temp1 = _mm256_add_epi32(
            _mm256_add_epi32(
                _mm256_add_epi32(h, S1),
                _mm256_add_epi32(ch, from_u32_avx2(kRoundConstants[j]))),
            w);

        h = g;
        g = f;
        f = e;
        e = _mm256_add_epi32(d, temp1);
        d = c;

        const __m256i S0 =
            rotate_avx2(a, 2) ^ rotate_avx2(a, 13) ^ rotate_avx2(a, 22);
        const __m256i maj = (a & b) ^ (a & c) ^ (b & c);
        const __m256i temp2 = _mm256_add_epi32(S0, maj);

        c = b;
        b = a;
        a = _mm256_add_epi32(temp1, temp2);

        if (j >= 16) {
          window[j & 15] = w;
        }
      }

      st->u.avx2[0 + left_right] =
          _mm256_add_epi32(st->u.avx2[0 + left_right], a);
      st->u.avx2[1 + left_right] =
          _mm256_add_epi32(st->u.avx2[1 + left_right], b);
      st->u.avx2[2 + left_right] =
          _mm256_add_epi32(st->u.avx2[2 + left_right], c);
      st->u.avx2[3 + left_right] =
          _mm256_add_epi32(st->u.avx2[3 + left_right], d);
      st->u.avx2[4 + left_right] =
          _mm256_add_epi32(st->u.avx2[4 + left_right], e);
      st->u.avx2[5 + left_right] =
          _mm256_add_epi32(st->u.avx2[5 + left_right], f);
      st->u.avx2[6 + left_right] =
          _mm256_add_epi32(st->u.avx2[6 + left_right], g);
      st->u.avx2[7 + left_right] =
          _mm256_add_epi32(st->u.avx2[7 + left_right], h);
    }

    block_start += 1024;
  }
}

__attribute__((target("avx2"))) void sha256x16_avx2_final(
    struct state *st, uint8_t out[32 * 16]) {
  const uint32_t *state_words = (uint32_t *)&st->u.avx2[0];

  const __m256i kByteSwapIndexes = _mm256_broadcastsi128_si256(
      _mm_setr_epi8(3, 2, 1, 0, 7, 6, 5, 4, 11, 10, 9, 8, 15, 14, 13, 12));
  const __m256i kSeq = _mm256_setr_epi32(0, 8, 8*2, 8*3, 8*4, 8*5, 8*6, 8*7);

  for (size_t left_right = 0; left_right < 2; left_right++) {
    for (size_t sublane = 0; sublane < 8; sublane++) {
      const __m256i result = _mm256_shuffle_epi8(
          _mm256_i32gather_epi32((const int *)state_words, kSeq, 4),
          kByteSwapIndexes);
      OPENSSL_memcpy(out, &result, sizeof(result));
      out += sizeof(result);
      state_words++;
    }

    state_words += 7*8;
  }
}
