# Fiat

Some of the code in this directory is generated by
[Fiat](https://github.com/mit-plv/fiat-crypto) and thus these files are
licensed under the MIT license. (See LICENSE file.)

# Generating Fiat Crypto Code

With the files already checked in to the fiat-crypto repository,
`make src/Specific/solinas32_2e255m19_10limbs/femul.c`
and `make src/Specific/montgomery64_2e256m2e224p2e192p2e96m1_4limbs/femul.c` do
pretty much what one would expect they do. The rest of this document will
cover the moving parts behind that interface.

# Understanding Difficulties in Fiat

The remainder of this file will describe how the code here was generated using
the fiat-crypto repository in case this procedure is to be repeated (either for
a new modulus, or to add optimizations to 2^255-19 or p256).

Since Fiat abuses the Coq proof assistant to build a proof-generating compiler,
working with the code generation pipeline is painful in a number of ways (most
of which are only tangentially related to achieving precise correctness
guarantees).

- The top-level compiler is written in Ltac, a Coq built-in scripting language
  with the elegance and flexibility of pre-2000 JavaScript. The semantics of
  most language constructs are defined by the implementation alone and make very
  little sense without the context where they were first used. Most COQBUG(...)
  comments in the fiat-crypto repository are of this variety.
- Some aspects of Coq are not scriptable; instead fiat-crypto uses Python and
  sed to generate Coq source files. Some aspects of Coq output are not flexible;
  so fiat-crypto uses Python and sed to translate output to the desired format.
- Coq is generally slow and memory-hungry. Even outside Ltac, Coq was designed
  for manipulating small ("elegant") mathematical definitions, not
  several-hundred-line optimized code. It is the opinion of fiat-crypto
  contributors that this slowness is not inherent to what Coq aims to do but
  rather an artifact of priorities and attention during implementation (for
  example, the Lean proof assistant is arbitrarily slow in different cases).

The claim here is not that fiat-crypto itself is a beautifully-designed
snowflake, just that the problems with it have very little to do with the
complexity of compilation or proving and a lot more to do with paying off the
technical debt of the environment in which it is implemented.

# Overview of the Fiat Crypto Pipeline

The top-level instructions for generating the 2^255-19 field arithmetic in
curve25519.c are in fiat-crypto file
`src/Specific/solinas32_2e255m19_10limbs/CurveParameters.v`.
This file itself is was generated by implementation strategy heuristic scripts
(`generate_parameters.py`, `./remake_curves.sh`) as a part of an experiment to
generate code for all curves proposed for ECC, but we will treat it as a source
file here since it is probably easier to edit the file directly than to tweak
the heuristic scripts to the desired effect. The parameters are
correctness-relevant in that parameters "s" and "c" determine the finite field
for which arithmetic code is synthesized (and "bitwidth" determines the target
architecture), but consistency between duplicative parameters is proven
automatically in the pipeline, not assumed. For example, entering a carry chain
that does not guarantee lack of overflow on `bitwidth`-sized integers would
cause the pipeline to report an error instead of generating bad code.

~~~~~~~~~~~~~~~
Definition curve : CurveParameters :=
  {|
    sz := 10%nat; (* number of limbs *)
    base := 25 + 1/2; (* numerical radix *)
    bitwidth := 32; (* machine integer width *)
    s := 2^255;
    c := [(1, 19)];
    (* modulus = s - c *)
    carry_chains := Some [seq 0 (pred 10); [0; 1]]%nat;
    (* carrying to be be done after multiplication and before freeze *)
    (* this: 0->1, then 1->2, ... then 9->0, then 0->1, then 1->2 *)

    a24 := None; (* optional (A-2)/4 for generating ladderstep *)
    coef_div_modulus := Some 2%nat; (* # of modulus to add before sub/opp *)

    ... := None ...
  }.
~~~~~~~~~~~~~~~

From this file, the data flow is roughly as follows:
src/Specific/Framework/SynthesisFramework.v maps implementation strategy
request to strategy-specific compilation pipelines
(`./src/Specific/Framework/ArithmeticSynthesis/Defaults.v`,
`src/Specific/Framework/ArithmeticSynthesis/Montgomery.v`,
`src/Specific/Framework/ArithmeticSynthesis/Karatsuba.v`).  All these are
rather thin wrappers around very similar building blocks, which will be
described below.

# Implementation Templates

All concrete field arithmetic implementations are created by specializing a
generic template implementations that encodes the implementation strategy.
There are two major templates: word-by-word Montgomery saturated reduction and
unsaturated Solinas reduction; Karatsuba's trick and Golilocks trick are
implemented on top of the latter. In principle, each template could be quite
simple and readable by roughly the same audience as, say, the Handbook of
Applied Cryptography chapter on the same topic. For some parts, rather
reasonable-looking files are even checked into the repository:
`src/Arithmetic/MontgomeryReduction/WordByWord/Abstract/Definition.v` and
src/Arithmetic/MontgomeryReduction/WordByWord/Abstract/Proofs.v, similarly
`src/Demo.v` for Solinas reduction.

Unfortunately, these more reasonable files are not what is actually used by the
pipeline. This is because there is no nice way to specialize them to concrete
parameters: doing it in Ltac was determined to be way too slow, and the use of
dependent types for encoding array lengths rules out the use of another Coq
program as a compiler. It is on the roadmap to re-do everything without array
lengths in types, and a specializer as Coq program, and prove it correct once
and for all to avoid the performance overhead of Ltac dynamic checks. Until
that is done, there is a separate copy of each template, written in
continuation passing style (look it up if you don't know it) to allow using the
built-in `cbv` function instead of Ltac specialization. Why this hack works is
explained at the top of `src/Arithmetic/Core.v` along with CPS-based Solinas
reduction. The manually CPS-converted version of Montgomery reduction is in
`src/Arithmetic/MontgomeryReduction/WordByWord/Definition.v`.

After applying the templates to concrete parameters (like the ones shown above)
and `cbv`-ing it down to straight-line code, there are a couple further
transformations that need to be performed to make this code efficient (and
embeddable in C).

# Post-Specialization Compilers

The compilation passes that are performed on straight-line-ish code are glued
together in `./src/Compilers/Z/Bounds/Pipeline/Definition.v`. The individual
compiler phases are in `src/Compilers`, for example `src/Compilers/Linearize.v`
ensures that the output code is straight-line (no binders inside expressions),
and the rules for inferring ranges (and thus C integer types) for intermediate
values are in `src/Compilers/Z/Bounds/Interpretation.v`. Overall, this part of
the codebase is more methodical than the rest, with significantly fewer silly
workarounds. On the other hand, compiler construction is a complicated topic,
and doing compiler correctness proofs in Coq requires a very systematic
understanding of the technique in question. In particular, the compilers that
add or remove variables use the PHOAS encoding (Adam Chlipala ICFP 2008) to
reuse Coq's variable binding implementation. Fortunately, most compilers are
"done" in the sense that they are proven to preserve the operation of the
program and their optimization goal is relatively simple and well-understood,
so changes may not be necessary.
